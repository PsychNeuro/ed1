# Chapter 5: Learning, Motivation, and Emotion

Learning is the single most important process taking place in the brain.  Without learning, nothing else is possible.  All of our focus on the three-C's of compression, contrast, and control presumes a brain with sensible patterns of synaptic connectivity, that produce *useful* forms of each of these phenomena.  Without learning, neurons would randomly compress incoming sensory information, detecting irrelevant, bizzarre features that don't have any behavioral relevance.  Contrast would compare these random things against each other, producing equally meaningless relative comparisons.  Control would drive us toward random goals, and our behavior would be just a jumble of strange impulses.

Learning is essential because there are *way* (way, way, way...) too many synapses for any kind of genetic process to shape in a detailed way.  There are only about 20,000 different protein-coding genes in the human genome, which is only 2 times the number of synaptic inputs on a *single* neuron.  It is inconceivable that genes could code for any sensible fraction of the 100 *billion* times that amount of information that would be required to configure the full human brain.  This genetic argument accords with the obvious fact that we learn the vast majority of our abilities over an extremely protracted developmental window, in a way that depends critically on the experiences and education that we are exposed to.

Thus, the brain (specifically the neocortex) is fundamentally a *self-organizing* system, which somehow magically transforms raw sensory inputs into *knowledge* encoded in its billions of synaptic connections.  The mystery of this process has long perplexed philosophers, who have explored the opposing ideas of *empiricism* vs. *rationalism* and positions in between.  Empiricists embrace the idea that learning proceeds directly from sensory experience, while rationalists argue that there is no way that raw experience by itself is sufficient to create the sophisticated level of knowledge an adult human (philosopher) has.  Modern scientific approaches to this question retain much of this ancient debate, with some favoring a generous amount of innate knowledge, and others arguing that almost everything is learned.

We'll return to these issues in the Development chapter, but the quick summary is that neither extreme view is likely to be correct, with genetic and experiential factors each playing critical roles.  In particular, there is ample evidence that genes establish broad patterns of initial connectivity and orchestrate developmental transitions, such as synaptic pruning, which in turn strongly influence an experience-driven learning process operating at synapses throughout the neocortex.

Our objective in this chapter is to first understand the nature of these synaptic learning processes, which have been figured out in spectacular detail at this point, and explore some broader ideas about how they might result in this magical self-organization of knowledge over development.  Then, we turn to the forms of learning that were the focus of behaviorism: *classical and operant conditioning*.  These both depend on similar dopamine-driven learning mechanisms operating in the basal ganglia, amygdala, and related areas, which are now very well understood.  These forms of learning shape our core decision making process to select actions that are likely to be rewarding, and not punishing.

Finally, we broaden our perspective beyond the limited world-view of the behaviorists, and consider the possibility that *internal* factors such as *goals*, *drives*, and, ultimately, *emotions*, might play a central role in driving both our learning and decision making behavior.  This perspective, long embraced by social psychologists, is only recently beginning to be explored from the neuroscientific angle, which has been perhaps overly-enamored with the remarkable alignment between the classic externally-driven behaviorist conditioning processes and the function of dopamine in the basal ganglia.

## Synaptic Plasticity

If learning is the most important thing in the brain, then the most important thing about learning is that it takes place in the synapses interconnecting neurons.  This idea goes back at least to **Santiago Ramon y Cajal** in the late 1800's, the pioneering Spanish neuroscientist who advanced the idea that interconnected networks of neurons are doing most of the work in the brain.  Logically, the strength of the connections between neurons should alter the patterns of information flow through these networks, and thus makes sense as the primary locus of learning, and knowledge.  **Donald Hebb** cemented this idea with a compelling, well-specified proposal that memories are formed when neurons that are active at the same time increase the strength of their synaptic connections, so that they are then more likely to co-activate each other in the future [@Hebb49] -- in effect "gluing together" the different elements of a memory.  This idea has been captured with the pithy statement that "neurons that fire together, wire together".

However, it was not until 1966 that this *Hebbian* form of learning was actually demonstrated in the brain, by Bliss and Lomo [@BlissLomo73].  They described a form of *Long Term Potentiation* (LTP) of the synaptic strengths between well-defined groups of neurons, where potentiation means "getting stronger" and the "long-term" aspect of it was critical to distinguish from earlier discoveries of synaptic potentiation that only lasted for a few minutes.  If synaptic changes are really the basis for learning and knowledge in the brain, they had better last for more than a few minutes, because clearly our memories and knowlege can last a very long time.

The field of LTP research expanded rapidly from that point onward, and progressively more detailed questions were addressed about the exact nature of what is changing in the synapses, and what specific factors in the activity of the sending and receiving neurons on either side of the synapse were critical for causing it to change.  After many controversies and twists and turns in this amazing story of scientific discovery, we now have a very solid and detailed understanding of how this process works, at least in terms of all the underlying biochemical mechanisms.  It is a fabulous success story for the power of the scientific method, to drill down and figure out exactly how some complex system actually works.  Perhaps most remarkably, Hebb's original idea seems to have been nicely supported, by a remarkable interaction of different moving parts: changing the strength of the synapse requires *both* the sending and receiving neurons to be active.

![Fig 5-1: Mechanisms of synaptic plasticity, resulting in changes in the overall strength of the synaptic connection between the sending axon and the receiving dendrite.  1. The receiving neuron must be active, so that its elevated membrane potential (Vm) kicks out the positively-charged Mg+ ions from the NMDA receptors (2).  3. The sending neuron must fire and release glutamate, which then binds to the NNDA receptors, causing them to open and Ca++ ions to enter (4).  Ca++ then triggers complex chemical pathways that ultimately result in changes in the numbers of AMPA receptors poking out across the membrane, which thus changes the overall amount of Na+ that can enter for any given firing of the sending neuron.](figures/fig_ltpd_synapse.png){ width=50% }

Figure 5-1 shows the major steps in the process of synaptic change.  The receiving neuron must be active enough so that its elevated membrane potential pushes out positively-charged magnesium ions (*Mg+*), which are otherwise blocking the opening of the *NMDA* receptors.  And the sending neuron must be actively releasing glutamate neurotransmitter, as a result of spiking, because glutamate binding to the NMDA receptors (in addition to the AMPA receptors) is necessary to cause them to open.  Whereas AMPA receptors allow Na+ ions to flow into the cell, NMDA allows *calcium* (*Ca++*) ions to enter, and these Ca++ ions then trigger a cascade of chemical reactions that ultimately leads to the change in synaptic plasticity.  This critical role for Ca++ is consistent with many other similar such biochemical processes throughout the body -- again, evolution often reuses existing mechanisms.

The main consequence of Ca++ entry is a change in the number of AMPA receptors in the synapse, which then changes the overall amount of Na+ that can enter when the sending neuron spikes.  Much more can be said about the details of these Ca++ driven chemical pathways [@Rudy13], and the other associated changes that take place in the synapse, but the core logic remains the same as Hebb envisioned it: both neurons must be active for the synapse to change.

![Fig 5-2: Direction of synaptic change as a function of the amount of calcium entering the dendritic spine.  Lower amounts of Ca++ result in LTD = long-term *depression* or decrease in synaptic strength, whereas higher amounts result in LTP = potentiation or increase in synaptic strength.](figures/fig_ltp_ltd_ca2+.png){ width=50% }

However, Hebb overlooked one *essential* aspect of learning, which was also neglected in the early days of research on LTP.  This is the fact that you can't alway just be increasing the strength of synapses.  Eventually, all the synapses would get ever-stronger, and the brain would blow up in a huge epileptic seizure.  Instead, it is equally if not more important that synapses also *decrease* in synaptic strength, which has been named *long-term-depression* or *LTD*.  Decreases may be more important than increases, from the perspective of the *compression* function of neurons: each neuron has to essentially throw away a huge amount of information in order to compress its 10,000 inputs into a single output signal, and LTD makes synapses weaker and thus facilitates this information filtering process.  In any case, Figure 5-2 shows that the balance between LTP and LTD is a function of the overall amount of calcium entering the dendrite -- lower amounts result in LTD, while higher amounts result in LTP.  This behavior emerges from a competition between two different chemical pathways, one which drives LTP and the other LTD, and their relative dependence on Ca++ levels.  This is yet another tug-of-war taking place within neurons -- this competitive dynamic is a very commonly-used mechanism at all levels of the brain.

One intriguing finding that makes sense in terms of this balance between LTP and LTD, is that weak activation of perceptual inputs seems to make those things harder to see, while strong activation makes them easier to see [@NewmanNorman10].  Thus, the weak activation leads to the lower levels of Ca++, and causes LTD, whereas the stronger activation drives higher levels and LTP.

## Neocortical Learning

Now that we know in detail how learning operates at the synaptic level, you might think that all of the mysteries of brain function should be solved, given what we said about the essential role of learning.  Unfortunately, this is not the case.  There are a number of challenges here, but chief among them is that there are so many synapses and neurons involved in learning any given bit of knowledge, that it is essentially impossible to go directly from behavior of the individual synapse up to this *emergent* behavior of learning in the larger neural network.  The major tool that can be used to bridge this gap are computer simulations of neural networks, with equations capturing things like the function shown in Figure 5-2, and the overall firing activity of neurons in response to stimulus inputs, etc.

Extensive work with such models has repeatedly shown that the known Hebbian-like learning mechanisms described above does *not* result in the kinds of larger-scale learning that people are clearly capable of.  The reasons for this are well understood, but beyond the scope of this discussion.  Furthermore, the kind of learning that *does* work reliably in these neural models, and is used in the recent powerful AI (artificial intelligence) models currently powering the speech recognition and other advanced capabilities in your cell phone and other gadgets, is called *error backpropagation* [@RumelhartHintonWilliams86], and it makes some additional demands on the biology that some influential people have argued are implausible [@Crick89].

This problem has been my specific area of research for over 20 years, and my colleagues and I have developed progressively more biologically plausible models of how this error-driven learning process could work within the neocortex [@OReilly96; @OReillyMunakataFrankEtAl12].  Our latest idea is that the brain is constantly making predictions about what will be seen next, at a rate of about 10 times per second (i.e., the *alpha* frequency), and very specific patterns of neural connectivity in the neocortex and thalamus provide a "ground truth" correct answer against which those predictions are compared.  Thus, the difference between these predictions and what actually happens provides the error signals driving learning, and we have shown how these error signals, which exist as differences in the activity states of neurons over time, could drive learning in synapses throughout the neocortex.  Furthermore, our computer models show that this form of learning can indeed acquire the kinds of sophisticated knowledge that people do, for example the ability to recognize different categories of objects.

There is just one problem with all this: while our proposed synaptic plasticity mechanisms are consistent with the existing body of detailed knowledge, they also make a few extra demands that have not been tested empirically.  So we do not yet know if this theory all goes through or not.  Furthermore, there are various other different theories about how all of this could work, which make different, testable predictions.  Thus, hopefully we'll get some answers in the not-too-distant future, and then we can potentially connect the dots all the way from the beautifully detailed biochemical level up to the high-level effects of these mechanisms in forming new knowlege representations within the neural networks of the neocortex.  For now, we have to live with a glaring hole in our overall understanding of this most important process of learning in the brain.

## Dopamine-modulated Learning

Most introductory textbooks do not even address any of the above topics in learning, and focus exclusively on the relatively well-understood domain of conditioning, which has been studied since the days of Pavlov and the behaviorist school in the early 1900's.  This has become an area of renewed interest in neuroscience, since the discovery that dopamine activity almost perfectly accounts for the nature of these conditioning phenomena [@MontagueDayanSejnowski96; @SchultzDayanMontague97].

### Classical (Pavlovian) Conditioning

### Operant / Instrumental Conditioning

## Motivation

* key distinction is external (extrinsic) vs. internal (intrinsic).  Behaviorists emphasized external factors: reward / punishment.  internal factors seem much more important, given everything we subjectively know about ourselves.  Did behaviorists really think that everything *they* were doing was just the result of external reward and punishment?  Didn't they have any actual subjective insight into their own motivational systems?  Didn't they have *goals*, *desires*?

## Emotion and Arousal

* extensive interconnected networks of areas devoted to emotion and arousal, at all levels of the brain.  thus, from the modern perspective, emotion is as much of an emergent, interactive process as anything else in the brain.  However, historically, people took rather extreme and somewhat strange and hard-to-understand perspectives. James-lange / Cannon-Baird etc.

* is emotion really what makes humans unique?  hollywood clearly thinks so: evil robotic non-emotional beings...  part of this is that emotion is what keeps us from harming each other: psychopaths lack that basic emotional connection.  So, from a survival perspective, we really depend on everyone sharing these protective emotional responses, and anything that doesn't is immediately scary and foriegn.  

* circumplex model.

