---
bibfile: ccnlab.bib
---

# Chapter 5: Learning, Motivation, and Emotion

Learning is the single most important process taking place in the brain.  Without learning, nothing else is possible.  All of our focus on the three-C's of compression, contrast, and control presumes a brain with sensible patterns of synaptic connectivity, that produce *useful* forms of each of these phenomena.  Without learning, neurons would randomly compress incoming sensory information, detecting irrelevant, bizarre features that don't have any behavioral relevance.  Contrast would compare these random things against each other, producing equally meaningless relative comparisons.  Control would drive us toward random goals, and our behavior would be just a jumble of strange impulses.

Learning is essential because there are *way* (way, way, way...) too many synapses for any kind of genetic process to shape in a detailed way.  There are only about 20,000 different protein-coding genes in the human genome, which is only 2 times the number of synaptic inputs on a *single* neuron.  It is inconceivable that genes could code for any sensible fraction of the 100 *billion* times that amount of information that would be required to configure the full human brain.  This genetic argument accords with the obvious fact that we learn the vast majority of our abilities over an extremely protracted developmental window, in a way that depends critically on the experiences and education that we are exposed to.

Thus, the brain (specifically the neocortex) is fundamentally a *self-organizing* system, which somehow magically transforms raw sensory inputs into *knowledge* encoded in its billions of synaptic connections.  The mystery of this process has long perplexed philosophers, who have explored the opposing ideas of **empiricism** vs. **rationalism** and positions in between.  Empiricists embrace the idea that learning proceeds directly from sensory experience, while rationalists argue that there is no way that raw experience by itself is sufficient to create the sophisticated level of knowledge an adult human (philosopher) has.  Modern scientific approaches to this question retain much of this ancient debate, with some favoring a generous amount of innate knowledge, and others arguing that almost everything is learned.

We'll return to these issues in the Development chapter, but the quick summary is that neither extreme view is likely to be correct, with genetic and experiential factors each playing critical roles.  In particular, there is ample evidence that genes establish broad patterns of initial connectivity and orchestrate developmental transitions, such as synaptic pruning, which in turn strongly influence an experience-driven learning process operating at synapses throughout the neocortex.

Our objective in this chapter is to first understand the nature of these synaptic learning processes, which have been figured out in spectacular detail at this point, and explore some broader ideas about how they might result in this magical self-organization of knowledge over development.  Then, we turn to the forms of learning that were the focus of behaviorism: *classical and operant conditioning*.  These both depend on similar dopamine-driven learning mechanisms operating in the basal ganglia, amygdala, and related areas, which are now very well understood.  These forms of learning shape our core decision-making process to select actions that are likely to be rewarding, and not punishing.

Finally, we broaden our perspective beyond the limited world-view of the behaviorists, and consider the possibility that *internal* factors such as *goals*, *drives*, and, ultimately, *emotions*, might play a central role in driving both our learning and decision-making behavior.  This perspective, long embraced by social psychologists, is only recently beginning to be explored from the neuroscientific angle, which has been perhaps overly-enamored with the remarkable alignment between the classic externally-driven behaviorist conditioning processes and the function of dopamine in the basal ganglia.

## Synaptic Plasticity

If learning is the most important thing in the brain, then the most important thing about learning is that it takes place in the synapses interconnecting neurons.  This idea goes back at least to Santiago Ramon y Cajal in the late 1800's, the pioneering Spanish neuroscientist who advanced the idea that interconnected networks of neurons are doing most of the work in the brain.  Logically, the strength of the connections between neurons should alter the patterns of information flow through these networks, and thus makes sense as the primary locus of learning, and knowledge.

Donald Hebb cemented this idea with a compelling, well-specified proposal that memories are formed when neurons that are active at the same time increase the strength of their synaptic connections, so that they are then more likely to co-activate each other in the future [@Hebb49].  In effect, learning is "gluing together" the different elements of a memory.  This idea has been captured with the pithy statement that "neurons that fire together, wire together".

However, it was not until 1966 that this **Hebbian** form of learning was actually demonstrated in the brain, by Bliss and Lomo [@BlissLomo73].  They described a form of **Long Term Potentiation** (**LTP**) of the synaptic strengths between well-defined groups of neurons, where potentiation means "getting stronger" and the "long-term" aspect of it was critical to distinguish from earlier discoveries of synaptic potentiation that only lasted for a few minutes.  If synaptic changes are really the basis for learning and knowledge in the brain, they had better last for more than a few minutes, because clearly our memories and knowledge can last a very long time.

The field of LTP research expanded rapidly from that point onward, and progressively more detailed questions were addressed about the exact nature of what is changing in the synapses, and what specific factors in the activity of the sending and receiving neurons on either side of the synapse were critical for causing it to change.  After many controversies and twists and turns in this amazing story of scientific discovery, we now have a very solid and detailed understanding of how this process works, at least in terms of the underlying biochemical mechanisms.  It is a fabulous success story for the power of the scientific method, to drill down and figure out exactly how some complex system actually works.  Perhaps most remarkably, Hebb's original idea seems to have been nicely supported, by a remarkable interaction of different moving parts: changing the strength of the synapse requires *both* the sending and receiving neurons to be active.

![**Figure 5.1:** Mechanisms of synaptic plasticity, resulting in changes in the overall strength of the synaptic connection between the sending axon and the receiving dendrite.  1. The receiving neuron must be active, so that: 2. its elevated membrane potential (Vm) kicks out the positively-charged Mg+ ions from the NMDA receptors. 3. The sending neuron must fire and release glutamate, which then binds to the NNDA receptors.  4. Causing them to open and Ca++ ions to enter.  5. Ca++ then triggers complex chemical pathways that ultimately result in changes in the numbers of AMPA receptors poking out across the membrane, which thus changes the overall amount of Na+ that can enter for any given firing of the sending neuron.](figures/fig_ltpd_synapse.png){ width=50% }

Figure 5.1 shows the major steps in the process of synaptic change.  The receiving neuron must be active enough so that its elevated membrane potential pushes out positively-charged magnesium ions (*Mg+*), which are otherwise blocking the opening of the *NMDA* receptors.  And the sending neuron must be actively releasing glutamate neurotransmitter, as a result of spiking, because glutamate binding to the NMDA receptors (in addition to the AMPA receptors) is necessary to cause them to open.  Whereas AMPA receptors allow Na+ ions to flow into the cell, NMDA allows *calcium* (*Ca++*) ions to enter, and these Ca++ ions then trigger a cascade of chemical reactions that ultimately leads to the change in synaptic plasticity.  This critical role for Ca++ is consistent with many other similar such biochemical processes throughout the body -- evolution often reuses existing mechanisms.

The main consequence of Ca++ entry is a change in the number of AMPA receptors in the synapse, which then changes the overall amount of Na+ that can enter when the sending neuron spikes.  Much more can be said about the details of these Ca++ driven chemical pathways [@Rudy13], and the other associated changes that take place in the synapse, but the core logic remains the same as Hebb envisioned it: both neurons must be active for the synapse to change.

However, Hebb overlooked one *essential* aspect of learning, which was also neglected in the early days of research on LTP.  This is the fact that you can't only *increase* the strength of synapses.  Eventually, all the synapses would get ever-stronger, and the brain would blow up in a huge epileptic seizure.  Instead, it is equally if not more important that synapses also *decrease* in synaptic strength, which has been named **Long Term Depression** or **LTD**.  Decreases may be more important than increases, from the perspective of the *compression* function of neurons: each neuron has to essentially throw away a huge amount of information in order to compress its 10,000 inputs into a single output signal, and LTD makes synapses weaker and thus facilitates this information filtering process.

![**Figure 5.2:** Direction of synaptic change as a function of the amount of calcium entering the dendritic spine.  Lower amounts of Ca++ result in LTD = long-term *depression* or decrease in synaptic strength, whereas higher amounts result in LTP = potentiation or increase in synaptic strength.](figures/fig_ltp_ltd_ca2+.png){ width=30% }

In any case, Figure 5.2 shows that the balance between LTP and LTD is a function of the overall amount of calcium entering the dendrite -- lower amounts result in LTD, while higher amounts result in LTP.  This behavior emerges from a competition between two different chemical pathways, one which drives LTP and the other LTD, and their relative dependence on Ca++ levels.  This is yet another tug-of-war taking place within neurons -- this competitive dynamic is a very commonly-found mechanism at all levels of the brain.

One intriguing finding that makes sense in terms of this balance between LTP and LTD, is that weak activation of perceptual inputs seems to make those things harder to see, while strong activation makes them easier to see [@NewmanNorman10].  Thus, the weak activation leads to the lower levels of Ca++, and causes LTD, whereas the stronger activation drives higher levels and LTP.

## Neocortical Learning

Now that we know in detail how learning operates at the synaptic level, you might think that all of the mysteries of brain function should be solved, given what we said about the essential role of learning.  Unfortunately, this is not the case.  There are a number of challenges here, but chief among them is that there are so many synapses and neurons involved in learning any given bit of knowledge, that it is essentially impossible to go directly from behavior of the individual synapse up to this *emergent* behavior of learning in the larger neural network.  The major tool that can be used to bridge this gap are computer simulations of neural networks, with equations capturing something like the function shown in Figure 5.2, operating within networks of neurons that behave something like real neurons in response to stimulus inputs.

Extensive work with such models has repeatedly shown that the known Hebbian-like learning mechanisms described above does *not* result in the kinds of larger-scale learning that people are clearly capable of.  The reasons for this are well understood, but beyond the scope of this discussion.  Furthermore, the kind of learning that *does* work reliably in these neural models, and is used in the recent powerful AI (artificial intelligence) models currently powering the speech recognition and other advanced capabilities in your cell phone and other gadgets (as discussed in the previous chapter), is called **error backpropagation** [@RumelhartHintonWilliams86], and it makes some additional demands on the biology that some influential people have argued are implausible [@Crick89].

This problem has been my specific area of research for over 20 years, and my colleagues and I have developed progressively more biologically plausible models of how this error-driven learning process could work within the neocortex [@OReilly96; @OReillyMunakataFrankEtAl12].  Our latest idea is that the brain is constantly making predictions about what will be seen next, at a rate of about 10 times per second (i.e., the *alpha* frequency), and very specific patterns of neural connectivity in the neocortex and thalamus provide a "ground truth" correct answer against which those predictions are compared.  Thus, the difference between these predictions and what actually happens provides the error signals driving learning, and we have shown how these error signals, which exist as differences in the activity states of neurons over time, could drive learning in synapses throughout the neocortex.  Furthermore, our computer models show that this form of learning can indeed acquire the kinds of sophisticated knowledge that people do, for example the ability to recognize different categories of objects [@OReillyRussinZolfagharEtAl20].

There is just one problem with all this: while our proposed synaptic plasticity mechanisms are consistent with the existing body of detailed knowledge, they also make a few extra demands that have not been tested empirically.  So we do not yet know if this theory all goes through or not.  Furthermore, there are various other different theories about how all of this could work, which make different, testable predictions.  Thus, hopefully we'll get some answers in the not-too-distant future, and then we can potentially connect the dots all the way from the beautifully detailed biochemical level up to the high-level effects of these mechanisms in forming new knowledge representations within the neural networks of the neocortex.  For now, we have to live with a bit of a hole in our overall understanding of this most important process of learning in the brain.

Before moving on to dopamine, another way of understanding learning taking place in the neocortex is in terms of **imitation learning**, also known as **observational learning**, where somehow we are able to observe other people's behavior, and then turn around and produce some approximation of that behavior ourselves.  The popular phrase "monkey see, monkey do" suggests that this form of learning is widespread in primates, but the actual behavioral data across a range of species suggests that people are by far the most likely to engage in true imitation, while other species exhibit a range of socially-influenced learning that often falls short of direct imitation of actions [@CarceaFroemke19].  This imitative capacity is closely related to *cultural transmission* of behavior across individuals, and the best non-human examples of this comes from chimpanzees who learn techniques for getting termites using sticks, or using moss as a sponge [@LamonNeumannGruberEtAl17].

Although imitation may sound relatively simple, upon closer examination, the process of turning the perception of behavior into your own motor program requires a highly sophisticated perceptual and motor control system.  Thus, the fact that even young infants appear to be capable of this is quite remarkable [@MeltzoffMoore94; @FerrariVisalberghiPauknerEtAl06].  An important neural substrate for this form of learning has been found, in the form of **mirror neurons** that appear to achieve this feat of mapping observed behavior into the same patterns of neural firing that are active when you perform the same behavior [@IacoboniWoodsRizzolatti99].  However, it is not known how these neurons learn this mapping in the first place, so it remains a phenomenon in search of a deeper explanation.  Nevertheless, there is an intriguing suggestion that these mirror neurons might be affected in autism spectrum disorders, which could potentially account for the difficulties in empathy in this population [@GalleseKeysersRizzolatti04].

## Dopamine-modulated Learning

Most introductory textbooks do not address any of the above topics in learning, and focus exclusively on the relatively well-understood domain of conditioning, which has been studied since the days of Pavlov and the behaviorist school in the early 1900's.  This has become an area of renewed interest in neuroscience, since the discovery that dopamine activity almost perfectly accounts for the nature of these conditioning phenomena [@MontagueDayanSejnowski96; @SchultzDayanMontague97].

### Classical (Pavlovian) Conditioning

![**Figure 5.3:** The classical conditioning paradigm. ](figures/fig_classical_conditioning.png){ width=80% }

The classical conditioning paradigm (Figure 5.3) centers around learning the connection between a previously *neutral* stimulus (the **conditioned stimulus** or **CS**) and a biologically-established, affectively significant *outcome*, known as the **unconditioned stimulus** or **US**.  In the classic experiments by Pavlov, the ringing of a bell served as the CS, and food reward as the US, and the subjects were dogs, who learned over a few repetitions of the CS followed by the US to salivate after hearing the bell, in anticipation of receiving the food. The salivation is somewhat confusingly labeled the **un/conditioned response** (U/CR), where it is *un*-conditioned (UCR) prior to learning in response to the food US, and *conditioned* (CR) after learning in response to the CS.  So, the same response has two different labels depending on what is driving it.

Ecologically, this simplified lab experiment is thought to capture the real-world learning about different stimuli that help us anticipate and prepare for important upcoming outcomes.  For example, when you are hungry and driving down the highway on a road trip, the sight of a McDonald's sign alerts you to the availability of food there.  Thus, the McDonald's sign is effectively a CS, and indeed this conditioning paradigm applies well to the goal of advertising, which is to establish a solid connection between a brand logo and desirable US outcomes.  In many ads, the use of sexual imagery or famous faces directly activates the brain's reward pathways -- they are literally replicating the classical conditioning paradigm to associate the CS (brand / logo) with the US driven by these rewarding stimuli.

![**Figure 5.4:** Dopamine neuron firing in a classical conditioning paradigm of CS followed by US (labeled R for reward -- it was actually a drop of juice).  Top: Unexpected rewards (at time point R) drive dopamine firing.  Middle: Trained CS followed by R shows dopamine firing at the CS, but *not* for the reward.  Bottom: Trained CS followed by *omission* of R shows reduction of firing at R. Each row of dots shows when a dopamine neuron fired a spike on a given recording trial, and the bars at the top show the accumulated histogram of all the spikes at that corresponding point in time across all such trials.  Dopamine does *not* respond to raw reward input, because it fails to fire in when the reward is accurately predicted by the CS, in the middle panel.  Furthermore, it directly signals "disappointment" by reducing doapmine firing when an expected reward is not received.  These and many similar results show that dopamine responds to the *contrast* or difference between predicted and actual rewards.  From Schultz et al, 1997.](figures/fig_schultz97_vta_td.png){ width=40% }

Although Pavlov and the behaviorists where exclusively concerned with overt behavior such as salivation, we now know the internal biology that drives this form of learning.  Figure 5.4 shows how dopamine neurons in the *ventral tegmental area* (*VTA*) of the brainstem reticular activating system respond in a classical conditioning experiment [@Schultz86; @SchultzDayanMontague97].  When the US (labeled R = reward, a juice drop) is presented without any prior CS, dopamine responds with robust firing above its "tonic" steady base rate of firing.  This is consistent with the naive idea that dopamine encodes raw reward signals.

However, when the very same reward is presented after a CS (which has been reliably paired with the reward in prior conditioning trials), *dopamine no longer fires to the reward!*  Furthermore, when the CS is presented and the reward is *withheld*, dopamine neurons  show a suppression or *dip* in firing below their tonic baseline.  Psychologically, you would feel disappointed if you didn't get the reward you expected, and indeed that is exactly what the dopamine neurons are signaling.

These results, from the pioneering work of Wolfram Schultz and colleagues, have profound, far-reaching implications, and represent one of the most exciting and important findings in neuroscience.  They are also one of the most important examples of the **contrast** principle, as we emphasized in the introductory chapter.  Specifically, these results show that dopamine neurons respond to the contrast or difference between an expectation of reward, and what is actually received, *not* to the raw reward input itself.  This contrast property of dopamine is what drives insatiable greed, dissatisfaction, and apathy, because once we learn to expect any given positive reward-like outcome, we no longer receive dopamine for it!

This property of dopamine is what causes kids to be so entitled and spoiled: they come to expect all that coddling from their overprotective parents, and all the excitement they get from playing video games, so that when they finally get out into the "real world", it is all so difficult and filled with disappointing drudgery.  It is also why so many famous people, especially rock stars it seems, turn to dopamine-activating drugs of abuse -- once they adapt to their new amazing famous lifestyle, their dopamine system no longer gives them that amazing feeling of unexpected reward.  Drugs like cocaine artificially bypass the expectation-driven contrast mechanisms of the dopamine system, so they continue to drive dopamine bursts.  However, even here the system slowly adapts and more and more drug is required to achieve the same effects, so really there is no escape from the evil maw of the dopamine contrast effect!

From a hard-nosed learning theory perspective, there is a very good reason why the dopamine system must work in this contrast-based way: *learning is most efficient when it is focused on what is not yet learned.*  Learning something you already know simply doesn't make much sense.  Thus, in the case of classical conditioning, continuing to learn about the fact that the CS predicts the reward after the system has already acquired this association isn't very useful.  And this logic suggests that dopamine is fundamentally a *learning* signal, not a reward signal.  In particular, as we'll see in the next section, dopamine directly affects learning in the basal ganglia and other brain areas, including the areas that are learning about the CS -- US association in the first place.  Thus, as dopamine stops firing at the time of the US (R, reward), it stops the further learning of this association (because it has already been learned).

This basic theory of how learning should function was systematized by Robert Rescorla and Allan Wagner in a seminal paper [@RescorlaWagner72], where they proposed a simple mathematical "learning rule" that says that the amount of new learning should be proportional to the contrast or difference between the actual reward you receive and what you already expect the reward should be.  This is also known as a **reward prediction error** (**RPE**).

Roughly a decade later, Rich Sutton and Andy Barto published an important extension to this idea [@SuttonBarto81], known as the *temporal differences* (*TD*) learning rule, which can also account for the fact that dopamine learns to fire at the onset of the CS, even as it stops firing for the expected US.  Furthermore, this work led to the development of many advanced mathematical techniques in a field collectively known as **reinforcement learning** (*RL*), which is a branch of *machine learning* that deals specifically with learning from overall reward / punishment signals.  These RL techniques have been used in many different AI technologies, and play a central role in the recent advances from the Google DeepMind group, in their models that learn to play Go and challenging video games [@SilverSchrittwieserSimonyanEtAl17; @MnihKavukcuogluSilverEtAl15].

![**Figure 5.5:** Biological systems involved in classical conditioning.  There are two major learning systems, LV and PV (columns), each of which has cortex-like, basal-ganglia-like, and brainstem components (rows).  LV (*learned value*) reflects the contributions of the amygdala to forming CS -- US associations.  PV is the ventral striatum, which drives reward-prediction-error (RPE) firing in the dopamine system (VTA / SNc).  Together these constitute the PVLV system, named in honor of Pavlov.  The BLA (basolateral amygdala) within the LV system learns to associate CS's with corresponding US's, while the CEA (central amygdala) reduces these higher-level associations down to specific "Go" vs "NoGo" signals in a basal-ganglia-like fashion, and directly drives dopamine firing and core behavioral responses (*conditioned responses*) appropriate for different US's.  The PV system likewise has a cortical component in the ventral and medial areas of the prefrontal cortex (vmPFC), and a basal-ganglia component in the ventral striatum (VS).  Dopamine firing in the VTA / SNc drives learning throughout all of these areas.](figures/fig_bvpvlv_functional_org_simpler.png){ width=40% }

Although the TD learning rule provides an elegant and powerful mathematical description of classical conditioning, the brain networks actually involved in this form of learning are considerably more complex.  Figure 5.5 [@MollickHazyKruegerEtAlInPress; @HazyFrankOReilly10] shows a summary diagram of these networks.  Conditioning learning involves interactions between two "vertically" organized sub-systems, one involved in forming associations between CSs and USs and another that drives the contrast-with-expectations differences in the dopamine system.  This first, associative system depends on nuclei within the *amygdala*, and the second depends on the ventral (bottom) areas of the striatum in the basal ganglia.  This framework can account for a wide range of data about the biology and function of dopamine-driven learning in the brain, and, given the overall complexity of the system, the ability to simulate it all in a computer model is essential for understanding how it all works.

#### Extinction and Context in Conditioning

As was the case with LTD (long-term-depression), figuring out how the associations between CS and US are *unlearned* is just as important as figuring out how they are learned.  This involves the phenomenon of **extinction**, where the CS is repeatedly presented while withholding the US.  From Figure 5.4, this should produce repeated *dips* in dopamine levels (much disappointment), which in turn should drive LTD in synaptic connections, causing the association between the CS and US to be unlearned.

While this all does occur, the situation turns out to be considerably more complicated, in ways that make sense ecologically.  To make a long story short, the brain actually learns *new associations* during these extinction events, in addition to weakening (somewhat) the existing ones.  These new associations effectively encode **context-specific exceptions** to the original association -- e.g., "in this particular situation, you're not going to get the food, but you might still get it in other situations".  Furthermore, the nature of this new learning is under top-down control from the ventral / medial frontal cortex, which can play a critical role in interpreting the nature of what is going on: has the world really changed, or is it just kind of random [@QuirkMueller08; @GershmanBleiNiv10]?

The advantage of all this is that the initial CS -- US association is relatively preserved, and especially if this was something learned through a painful, dangerous experience, it is probably a good idea to keep these memories around.  Better safe than sorry.  The disadvantage is evident in the phenomenon of PTSD (post-traumatic stress disorder), where traumatic memories cannot be extinguished, and keep intruding into normal life.  There are significant individual differences in the extent of PTSD, and a major factor reflects the ability to exert top-down control and establish a strong new context to override the traumatic situation.

Another important consequence of this type of learning is that people will tend to hold onto these associations even in the face of repeated disconfirmation, explaining each new failure as another "special case" or circumstance -- this is evident for example in doomsday cults, which respond to each absence of predicted doomsday by reinforcing their core beliefs, while attributing the failures to unforseen contingencies [@BoudryBraeckman12].

In the lab, these extinction phenomena are observed in the phenomena of **spontaneous recovery**, **reinstatement**, and **renewal**, which are typically observed in aversive conditioning situations (i.e., when the US is a negative outcome, like getting shocked).  Spontaneous recovery refers to the re-emergence of the CS -- US association after extinction (typically after some time has passed), without any further training, clearly showing that extinction learning did not erase this original memory.  Reinstatement occurs after a single US presentation without the prior CS, after which the CS -- US association is reinstated.  The US reactivates the associated memories and this is enough to overcome the extinction learning.  Renewal is particularly revealing of the important role of context.  In this case, the subject is conditioned in one environment (A) and extinguished in a second, novel context (B).  When put back into the original context (A), the original CS -- US association is *immediately* effective without any further learning.  In other words, the subject learned a context-specific exception ("when in context B, I won't get shocked") instead of unlearning the original association.

### Operant / Instrumental Conditioning

Classical conditioning is the sensory front-end to the other major form of learning studied by the behaviorists: *operant* or *instrumental* conditioning.  This form of learning occurs through the reinforcement or punishment of *actions*, instead of stimuli.  The central idea is captured in **Thorndike's law of effect**: *actions that lead to good outcomes are more likely to be taken, while those that lead to bad outcomes are less likely* [@Thorndike1911].  This is so intuitive that it is difficult to imagine it being otherwise, but nevertheless, it captures a considerable amount of behavior in humans and animals.

![**Figure 5.6:** How increases in dopamine (bursts) and decreases in dopamine (dips) drive learning in opposing Go vs. NoGo pathways in the basal ganglia.  Through the complicated basal ganglia circuitry, the firing of Go (aka direct pathway) neurons leads to a net excitation of motor plans in the frontal cortex.  The NoGo pathway has the opposite effect, preventing the frontal activation that would otherwise occur from Go activation.  When an action leads to an unexpected positive outcome, the resulting dopamine burst activates a special type of dopamine receptor (the D1 receptor), which drives LTP learning in the input synapes to the Go neurons.  This makes those neurons more likely to fire again under similar circumstances, achieving Thorndike's law of effect.  The opposite happens when dopamine dips occur for unexpectedly bad outcomes, which interesingly has a net LTP effect on the NoGo neurons via D2 receptors, and an LTD effect on the Go neurons.](figures/fig_bg_frontal_da_burst_dip.png){ width=70% }

We now know how this type of learning works, in terms of dopamine's effect on the basal ganglia (Figure 5.6) [@Frank05; @GerfenSurmeier11].  Unexpected positive outcomes following a given action result in a burst of dopamine (as we saw in Figure 5.4), and this dopamine burst acts on D1 receptors located on the "Go" neurons of the basal ganglia to drive LTP of the synapses into the neurons that decided to trigger that action.  Thus, these stronger synaptic inputs make it more likely that the same action will be triggered again in the future, when similar inputs are driving the basal ganglia (i.e., in similar situations), thereby achieving Thorndike's law of effect.

The opposite pattern of changes occurs when unexpectedly bad outcomes arise, which drive dips in dopamine firing, and end up strengthening inputs to the "NoGo" neurons that compete against the Go pathway and prevent an action from being triggered.  Thus, actions that lead to bad outcomes are less likely to be triggered, consistent with the other half of Thorndike's law of effect.

![**Figure 5.7:** *Actor-Critic* schematic for the relationship between the dopamine (DA) signal driven by principles of classical conditioning (the critic), and action decisions triggered in the basal ganglia (BG) (the actor).  The environment *state* is represented by various abstract, higher-level *compressed* representations in neocortex, which feeds into both the critic and actor.  The actor decides on actions to take as a function of these neocortical inputs, and the critic generates predictions about the kinds of US outcomes that are likely to result.  Learning in both the critic and actor is a function of the dopamine signal, which is symbolized as a delta, or *reward prediction error* (RPE).  Thus, classical and operant / instrumental conditioning are connected through this actor -- critic relationship. ](figures/fig_actor_critic_state_bio.png){ width=30% }

The overall relationship between dopamine and the basal ganglia is summarized in Figure 5.7, where classical conditioning processes train the **critic** what kinds of rewards or punishments to expect, and the resulting differences between these expectations and actual rewards / punishments, reflected in the dopamine signal, then drives learning in the **actor** (basal ganglia).  This image of dopamine as a critic fits with our overall conception of the *contrast* nature of this signal: it is never satisfied and quick to criticize, just like a critic.  Of course, the poor actor has to do all the hard work of coming up with stuff for the critic to critique, but, as you may have experienced, it is often hard to be properly critical of your own behavior, whereas it is much easier to see what is wrong with other people.  Thus, separating the critic and actor components in the brain makes sense, and is another example where two fully interdependent systems can nevertheless be seen as performing distinct functions.

#### Partial Reinforcement, Gambling, and Shaping

One of the topics that the behaviorists explored extensively was *reinforcement schedules* -- different rates and patterns of delivering rewards.  The most interesting and relevant finding from this work is that **partial reinforcement** can have surprisingly strong effects compared more reliable reinforcement schedules.  In a partial reinforcement schedule, rewards are only delivered randomly on a fraction of successful action trials.  In effect, it is just like gambling, where there is a relatively infrequent, random payout.  The net effect of this is to confound the critic system, which can no longer accurately predict what kind of outcome to expect.  Therefore, when a positive reward is received, it is not *discounted* like would have been if it was perfectly predictive.  You will get that burst of dopamine for the reward!  This is why gambling can be so addictive -- it works just like addictive drugs in disabling the stingy, harsh dopamine critic.

Another important discovery in instrumental conditioning was that more complex behaviors can be built up from simpler elements through the process of **shaping**.  This is the technique used to get circus animals to perform their complex tricks, for example, and is often used in scientific research with animals to study more difficult cognitive tasks.

Finally, it is important to recognize the difference between a **primary** vs. **secondary reinforcer**.  A primary reinforcer directly satisfies a biological need (e.g., food or water), while a secondary reinforcer is indirect, and must be learned.  Money, points, and gold stars are common examples of secondary reinforcers, which are effective for motivating people to do things.  Interestingly, animals typically require primary reinforcers, but people readily learn to value secondary reinforcers.  This ability to value initially arbitrary stimuli is essential for modern economic life -- it would be rather inconvenient to have to directly exchange food, water, or other items of direct value.

## Motivation

Despite the satisfying modern synthesis between dopamine and the behaviorist-era conditioning phenomena, this overall view of behavior focuses almost entirely on **external / extrinsic** factors (reward / punishment) to the exclusion of **internal / intrinsic** factors such as goals, drives, desires, etc.  This is consistent with the behaviorist-era prohibition on considering internal factors more generally, but we should have no such constraints on our modern thinking about this topic.  Nevertheless, the current neuroscience-based research still carries some of this extrinsic bias, with the central role of internal factors having been somewhat less emphasized.  By contrast, researchers in the field of social psychology have a long tradition of thinking about the central role of goals, desires, emotions and mood on behavior.

Before exploring some of these ideas, it is interesting to ponder the state of mind of a behaviorist from the 1920's: did they really think that their *own* personal behavior was fully determined by external rewards and punishments?  Were they not aware of having internal goals that drove them to torture rats for long hours, day after day, in pursuit of such ineffable, remote rewards as scientific understanding and a chance of prestige and fame?  The tangible rewards associated with scientific research are sufficiently distant and improbable, while the immediate working conditions involve relative poverty and extreme hard work, that it is really hard to understand why people would do such a thing in terms of purely external rewards.  Instead, there must some significant long-term internal forces driving such "crazy" pursuits, which are evident across many domains of human endeavor.

![**Figure 5.8:** Drive reduction theory according to Hull, 1943.  Basic needs create drives when those needs are not satisfied, and behavior is then recruited to satisfy those drives. ](figures/fig_drive_reduction_hull.png){ width=70% }

One of the few types of internal state that behaviorists did consider was the notion of a *drive* or state of internal discomfort (e.g., due to lack of food or water) that then motivates behavior toward reducing that discomforting state [@Hull43] (Figure 5.8). But this **drive reduction** theory has trouble accounting for motivations such as our desire to learn and work, which don't really seem to be associated with discomfort-reduction processes. 

![**Figure 5.9:** Maslow's hierarchy of needs. Higher-level needs are only considered once lower-level ones are satisfied.](figures/fig_maslow_hierarchy_needs.png){ width=80% }

A more comprehensive theory of motivation was developed by *Abraham Maslow*, at around the same time as Hull [@Maslow43].  Maslow's **hierarchy of needs** (Figure 5.9) captures the intuitive idea that higher-level needs are not relevant unless the more basic needs essential for survival are satisfied.  The two lowest levels in the hierarchy are physiological needs (breathing, food, water, etc) and safety.  Once those are satisfied, then higher-level needs such as love and belonging and esteem become relevant.  Finally, at the highest level, Maslow put *self actualization*, which includes things like morality, creativity, and lack of prejudice.  Interestingly, this highest level resembles the Buddhist notions of enlightenment, where one transcends lower-level attachments and needs, and can act in a more principled, rational, and yet spontaneous manner.  These frameworks capture the subjective feeling that we are controlled by our basic needs, and yet we yearn to be free from these low-level demands.

One problem with Maslow's theory, shared with any theory that attempts to articulate universal features of human behavior, is that people are rarely so compliant, and regularly violate his strict hierarchy.  For example, people have been known to literally work themselves to death, including recent cases of video gamers playing to death as a result of neglecting basic bodily needs.  Furthermore, teenagers routinely risk their personal safety in order to show off and otherwise enhance their social belonging and perceived self-esteem.  Nevertheless, as a general tendency, the hierarchy makes sense, and certainly the numerous cases of cannibalism in the face of extreme hunger suggest the power of these more basic physiological needs.

### Goal-driven Behavior

A more general motivational framework is based on the notion of *goals* and the idea that people are specifically motivated to achieve their goals.  These goals can be highly diverse in their specifics, but they share the common property of delivering positive reward signals upon goal completion (e.g., "the satisfaction of a job well done"), or even progress toward goal completion ("almost there, just around the corner.."), and corresponding negative states associated with failure (disappointment, embarrassment, lack of self-esteem).  Many aspects of goal-driven behavior have been studied over the years [@Tolman48; @MillerGalanterPribram60; @Powers73; @Klinger75; @Gollwitzer93; @CarverScheier90].

In the animal behavioral tradition, goal-driven behavior has been studied in the context of paradigms such as **satiety** and **devaluation** [@BalleineDickinson98].  In these cases, an animal is instrumentally conditioned to press one lever for food while in a state of hunger, and is then given as much food as they want.  They are then put back into the box with the lever -- if behavior is driven by purely *habitual* stimulus -- response associations, they should push the lever even if they are no longer hungry.  However, if they are actually thinking about the outcome produced by pressing the level, and recognizing that they don't want that outcome, then they should not press the lever.  Interestingly, results show that damage to the ventral and medial areas of prefrontal cortex (**vmPFC**) cause rats to press the lever even when they are full.  The same kinds of results have been shown in the devaluation studies, where the food is subsequently paired with a bitter taste outside of the lever-pressing context, so that it is no longer desirable.  If the animal still presses the lever, then they aren't clearly representing the outcome of the lever press.

The importance of the vmPFC brain areas for goal-driven cognition is consistent with neural data showing that these areas (in particular the *orbital frontal cortex*, *OFC* and *anterior cingulate cortex*, *ACC*) have many neurons that anticipate the possible US outcomes associated with a given situation and actions taken within that context, and impair goal-directed behavior when damaged [@RudebeckWaltonSmythEtAl06; @WallisKennerley11].  More generally, this is consistent with the overall role of the prefrontal cortex in driving goal-driven controlled behavior, which requires the tight coordination between plans and their potential outcomes in order to decide on the plans that will lead to the most desirable potential outcomes.

As discussed in the Neuroscience chapter, these vmPFC areas are directly interconnected with the basal ganglia, amygdala, and dopamine system, forming the overall *control* and *decision-making* system of the brain, and each of these areas plays a critical role in supporting the overall emergent ability to behave in a goal-driven, controlled manner.  Furthermore, as we'll see in the clinical disorders chapter, these are the brain systems that are implicated in most of the major clinical disorders.

Finally, one of the most fascinating and important demonstrations of the importance of intrinsic motivation comes from studies showing that giving people extrinsic rewards can actually *undermine* intrinsic motivation [@DeciKoestnerRyan00]!  For example, giving kids awards for drawing actually caused them to draw less than kids who did not receive these awards.  These results are controversial, however, and systematic reviews of the literature have reached opposite conclusions [@CameronBankoPierce01].  One of the most important factors appears to be whether the task in question is actually reasonably strongly intrinsically motivating in the first place: there is stronger evidence of the undermining effect when the task has stronger intrinsic interest, compared to more "boring" tasks, for which external rewards might be useful.

## Emotion and Arousal

![**Figure 5.11:** Valence (positive vs. negative) vs. arousal (high vs. low activation) *circumplex* model.](figures/fig_emotion_valence_arousal.png){ width=60% }

The fact that the same brain areas involved in goal-driven motivated behavior are also the primary areas associated with emotion raises the important question as to the relationship between emotion and motivation.  It is somewhat difficult to provide a crisp, principled definition of *emotion*, which thus makes it difficult to arrive at a clear understanding of its relationship with motivational states.  Some widely-recognized properties of emotion are that it has some kind of distinctive, characteristic subjective feeling, is associated with physiological arousal at least to some extent, and that it drives associated behavioral responses.  It is also generally agreed that emotion should be biologically grounded, at least in the more "primitive" or basic level of emotions.

All of these properties are consistent with the idea that emotional states and motivations have a strong connection [@CardinalParkinsonHallEtAl02].  For example, one's overall *happiness* is most strongly associated with feelings of personal self-efficacy and control (along with interpersonal connectedness and belonging).  Likewise, feelings of *sadness* are strongly associated with disappointment, failure, and lack of control.  Thus, a simple overall hypothesis is that emotions are the subjective states associated with our core motivational systems.  Let's see how far this idea can take us, in understanding the full spectrum of emotional states.

![**Figure 5.12:** Six different basic emotions as represented by facial expressions: anger, disgust, fear, happiness, sadness, and surprise.](figures/fig_emotion_faces.png){ width=60% }

Figure 5.11 shows the simplest standard model of emotion, known as the **circumplex model**, which distinguishes between two separate dimensions of *valence* vs. *arousal*. Valence refers to the "sign" of the emotion, positive vs. negative, while arousal refers to the intensity of the emotion.  Anger and exhilaration are opposite valence but the same high level of arousal.  These two valences are also associated with opposing *approach* vs. *avoid* behavioral orientations, which have been identified as core opponent aspects of emotional / motivational states and corresponding personality dimensions [@CarverWhite94; @ReadMonroeBrownsteinEtAl10].

While this simple framework captures the most essential dimensions of affective / emotional states, it is likely that the valence aspect of emotion is considerably more complex than the *bivalent* (two valences) nature of the circumplex model.  For example, *Paul Ekman* found that there are 6 **basic emotions** that have clearly recognizable facial expressions, which are universal across cultures: anger, disgust, fear, happiness, sadness, and surprise [@EkmanFriesen76].  Later work added other emotions based on vocal and facial expressions, and Plutchik proposed a systematic wheel of emotions based on 8 emotion categories arranged in opponent pairs, with an arousal dimension as well (Figure 5.13).

![**Figure 5.13:** Plutchik's wheel of emotions, with arousal (intensity) represented as distance from the center along any of 8 different categories of opponent emotions.](figures/fig_emotion_plutchik.png){ width=80% }

In addition to the basic happy / sad elements of emotion, which may be more closely related to goal-driven motivational states, some of these other emotional states are more clearly social in nature, and the role of facial expressions and vocalization clearly implicates a strong social communication role for emotions.  Thus, we can potentially organize emotional states in terms of a set of distinct functional domains, where these states can serve to motivate people toward appropriate patterns of behavior.  We roughly organize these according to Maslow's hierarchy of needs, with slightly different groupings.

* **Physiological States:** **Hunger, thirst, pain, tiredness, lust,** and the **need to excrete** are all basic motivational states associated with core body functions necessary for survival, and correspond with the physiological level in Maslow's hierarchy.  These may not be considered "emotional" states per se, but they share the same properties of being strongly biologically determined, varying in level of intensity or arousal, and capable of driving appropriate behaviors to mitigate negative states and approach positive ones.  While hunger and thirst may not typically need to be communicated socially using basic facial expressions, tiredness and lust likely do, and have clear social cues in the form of yawning and flirting behavior.

* **Safety states:** **Fear** is the emotional correlate of Maslow's safety level, and has both a direct internal motivational role (driving you to avoid scary situations), and an important social communication role for alerting others of potentially dangerous situations, which is facilitated by the presence of the unique fear facial expression.  **Disgust** is an interesting case which may be more strongly social in nature: it is important to communicate to others that food might be rotten and disgusting, and it seems likely that this original function has been extended to apply to labeling the behaviors of others in the group as dangerous or otherwise something to be avoided.  **Hate** is an emotional state that is also clearly negative and social in nature, and associated with disgust: it is the emotional state and social communication associated with labeling others as belonging to the out-group.

* **Social states:** **Love** is the opposite of hate, and is the positive social affective state associated with members of the in-group.  It obviously corresponds with Maslow's 3rd level of love and belonging.  Other important social states include **dominance** and **submission** dynamics, along with trust and admiration, which have to do with establishing and perceiving relative status within the social order.  These correspond with Maslow's esteem level, and are very strong and often-overlooked motivational states for social beings, from dogs to monkeys to humans.  We do not necessarily have clear terms for these as emotional states (e.g., the feeling of being dominated by, or of dominating, a social other), but there is evidence that they are important factors in personality and interpersonal interactions [@HopwoodWrightAnsellEtAl13], and certainly we have terms such as "diss" = disrespect and "pissing contest" that refer to such interactions.

* **Goal-associated states:** many of the remaining states are associated with goal-driven behavior, including **happiness** and **sadness** (and their varying levels of intensity or arousal) as noted above, but also **anger** and **frustration** which are associated with impediments to progress toward achieving one's goals, and **curiosity**, **interest**, and **surprise** which are associated with recognition of new interesting avenues to pursue.  **Boredom**, **distraction**, **optimism**, and **anticipation** are also other states that clearly seem to be goal-related.  **Grief** and loss are perhaps not so obviously goal-related, but in some ways they reflect a profound disruption of one's sense of overall control and order in the universe, in addition to the basic feelings of missing a loved one. 

Thus, overall, it does seem that emotional states can generally be understood as corresponding to biologically-determined motivational states, which provides a clear functional story for why we have emotional states in the first place [@CardinalParkinsonHallEtAl02].  As such, this raises important questions about the standard "Hollywood" story about the special status of emotion as a unique aspect of human beings.  Under this motivational framework, many of our emotional states are common across all mammals at least, and represent a genetically-coded, low-level aspect of our brains, not something special and unique about humans.  On the other hand, because our emotional states are so strongly felt, and provide dramatic color to our lives, we regard them as special.

Also, emotion is what keeps us from harming each other (except when it is what drives us to harm each other, in the case of hate and anger), and the lack of basic emotional connections in psychopaths enables them to do horrible things that "normal" people would never do.  So, from a survival perspective, we really depend on everyone sharing these protective emotional responses, and anything that doesn't is immediately scary and foreign.  Furthermore, we do have a large portion of our vmPFC devoted to emotional processing, and these emotional representations are likely novel combinations of more basic, lower-level emotional states, shaped over our personal histories, and thus likely provide a much richer and elaborated emotional tapestry than found in other animals.

### Emotional / Motivational Encoding in vmPFC

![**Figure 5.14:** Map of ventral / medial frontal cortex (vmPFC) areas and their associated roles in emotional / motivational states, as a function of connectivity with subcortical areas that have established emotional / motivational valences.  The broad organization is consistent with the circumplex model, with separate positive (appetitive) and negative (aversive) areas, and a separate arousal area, along with other forms of specialization.  BLA = basolateral amygdala; CNA = central amygdala; PAG = periaqueductal grey; LH / MH / DMH = lateral / medial / dorsomedial hypothalamus.](figures/fig_macacque_medial_pfc_theory.png){ width=80% }

Figure 5.14 shows a map of what the vmPFC emotional / motivational tapestry might look like, based on tracing the inputs and outputs of these areas relative to lower-level emotional and motivational areas in subcortical areas [@OngurPrice00].  Consistent with the circumplex model (Figure 5.11), there are separable areas for positive vs. negative valence, and arousal.  Interestingly, the negative valence area, known as area 25 or subgenual ACC, has been implicated in major depressive disorder through the work of *Helen Mayberg* and colleagues, and electrical stimulation in this area is a promising treatment [@Riva-PosseChoiHoltzheimerEtAl14].

Also, consistent with the anatomical principles from the Neuroscience chapter, these areas relate to nearby areas in terms of the ACC areas at the top relating to motor plans coded in surrounding PFC areas, and OFC areas toward the bottom being driven by visual, olfactory, taste, and viceral inputs coded in nearby areas.  Thus, we can thing of ACC as being more associated with action planning, including things like effort and difficulty costs, while OFC is more important for representing outcomes in terms of their relevant sensory features (taste, appearance, etc).

### Biological Grounding of Emotion and Arousal

Finally, there is a somewhat strange history of thinking about emotion that is typically emphasized in introductory textbooks, and seems to reflect the desire to understand emotional states as special, biologically-grounded, important states.  Specifically, William James and Carl Lange each independently proposed that emotion arises first in our bodily responses such as sweating, heart racing, etc, and is only later recognized as an emotional response as a direct result of these initial physiological responses.  In contrast to this **James-Lange** theory, Walter Cannon and Phillip Bard proposed that higher-level processes in the brain play a critical role in driving our emotional experiences.  Finally, Stanley Schacter and Jerome Singer argued that both physiological and higher-level interpretational processes were both essential, with their **two-factor theory**.

Ultimately, all of these theories still emphasize that emotional states have both physiological and higher-level interpretational aspects, and the unique, interesting aspect of emotion is that it can activate the body in ways that purely abstract mental states do not.  From a modern perspective, it is clear that many different brain and body responses occur essentially in parallel, producing our rich, complex, and fascinating subjective experiences of emotion.

One final issue concerns the optimal level of arousal for driving motivated behavior.  The **Yerkes-Dodson law** [@YerkesDodson08] established the principle that there is an optimal level of arousal somewhere in the middle between low and high levels, following an **inverse-U-shape** curve. This same curve has been found for levels of dopamine as well.  You may have experienced this in experimenting with different levels of caffeine -- too much is actually not productive, as you get too hyper and unable to focus.


## Summary of Key Terms

This is a checklist of key terms / concepts that you should know about from this chapter.  As we'll learn in the memory chapter, it is a great idea to test yourself on what was said about each of these terms, and then go back and double-check -- that provides both beneficial repetition and also the *testing effect*.

* Synaptic Plasticity
    + Hebbian Learning
    + Long Term Potentiation (LTP)
    + Long Term Depression (LTD)
    + 5 Steps of NMDA / Ca++ synaptic plasticity
    + What determines LTP vs. LTD direction of synaptic plasticity?
    + Error backpropagation

* Classical Conditioning
    + Conditioned stimulus (CS)
    + Unconditioned stimulus (US)
    + Un/conditioned response (U/CR)
    + Dopamine responses to CS, R, no-R in conditioning expt
    + Rescorla-Wagner learning rule / reward prediction error model of dopamine
    + Extinction, and its context sensitivity: spontaneous recovery, reinstatement, renewal

* Operant / Instrumental Conditioning
    + Thorndike's law of effect
    + Implementation thereof in terms of dopamine effects on Go / NoGo
    + Actor / Critic model
    + Partial reinforcement and gambling
    + Shaping to build up complex behaviors
    + Primary and secondary reinforcers

* Motivation
    + External (extrinsic) vs. internal (intrinsic) motivation
    + Drive reduction
    + Maslow's hierarchy of needs (levels of hierarchy)
    + vmPFC in satiety / devaluation effects

* Emotion and Arousal
    + Circumplex model
    + Six basic emotions according to Ekman's original faces
    + Relationship between emotion and motivation
    + Importance of both physiological and higher-level interpretations for emotion
    + Yerkes-Dodson law



