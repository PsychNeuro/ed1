---
bibfile: ccnlab.bib
---

# Chapter 4: Sensation, Perception, and Attention

Channeling my hard-boiled teenage son, he would say: "Dad, why are you writing a whole `!@$#%ing` chapter about *seeing* --- you just look and you *see* `sh*t` --- what's the big deal!"  This is, actually, quite accurate.  I mean, it is what he would say (this has been confirmed), and it is also what makes this chapter difficult to write: we all just take perception for granted, because from our subjective perspective, it does just happen, preconsciously, and we are only aware of the results.  From a *compression* standpoint, that's about all you need to know, right!?  It just works, so get on with it!

Nevertheless, despite the potential futility of the exercise, I will persist in trying to convince you that perception is amazing and fascinating, and give you some sense of how it works, and why roughly 50% of your massive neocortex is required to solve this "trivial" problem.  We'll start by understanding the key *compression* process that gives us such a simple, direct perception of the world, and how we can start to pull back the curtain on this amazing wizard, to see some of its deeper secrets, using the magic of *illusions*.  Then, we'll discuss many aspects of sensation and perception in the context of our dominant sense: vision, followed by discussion of the remaining sensory systems.  Finally, we'll consider how *attention* emerges from the basic properties and dynamics of neurons in the brain, and how important it is for further compressing and filtering the firehose of incoming sensory signals, to capture just the right information for whatever it is that we're currently trying to do.

## Perception is (Hierarchical) Compression

The tendency to underestimate the complexity of perception has been around for a long time: there is a famous story about how, at the dawn of +AI research in the 1950's, a random graduate student was tasked with solving the vision problem over the summer, so they could plug it into the rest of the system next year.  Needless to say, it didn't happen, and in fact it is only in the last 5 years or so that AI systems finally have semi-functional perceptual front-ends.  You have likely experienced these AI systems in the speech recognition domain, when talking to Siri or other similar digital assistants: they often work but still make some basic mistakes, and they *definitely* don't seem to really understand what you are saying at any deeper semantic level, but that's another issue.

The trick to getting these AI systems to finally work was to adopt the strategy that the brain uses, by employing large networks of simulated neuron-like processing elements, organized over many +hierarchical layers (i.e., "deep" [neural networks](#neural-network)).  These networks are trained by a learning mechanism known as +backpropagation, which was developed by psychologists in the 1980's to better understand many properties of human cognition and learning [@RumelhartHintonWilliams86; @McClellandRumelhart86].  A few important computational tricks made these networks work better and faster [@LeCunBoserDenkerEtAl90], and the advent of fast computer chips developed for video gamers enabled these networks to be dramatically scaled up in size, resulting in significant performance improvements [@KrizhevskySutskeverHinton12; @LeCunBengioHinton15].

![**Figure 4.1:** Hierarchical organization of detectors in the visual pathway going into the temporal lobe, supporting the ability to recognize (detect) entire objects, based on earlier levels detecting parts and features of parts.  This shows the large-scale, cumulative effects of *compression* from very high-dimensional raw sensory inputs, to high-level, succinct interpretations of the world.  Although a highly simplified cartoon, this roughly captures the nature of the process actually taking place in the brain.](../figures/fig_category_hierarch_dist_reps.png){ width=100% }

The essential strategy learned by these deep neural networks, and the brain, is shown in Figure 4.1 (we already saw this figure in Chapter 2), where each layer **compresses** the complexity of the patterns on the layer before: *getting rid of irrelevant differences, while extracting the important ones that the system actually cares about.*  This is the essential function of perceptual systems, served by the 10,000-to-1 compression property of individual neurons that are detecting relevant patterns and ignoring irrelevant ones.

To use another metaphor, you can think of perception as a *filter*, filtering out irrelevant "junk" from the perceptual input signal, and purifying the relevant, important stuff.  It takes multiple layers of such filters because one step of filtration can only do so much purification, and each such layer builds on the partially-purified outputs of the layer before.  This is why it takes so many neurons in the brain, and in the AI models, to do a good job at perception --- each individual neuron can only do a small part of the overall job.

## We See the "Real" World, not Raw Sensation

Our subjective, conscious experience is dominated by the higher levels of this hierarchical perceptual filtering system, because that is what most strongly and directly interconnects with all the other brain areas, and this process of bidirectional communication and influence across brain areas is what drives consciousness as we discussed in the previous chapter.  These higher layers are called **association cortex** because they "associate" with all these other brain areas.

![**Figure 4.2:** The doors of perception, as represented by the author in a computer painting from college, depicting the hidden elemental nature of visual input and other potentially mysterious elements, including a mirror on the table in the foreground, reflecting the distant sky, but inviting the viewer to reflect on what is real in our perceptions.](../figures/fig_the_answer.png){ width=50% }

This is the reason for my son's hard-boiled attitude about perception: by the time we're aware of it, perception has already done all the hard filtering and compression work for us, and we just experience this nice simple impression of what is out there in the world!  Interestingly, we nevertheless seem to retain a bit of a sneaking feeling that our perceptual systems might be hiding something interesting from us, e.g, in the popular notion of the "doors of perception" being blown wide open by psychedelic drugs and other such experiences (e.g., Figure 4.2).  Somehow, we feel like we want to be able to break through all those filters, and see the world as it "truly is".

However, the truth (represented by the mirror on the white table in Figure 4.2) is that our perceptual systems do a really amazing job of delivering a highly accurate representation of the world --- there is no greater truth than what your eyes deliver to you, tirelessly, every moment.  So look in the mirror and behold the truth!  And, if you really want to get a different perspective, try a magnifying glass or, better yet, a microscope.  That will reveal the separate Red, Green, Blue (RGB) dots in your laptop or cell phone display, which combine at different strengths to give you a perception of different frequencies of light.  We'll discuss how this works later in the chapter.

![**Figure 4.3:** Illustration of color constancy, and more generally how we see the world, not the raw sensory signals.  The raw pixels in A and B are identical, and yet we see them as strikingly different shades, based on all the "contextual" information about shadow and the regular checkerboard pattern, etc.](../figures/fig_shadow_contrast_cylinder.png){ width=50% }

In perceptual science, the fact that our perception is of the world, not the raw sensory signals that our perceptual systems receive from our sense organs, is referred to as **perceptual constancy** in general, with more specific versions such as **color constancy**, **size constancy**.  Figure 4.3 (which we saw already in the Introduction) provides a powerful demonstration of color constancy, where the exact same raw pixel RGB values are perceived as strikingly different shades, based on our ability to integrate the various elements of the scene into a coherent overall interpretation, including the effects of lighting and shadows, and the regularity of the checkerboard pattern.

![**Figure 4.4:** a) "The Dress", which sparked a viral internet sensation in February, 2015, because different people have strongly divergent perceptions of the dress colors.  Some see it as black and royal blue, and others as white and gold.  b) Shows an explanation for the two different interpretations, with a gold vs. blue filter over the two different colors of underlying dress --- these filters correspond to different assumptions your brain makes about the overall lighting in the picture, which is really behind the individual differences.  Similar to Figure 4.3, it is hard to convince yourself that the colors within these filtered boxes are actually identical across the left and right boxes, but they are. (Figure design by Kasuga~jawiki; vectorization by Editor at Large; "The dress" modification by Jahobr, CC BY-SA 3.0, [wikimedia.org](https://commons.wikimedia.org/w/index.php?curid=59279133.))  c) The "objective" RGB colors sampled from the original image, which in isolation are clearly gold and blueish.](../figures/fig_the_dress_explained.png){ width=100% }

Perhaps the most striking and popular demonstration of the divergence between raw sensory input and subjective perception is from "The Dress", which was a viral internet sensation in 2015 (Figure 4.4), because people experienced very divergent yet strongly felt percepts of the dress colors.  From a raw RGB perspective (Figure 4.4c), the dress colors are gold and blue, but studies show that 57% of people see it as black and blue, while most of the rest see it as more white and gold.  Figure 4.4b shows how two different color filters (gold on the left, blue on the right) produce identical stimulus-level color values for the two different dress colors that people report seeing.

In this case (and always), the visual system is just making different **assumptions** about the lighting conditions present in the picture, and because the cues for these lighting conditions are ambiguous, different people's brains make different assumptions.  If your brain assumes that the dress is in shadow and strongly backlit, the blue filter applies (and you see the dress as white and gold), whereas if you assume it is brightly lit by direct sunlight, the yellow filter applies (and you see it as blue and black).  Critically, your subjective percept is irrevocably "colored" by those assumptions, in an attempt to tell you what the *real* underlying color of the dress is, independent of the specific lighting conditions.

This is the way in which your perception is more real than your raw sensation --- most of the time it is amazingly accurate in telling you what the *real* materials (and shapes, sizes, etc) of things are in the world.  And if you see the dress as blue and black, apparently you see it as it really was, but one of the most striking aspects of this picture is that it is very hard for people to switch to seeing it the other way, and I personally am forever stuck on white and gold.

The fact that our perceptual systems need to make these assumptions sets them up for the inevitable *ass-u-me* situation (i.e., making an ass out of u and me): **illusions** reveal the nature of these assumptions, and are thus a fun and informative way to understand how the perceptual system works, under the hood.  Some lucky researchers basically spend their entire day just coming up with new illusions (although this is increasingly less viable of a career path these days, as the chances of finding truly new illusions are dwindling).  Anyway, this is certainly the approach we're going to take for the rest of the chapter, so buckle up and get ready to see some crazy stuff!

Before we go there, however, it is important to point out that the current generation of AI models discussed above almost certainly do *not* "see" the world in the way we do.  There is nothing in the way that these models are trained that would cause them to make the same kinds of assumptions necessary for the color constancy demonstrated in the above figures.  Furthermore, they are typically strictly +feedforward in their processing of the sensory input --- raw image pixels go straight up the hierarchy, without any higher-level interpretations of the scene coming back down to influence the way these lower levels process things.  By contrast, the mammalian visual system is massively [bidirectionally](#bidirectional) connected, and the +top-down connections from higher levels to lower levels is critical for enabling the overall scene-level information to so strongly affect our basic perception of the elements of the scene, as is so well demonstrated in Figure 4.3.

Extensive neural evidence shows how these top-down processes shape the firing of neurons throughout the visual system, all the way down in primary visual cortex (V1) [@LeeYangRomeroEtAl02; @AngelucciBressloff06].  And there are some neural network models with bidirectional connectivity, which demonstrate simple versions of these kinds of top-down phenomena [@OReillyWyatteHerdEtAl13], but considerable more work needs to be done to capture something like Figure 4.3.  There is a further important question for AI models of perception: what kind of learning signal might cause the model to encode the stable features of the world instead of the raw sensory features?  One idea is that the brain may learn by trying to predict what will happen next, and the stable, predictable things in the world are therefore what is learned (more on this in the learning chapter).  By contrast, existing AI models are typically trained to label the category of objects in a scene, using massive human-labeled datasets --- in effect, they are just learning to imitate one small part of the human perceptual filtering process.

## Sensory Systems

Now that we have a sense of some of the big-picture issues and challenges in perception, we will dig into some of the more specific details about different sensory systems, so we can understand how it all works.  As you undoubtedly already know, we have 5 major sensory modalities, which we have reliable and salient conscious awareness of.  However, there are two other important sensory pathways that are critical for motor control, that we are not as aware of, likely because they are typically activated by our own motor actions, so we don't experience them as distinct from these actions.  A number of other sensory modalities exist in other animals, including echolocation in bats, whiskers in many different types of animals, and magnetic sensing in a range of animals, especially birds.

![**Figure 4.5:** The 5+2 main sensory modalities for people (first 5 are consciously salient, last 2 less so), the stimulus that activates the corresponding receptors, and the pathway through the thalamus into cortex.  LGN = lateral geniculate nucleus; MGN = medial...; VPN = ventral posterior nucleus; PMN = posterior medial nucleus; VPS = ventral posterior, superior nucleus.  The absolute threshold suggests how sensitive our receptors are (most animals are much more impressive in the olfaction department) ](../figures/fig_sensory_modalities_table.png){ width=100% }

Figure 4.5 shows the basic facts about each of these sensory modalities, including what physical **stimulus** activates them, the names of the receptor(s) that **transduce** (convert) this stimulus into a neural signal, and how that neural signal makes its way into the cortex by way of the thalamus.  Interestingly, our sense of smell, which is evolutionarily the most ancient sense, present in even the most primitive beasts in the ocean, bypasses the thalamus and jacks straight into the cortex, in a brain area that is close to the hippocampus.  This may explain why odors can be such powerful memory triggers, as famously captured in Marcel Proust's *Remembrance of Things Past*.

The **subcortical** part of the sensory pathway, prior to the arrival of the neural signal in the thalamus and cortex, performs significant **preprocessing** of the signal using complex, sophisticated, and evolutionarily pre-wired circuits.  For example, the subcortical auditory pathways have very fast spiking neurons that can process the tiny time differences between when a sound arrives in one ear versus the other, to extract the angle of the sound source relative to the head.  This angle signal is then sent up to the cortex, which otherwise is way too slow to be able to pick up on these fine timing differences.  Many other forms of initial processing of the sensory signal are performed across different modalities, as we discuss below in the context of each such modality.  In general, these subcortical processing steps also perform the first pass of **compression** and **contrast**, which also makes the cortex's job significantly easier.  Many of these forms of subcortical processing are well-enough understood to be implemented in signal processing toolkits implemented in computer software.

The role of the thalamus in the sensory pipeline is somewhat less clear.  A unique feature of the thalamus is that the neurons there have essentially no direct connections amongst themselves, and generally seem to just pass the signal through to the cortex relatively unchanged (hence its characterization as a *relay*).  However, the thalamus receives massive numbers of top-down connections from its corresponding sensory cortex (e.g., more than 90% of connections in the LGN come from V1), suggesting that a major function of the thalamus is in supporting *top-down modulation* of the incoming sensory signal.  For example, this can support **attention**, where these top-down cortical signals "shine a spotlight" on a subset of incoming sensory signals --- we return to this topic later in the section on Attention.

## Vision

![**Figure 4.6:** The visual pathway.  Light rays reflect off of objects, which could be either in the left or right visual field, and proceed through the lens of the eyes, onto the retina at the back of the eyeball.  The photoreceptors there transduce the light into electrical signals, which are then transformed through a relatiely complex network within the retina itself into the firing of the retinal ganglion cells.  These cells send their axons to the LGN of the thalamus, crossing over at the optic chiasm so the full left visual field ends up going to the right hemisphere, and vice-versa.  The LGN neurons then communicate the visual signal up to area V1 in the very back of the brain, in the occipital lobe, and from there it makes its way back forward up the hierarchy of compression and filtering as shown in Figure 4.1.](../figures/fig_vis_optic_path.png){ width=50% }

Figure 4.6 shows the pathway of visual information, from light rays reflecting off of objects in the world, that are then focused by the lenses of your eyes onto the **retina** at the back of the eyeball, where the light is transduced (converted) from photons into electrons by **photoreceptor** cells (rods and cones), in very much the same way that the camera in your cell phone does it.  Speaking of which, your eye has roughly 120 megapixels (i.e., 120 million photoreceptors), but they are not distributed uniformly as they are in your camera (Figure 4.7).  Instead, there are many more **cones** (color sensitive photoreceptors) in the center of your retina, called the **fovea**, with the density falling off rapidly as you go out from this center into the **perhiphery**, where the monochromatic, motion-sensitive **rods** predominate.  If you do some math about *visual angles* as shown in Figure 4.7, the fovea can resolve about 300 dots per inch (dpi), which is the resolution of current high-res displays like those found on most higher-end cell phones.  Thus the "retina" marketing from Apple is actually accurate.

![**Figure 4.7:** How many megapixels is your eye?  First, unlike a camera, the photoreceptors are concentrated in the very center of your retina, called the **fovea**, which is also where most of your color-sensitive **cones** are (left panel).  Out in the **periphery**, monochromatic **rods** predominate --- they are also better at detecting motion, so you are actually better at seeing things move when you're not looking right at them.  The right panel shows that we can resolve about 300 dots per inch (dpi) at a distance of 10 inches, which is not coincidentally the "retina" screen resolution on modern high-res displays, and even on old-fashioned laser printers. ](../figures/fig_vis_retina_cone_rod_dpi.png){ width=100% }

While the rods do not resolve multiple different colors, and have lower resolution, they are much better at detecting **motion**, and thus it can be useful to actually look away from something to detect motion better.  Cats seem to know this trick, and will look away from the mouse playing dead (or the cat toy playing dead, more likely), to better detect when it starts to move.  Or maybe they are just communicating disdain.  Hard to know with cats.

There are several crazy-but-true things about the visual system:

* The rod and cone photoreceptors are at the very *back* of the retina, behind a bunch of extra pre-processing circuitry that does the compression and contrast enhancement.  There are also blood vessels all over the place.  Apparently they don't end up blocking the light too much, or distorting its path, but still.  Also, as shown in Figure 4.7, there is a huge chunk of visual space, about 1 degree of visual space in diameter (same as the fovea), which has no photoreceptors at all!  This is the so-called **blind spot** where the axons for the optic ganglion cells (i.e., the *optic nerve*) are all gathered and head back to the thalamus.  

    You can find this spot (if you haven't already), by moving your outstreched arm with thumb pointed up, out to an angle of about 12:30 or 1 o'clock (where 12 is straight ahead), and noticing where, now that you're paying attention, you see it disappear!  You don't notice it normally because, as we've said already, your perception is about the world, not your raw sensory signals, so your brain just papers over that little hole there for you, using all the signals surrounding it.

* The photoreceptors are constantly active (*depolarized*), and light actually *inhibits* them, instead of turning them on, as you would otherwise expect.  Why this is the case seems to still be a mystery --- it is not true across the animal kingdom, so it could go the other way.  Interestingly, there are *many* instances like this across the brain, where neurons are *tonically* (continuously) active and then get inhibited by relevant signals.  Although you would think this would cost a lot of energy, and the brain does consume about 1/3 of the human body's energy budget, the actual amount of energy required to sustain neural firing is likely a small fraction of this total cost (all the maintenance and upkeep and building of synapses, etc is likely much more expensive).

* Everything in the retina is upside-down and backward relative to the world (Figure 4.8).  This is a result of basic optics, as evident in Figure 4.6 tracing the light rays from the world, through the lens, and into the retina.  Do you need any more convincing that we see the world and not our raw sensory stimulus?  Through experience in the world, we quickly learn the very systematic relationship between patterns of neural firing in the retina, and the direction of gravity, etc, and this is then what we perceive.  This lesson is really important more generally: all neurons communicate with spikes that are essentially identical to those from any other neuron --- they do not come with extra "annotations" indicating things like "up" or "down" --- so each neuron has to learn *de novo* the meaning of each of its inputs in relation to all the others.  This is also why it is actually quite possible that you are a brain in a vat (as in the *Matrix* movie) --- everything we know is encoded in these patterns of neural spiking, and we only make plausible inferences about the most "reasonable" interpretation of all these patterns.

    Also, if you want to know why mirrors only flip left and right, but not top and bottom, check out this video by [Physics Girl](https://www.youtube.com/watch?v=vBpxhfBlVLU) who provides a comprehensive explanation.  Basically, mirrors only reflect, they don't flip anything --- we just get confused when looking at a reflection, because it shows us as we would look if we were facing ourselves from a point on the other side of the mirror.  Careful not to fall into the looking glass!

![**Figure 4.8:** Our raw visual inputs are upside-down and backward relative to the real world, due to the basic optics of lenses --- this is is also true of a camera.  Because we see the world, not the raw sensory signal, we learn the relationships between these visual signals and other reliable features of the world, like gravity.  Indeed, all neural signals are ompletely arbitrary in the first place --- there is nothing "up" or "down" about a pattern of neural firing --- it is just spikes!](../figures/fig_vis_retina_image_inverted.jpg){ width=50% }

### Compression and Contrast in Vision

![**Figure 4.9:** Circuits of dynamic pre-processing within the retina itself, which transform the raw transduced light signals by center-surround contrast coding.](../figures/fig_vis_retina_on_off_center.png){ width=50% }

![**Figure 4.10:** Diagrams of the receptive fields for on-center vs. off-center versions of center-surround cells.  The receptive field (RF) refers to the pattern of illumination across the retina (or out in the world, for a given fixed position of the eyes) that directly influences the firing of a given neuron.  For the on-center neuron, it is excited by light in a smaller central region, and inhibited by light in the wider surround region, while the off-center is the opposite.  This center-surround organization is critical for *compression* by reducing or eliminating firing where the light is constant across the entire RF (the excitation and inhibition cancel out).  Such neurons only respond to regions of *contrast* in the image, where there is a transition (edge) between a darker and lighter level of illumination.  Thus, our visual systems mainly encode the regions where light and dark are changing --- i.e., regions of *contrast* in the image --- not the relatively uninteresting uniform regions in between.](../figures/fig_on_off_rfields_edge.png){ width=50% }

Despite all the weirdness, the retina works really well, and one of the most important things it does is to **compress** and enhance **contrast** in the visual signal, before it is converted into neural spikes and sent through the **retinal ganglion** cells up to the LGN of the thalamus.  Figure 4.9 shows some of the preprocessing circuits that achieve this compression and contrast enhancement, through a feature known as **center-surround** contrast, which is illustrated in more abstract form in Figure 4.10.

Specifically, there are two main types of these center-surround signals that emerge out of the retina: **on-center** and **off-center**.  The on-center type of retinal ganglion cell gets excited by light hitting the central region of the target-like **receptive field** shown on the left of Figure 4.10, and they are inhibited when light hits the wider outer-ring (the *surround*).  The off-center is the opposite: it is excited by light in the surround, inhibited by light in the center.

Figure 4.10 illustrates the main consequence of these opposing patterns of center-surround excitation and inhibition: if there is a uniform, consistent amount of light across both the center and surround, the excitation and inhibition cancel out, and the cell will not fire!  Only when there is some kind of transition or *edge* where more light falls on the center relative to the surround, or vice-versa, will these cells fire.  Thus, the retina is already doing a huge amount of *compression* before sending signals up to the cortex, by filtering out regions of uniform illumination.  These are the blank walls and blue sky in our visual inputs, and they are not where the interesting stuff is, so it is a good idea to filter this stuff out, and focus processing specifically on the *edges*.

![**Figure 4.11:** Simple cells in primary visual cortex (V1) combine multiple center-surround inputs from the LGN to form *edge detectors* that encode a consistent, elongated edge or transition in illumination across an image.  This is referred to as the *classical receptive field* for V1 neurons, and its discovery by Hubel and Weisel in the 1950's and 60's won them the Nobel prize!  The JPEG compression technique for pictures works by extracting these same kinds of oriented edges of contrast in images, and it greatly reduces the number of bits of information needed to encode large images.](../figures/fig_v1_simple_dog_edge.png){ width=30% }

The primary visual cortex (V1) builds on the center-surround signals coming from the LGN (which passes them along from the retina more-or-less intact), to detect more elongated, oriented edges of light / dark *contrast* in the image.  Hubel and Wiesel discovered this property of V1 neurons by recording from V1 neurons in anesthetized cats shown simple images of oriented bars of light, and won the Nobel prize in 1981 for this and other discoveries about the visual system.  Here's a [YouTube Video](https://www.youtube.com/watch?v=KE952yueVLA) about their work, and a more modern approach using [reverse correlation](https://www.youtube.com/watch?v=n31XBMSSSpI).  These oriented edge detectors are shown in Figure 4.1 as the starting point for even more complex patterns detected at higher layers in the visual system.

Thus, in addition to seeing upside-down and backward, we mainly see the outlines or edges of things, and essentially assume the continuation of surfaces in between these edges.  This may explain why we so readily process line drawings, which provide a good "illustration" of what our higher visual areas are largely processing.  The efficiency of encoding the visual world in this way is demonstrated also by the widely-used JPEG compression standard, which greatly reduces the size of image files.

In summary, vision provides clear, well-understood examples for how the brain compresses incoming signals to extract features that will be of greatest value for subsequent stages of processing.  In this and many other cases, this compression occurs by focusing on the key points of *contrast* --- where things are changing.  This principle of contrast also applies to the time domain as well --- all stages of visual processing are also particularly sensitive to changes over time, such as the onsets and offsets of illumination.  This is particularly true of the rod-driven motion processing pathways, which dominate in the cat visual system, and explain why cats are so captivated by moving laser pointer dots.

### Color Contrasts

![**Figure 4.12:** Color is encoded by receptors sensitive to different wavelengths of light (just barely different in the case of M and L) which are paired into two sets of opponents: red (L = long wavelength) vs. green (M = medium wavelength) and blue (S = short wavelength) vs. yellow, which is created by the combination of red and green, and is not provided by its own separate photoreceptor.  Luminosity (black vs. white) is coded by integrating across all receptors, including rods which have a more blue-shifted tuning.](../figures/fig_vis_color_tuning_opponents.png){ width=70% }

In addition to coding illumination contrasts at edges, the visual system is also tuned to contrasts between different wavelengths of light, which forms the basis for color vision (Figure 4.12).  Color is efficiently encoded in the brain by using only three different types of cone photoreceptors, which can span the entire spectrum of visible light by mixing these three elements in different relative strengths, just as a painter can mix a wide range of colors from a small set of primary colors.  Although we commonly think of these **primary colors** as Red, Green and Blue (RGB), Figure 4.12 shows that the Red and Green detectors are surprisingly overlapping in their response to different frequencies.  For this reason, and because of their wide tuning, scientists refer to these photoreceptors as L = long wavelength (Red), M = medium wavelength (Green), and S = short wavelength (Blue), but we'll stick to the more familiar primary color names.

For the same reason that center-surround coding produces effective compression and contrast-enhancement, the brain also encodes color using color contrasts, using two **opponent** pairs of colors: Red vs. Green, and Blue vs. Yellow.  Indeed, these color contrasts are often superimposed with the center-surround contrasts, such that e.g., the center responds to Red and the surround Green.

![**Figure 4.13:** Rotating snakes illusion from Akiyoshi Kitaoka, which depends on eye movements interacting with blue-yellow opponent color coding.](../figures/fig_vis_rotating_snakes_illusion.jpg){ width=70% }

![**Figure 4.14:** Moving snakes illusion from Akiyoshi Kitaoka.](../figures/fig_vis_moving_snakes_illusion.jpg){ width=50% }

A number of visual illusions reveal the underlying color opponency at work in our visual systems, such as those in Figures 4.13 and 4.14, which interact with the constant motion of our our eyes to create apparent motion where there is none (see [Akiyoshi Kitaoka's Web Page](http://www.psy.ritsumei.ac.jp/~akitaoka/ICP2016.html) for many other such demonstrations).  Other illusions involve staring at one set of colors and then looking at a white page, wherein the fatigue caused in the one pair of the opponent allows the other to get a bit more active, driving its perception.  [This illusion](https://www.youtube.com/watch?v=gur-_IGV7F8) provides a compelling set of illusory percepts based on red vs. green opponency.

Despite the presence of these "oopsie" cases in carefully-crafted illusions, the color opponent system normally enables us to make highly accurate inferences about the "real" colors of different objects, compensating for the impact of different lighting conditions.  This is known as *white balance* in photographic terms, and in the brain it depends on the ability of e.g., a preponderance of yellow activation from yellow lighting producing a compensatory accentuation of the blue opponent, which we could see at work in "The Dress" illusion discussed at the outset.

### Depth

Our retinas only provide a 2D window onto the full 3D world, so we have to rely on a number of assumptions to reconstruct that missing 3rd dimension --- these assumptions then provide a treasure trove of illusions that can keep you entertained for hours!  This missing 3rd dimension of depth is an important example of the *ill posed* nature of perception, which is a mathematical description of a situation where you have more unknown variables than data points available, so you have to rely on extra constraints or assumptions.

There are two major categories of **depth cues** used to reconstruct the missing 3rd dimension:

![**Figure 4.15:** Convergence of eyes on a common point provides one of the two binocular depth cues.  The other is different offsets of features across the two eyes, produced by parallax.  ](../figures/fig_vis_depth_binocular_convergence.png){ width=50% }

* **Binocular** cues that depend on the **parallax** effect, where the left and right eyes receive slightly different views of the world based on their different horizontal positions.  The use of these two views to extract depth is known as **stereopsis** (same root as "stereo", and same effect as stereo sound).  There are two forms of these binocular cues: the **retinal disparity** (differences) across the left and right eye views of the same general region of visual space (enabled by the cross-over and combination of different-eye views into the same visual hemisphere, as shown in Figure 4.6), and the **convergence** of the two eyes to focus the center of vision at the same point in depth (Figure 4.15).

   These binocular cues give the most vivid sense of 3D depth, and are what 3D movie and TV technologies provide, by being able to project different images to the two eyes, either by using different color filters (red vs. blue, as in the old 1950's 3D glasses) or different polarization of light (the current generation of 3D glasses).  3D technology has generally remained a bit of a "fad", not essential like color, because we have so many other cues to depth (and the glasses can be uncomfortable, and they cut the image brightness in half).

   Figure 4.16 shows an *autostereogram* or single-image stereogram, which is a version of a *random dot stereogram*, popularized by the *Magic Eye* books several years ago.  To see depth here, you have to adjust your eye convergence outward ("wall eyed", focusing well beyond the image plane itself), causing each eye to see a different offset of the random dots in the image.  These offsets across the two eyes are the *retinal disparity* signal, and the fact that you can see depth with *only* this retinal disparity signal was a big deal when first discovered by Bela Julesz in 1959.  When you (eventually) get just the right eye position, a previously-hidden 3D world gradually materializes out of the sea of random dots!  This takes a lot of practice and patience --- see [wikipedia page](https://en.wikipedia.org/wiki/Autostereogram) for more info and examples.

![**Figure 4.16:** Autostereogram --- the 2D image gives a hint as to the magic 3D world that lies just under its surface --- you just have to focus your eyes out in the distance, and let your brain "settle" to see it.](../figures/fig_vis_depth_autostereogram.png){ width=100% }

* **Monocular** cues, which operate strictly within a single 2D image, include **occlusion** (one object in front of another), **relative size** (larger = closer), **texture gradients** (also including local surface texture indicating convex and concave shapes), and **linear perspective**, as shown in Figure 4.17.  There are many others, especially if you include motion, which can give strong depth signals.  One can trace a progression in art over centuries in terms of the use of these cues to create a perception of depth, with occlusion, relative size, and texture gradients being among the earlier ones, while the use of linear perspective was revolutionary in  the renaissance period.  Modern-day sidewalk artists employ these techniques to great effect (Figure 4.18).

![**Figure 4.17:** Monocular depth cues including linear perspective and texture enable us to see depth in otherwise flat, 2D images.  The left panel also shows how our brains automatically use these depth cues to infer the size of different objects in the scene, causing you to automatically perceive the top bar as much larger than the bottom one, even though they are identical in size.  This is another example of how we perceive the world, not the raw sensation, and is known as *size constancy* in this case.](../figures/fig_vis_depth_perspective_texture.png){ width=70% }

![**Figure 4.18:** Sidewalk art that takes advantage of monocular depth clues to provide a compelling illusion of depth (but only when viewed from the right point).](../figures/fig_vis_depth_sidewalk_illus.png){ width=50% }

### Compression in Object Recognition

![**Figure 4.19:** We are biased to see objects, especially faces, even where none exist.  This reflects the higher-level compression of visual scenes into known object categories.](../figures/fig_vis_clouds_jesus.png){ width=50% }

As illustrated in the hierarchical filtering process from Figure 4.1, every stage of processing in the perceptual pathways (and everywhere in the cortex more generally) produces more and more compression and contrast effects like those we've seen already, starting right in the retina itself.  Thus, you should not be surprised to learn that the brain has a strong bias toward organizing the features in an image into a much simpler, compressed encoding in terms of *objects*.  Saying "its a dog" represents a massive degree of compression relative to all the visual information that goes into the image of a typical dog.  This bias toward seeing known objects in images is behind the common "mild hallucination" of perceiving objects where none exist, for example in the shapes of clouds, wood grain, toast --- whatever has enough raw material to organize into objects (Figure 4.19).

![**Figure 4.20:** An image of a plate of spaghetti transformed by Google's Deep Dream neural network algorithm that progressively enhances features of the image consistent with what it has learned across a large number of "normal" images.  This produces hallucinogenic images similar to those seen on LSD and other psychedelic drugs, reflecting the imposition of our simplifying biases onto images --- just a more extreme form of Figure 4.19.](../figures/fig_vis_deep_dream_spaghetti.jpg){ width=50% }

Figure 4.20 (yep) shows the output of a [neural network](#neural-network) model trained on millions of photographs, when the input image (a plate of spaghetti in this case) was progressively altered in a way that better fit the internal biases of the model.  These images resemble the kinds of hallucinations produced by psychedelic drugs such as LSD, suggesting that these drugs have the effect of enhancing the influence of internal representations over the raw input stimuli.  Thus, we are all mildly hallucinating all the time, and the waking dream state produced by these drugs just accentuates these processes.

![**Figure 4.21:** Is there something more than just dots in this image?  The ability to extract the underlying shapes from this image represents the benefit of top-down activation in driving our perceptual system --- even though it has a mild hallucinatory side-effect, it is often beneficial to be able to see partially-occluded or otherwise hard to see objects.](../figures/fig_dalmatian.png){ width=50% }

Figure 4.21 demonstrates the important benefits of this bias to "see things" in images --- more often than not, there actually are things there, which might be obscured in various ways, and having the ability of top-down expectations and biases to influence our perception helps pull these things out [@OReillyWyatteHerdEtAl13].

![**Figure 4.22:** The classic Kanisza triangle, demonstrating how we interpret the "suspicious coincidences" of the wedges and apparent occlusion of the background triangle in terms of the simpler percept that there is a white triangle in front of the other items.](../figures/fig_vis_kanizsa_triangle.png){ width=25% }

The classic **Kanisza triangle** (Figure 4.22) provides another demonstration of this bias toward seeing a simpler, more compressed encoding of the world.  These kinds of effects have traditionally been explained in terms of **gestalt principles**, developed by a school of influential German school of psychologists in the early 1900's to explain how we tend to impose higher-level *gestalt* groupings onto images (Figure 4.23).  From the modern perspective, the attempt to explicitly enumerate long lists of such "principles" seems like a mismatch relative to the way that top-down and bottom-up *constraints* or *biases* interact in a more graded, emergent way in actual perception.  These biases and constraints are much "softer" and more "fluid" than things you might articulate as "principles".

![**Figure 4.23:** Some of the many gestalt principles for how items in an image are grouped and organized.](../figures/fig_vis_gestalt_principles.png){ width=60% }

And speaking of soft, fluid images, the artwork of Salvador Dali provides an excellent illustration of the hallucinogenic top-down biases at work, trying to organize and simplify the world (Figure 4.24).

![**Figure 4.24:** The art of Salvador Dali demonstrates the power of object (especially face) biases in perception.](../figures/fig_vis_dali_paintings.png){ width=100% }

TODO: HERE

### Time Contrast: The Novelty Filter

Another very important form of **contrast** that drives perception is *contrast over time* --- i.e., "the news" --- the visual system prefers new stimuli and new ways of seeing things.  In other words, it functions as a **novelty filter** --- filtering out the old and focusing on the new.  Like compression, this happens at all levels in the system, from the retina on up.  In the retina, if you prevent the eye from moving at all, and present a static image, the firing of retinal neurons will slowly start to fade away, and the world will go black (this experiment has actually been conducted by paralyzing the eye muscles!  The various color opponency illusions mentioned earlier also demonstrate this novelty-filter property, where staring at one color causes the opponent color to be relatively more activated.

![**Figure 4.25:** The visual system also enhances *contrast* over *time* --- active neurons experience *accommodation* or *adaptation*, thus favoring novel perspectives instead of continuing to see the same thing over time.  Stare at the *necker cube* on the left, or the ambiguous figure-ground image on the right, and you'll find your brain spontaneously switching between the two different ways of seeing them.](../figures/fig_vis_time_contrast_necker_figgr.png){ width=80% }

Figure 4.25 demonstrates a higher-level version of the neural adaptation that drives these novelty filter effects.  If you stare at these images for long enough, you will find your brain spontaneously switching to a new way of seeing the image, without any explicit attempt on your part to do so.  In fact, it is difficult to prevent your brain from switching in this way --- because it is built right into the neural hardware.

## Audition

![**Figure 4.26:** Overview of the auditory system, where rapid changes in air pressure (sound) are amplified by the tiny bones in your inner ear, causing the hair cells in the cochlea to bend --- this mechanical force is then transduced into neural firing and processed by many stages of subcortical networks before making its way up to the primary auditory cortex (A1) by way of the MGN nucleus of the thalamus.](../figures/fig_auditory_system_sound.png){ width=80% }

Because the same principles just explored for the visual system apply to all of the other sensory modalities, and it is harder to share the relevant perceptual experiences via a book, we will give these other modalities the short shrift that my son recommended at the outset.  Figure 4.26 shows the essential features of the auditory transduction process, where sound waves are converted into neural firing, via hair cells in the cochlea.

First, sound travels as *waves*, and thus can be described in terms of its **amplitude** (intensity) and **frequency** or **wavelength** (pitch).  These vibrating sound waves cause the **ear drum** to vibrate (like a drum!) and this vibration is then amplified by tiny bones (*ossicles*: Malleus, Incus, Stapes --- no you don't need to memorize these!) that cause your inner squid (the liquid-filled **cochlea**) to vibrate in tune with the sound.  Inside the cochlea are only about 3,500 inner hair cells that are responsible for transducing the liquid vibrations into neural firing signals.  This is an incredibly tiny number of *anything* at the cellular level --- your hearing is precious and you should do everything you can to preserve those priceless cells!

The cochlea transforms the time-varying sound waves into a much more useful kind of representation, remarkably similar to what is typically done in artificial signal processing approaches, by effectively performing a *fourier transform*.  This occurs by the **place coding** of frequency, such that different subsets of hair cells are activated for different frequencies of sound (for low frequency sounds, however, the frequency remains as a time-varying neural firing signal).  This is called a **spectrogram**, typically plotted with time on the X axis and frequency on the Y axis.  By splitting out different frequencies across different neurons, it becomes easier to recognize patterns that span across these frequencies --- e.g., the distinctive patterns of human speech, called *formants* are characteristic patterns of frequency changes over time.

Interestingly, in the spectrogram representation, these patterns look like contrast edges at different orientations (increasing, decreasing), which are exactly the same patterns processed at the lowest levels of the visual system.  Indeed, the very same neural networks that do a good job of recognizing visual patterns can recognize these auditory patterns as well.  This then makes sense of the amazing experiment where the visual signals in a ferret were re-routed to its auditory cortex, and the cells there developed very similar firing patterns as are typically found in visual cortex [@AngelucciClascaBricoloEtAl97].

In short, the auditory cortex does the same kind of compression and contrast processing of the auditory signals, extracting simpler ways of summarizing all of the sound information through a series of hierarchical layers.  At the upper levels, which we are consciously aware of, we have things like "Max wants a banana", summarizing a long complex auditory stimulus with a relatively few bits of information.

## Attention

In addition to all the compression described so far, there is another critical driver of compression effects in perception, known as **attention**.  Subjectively, attention is often described as a **spotlight**, shining bright mental light on one, or at most a few, items in the current *attentional focus*, which also has the consequent effect of pushing everything else off into the shadows.  Like compression, attention operates everywhere in the cortex, and can be understood in terms of the interactions between bidirectional excitation and inhibition among neurons, where inhibition is what pushes everything else out into the shadows, and bidirectional excitation reinforces the attentional focus [@CohenRomeroFarahEtAl94].  Not coincidently, this bidirectional excitation is the same central ingredient in consciousness that we discussed in Chapter 3 (aka recurrent processing) --- the focus of attention typically corresponds to what we are consciously aware of.

There is a special part of the brain, in the *parietal lobe*, that seems to be particularly important for **spatial attention** (i.e., paying attention to different parts of space), which has been extensively studied.  Although attention itself is ubiquitous, spatial attention is particularly important in perception because discrete objects tend to occupy different regions of space, and thus we tend to use this spatial attentional focus as a way of directing attention at different objects of interest.  This special role for spatial attention is also directly tied to the fact that the very same neural circuits in the parietal lobe are used for deciding where to move our eyes.  Thus, attention is typically synonymous with *looking*, and the motor action of looking (moving the eyes) requires working with the spatial coordinates of where to look.

![**Figure 4.27:** Demonstrations of hemispatial neglect.  Upper left: Progression of self portraits by an artist with hemispatial neglect, showing gradual remediation of the neglect over time.  Right: Drawings of given target objects by patients with hemispatial neglect, showing profound neglect of the left side of the drawings.  Lower left: Results of a line bisection task for a person with hemispatial neglect. Notice that neglect appears to operate at two different spatial scales here: for the entire set of lines, and within each individual line](../figures/fig_neglect_all.png){ width=50% }

Some of the most striking evidence that the parietal cortex is important for spatial attention comes from patients with **hemispatial neglect**, who tend to ignore or neglect one side of space (Figure 4.27). This condition typically arises from a stroke or other form of brain injury affecting the right parietal cortex, which then gives rise to a neglect of the left half of space (due to the crossing over of visual information).  Interestingly, the neglect applies to multiple different spatial *reference frames*, as shown in the line bisection task, where lines on the left side of the image tend to be neglected, and also each individual line is bisected more toward the right, indicating a neglect of the left portion of each line (Figure 4.27).

### The Posner Spatial Cueing Task

![**Figure 4.28:** The Posner spatial cueing task, widely used to explore spatial attention effects. The participant is shown a display with two boxes and a central fixation cross --- on some trials, one of the boxes is cued (e.g., the lines get transiently thicker), and then a target appears in one of the boxes (or not at all on catch trials). The participant just presses a key when they first detect the target.  The typical data plotted on the right, showing that the target detection reaction time is quicker for valid cues vs. invalid ones.  This suggests that spatial attention was drawn to that side of space. Patients with hemispatial neglect exhibit slowing for targets that appear in the neglected side of space, particularly when invalidly cued.](../figures/fig_posner_task_data.png){ width=70% }

One of the most widely used tasks to study the spotlight of spatial attention is the Posner spatial cueing task, developed by Michael Posner [@Posner80] (Figure 4.28).  One side of visual space is cued, and the effects of this cue on subsequent target detection are measured.  If the cue and target show up in the same side of space (*valid* cue condition), then reaction times are faster compared to when they show up on different sides of space (*invalid* cue condition).  This difference in reaction time (RT) suggests that spatial attention is drawn to the cued side of space, and thus facilitates target detection. The invalid case is actually worse than a neutral condition with no cue at all, indicating that the process of reallocating spatial attention to the correct side of space takes some amount of time. Interestingly, this task is typically run with the time interval between cue and target sufficiently brief as to prevent eye movements to the cue --- thus, these attentional effects are described as *covert attention*, while eye movements constitute *overt attention*.

Patients with hemispatial neglect show a disproportionate increase in reaction times for the invalid cue case (Figure 4.28), specifically when the cue is presented to the good visual field (typically the right), while the target appears in the left.

## Psychophysics

Last, and frankly, likely least for most readers, we are required by an unwritten psychology rule to cover the field of *psychophysics*, which is an attempt to measure the most basic aspects of perception with a degree of precision that is intended to impress our colleagues in physics (who likely remain unimpressed).  These basic aspects of perception center around finding the very weakest stimulus intensity that can be detected, across different modalities.

The **absolute threshold** is the name of this very weakest stimulus, and it is listed in Figure 4.5 for each of the main sensory modalities.  A key factor here is defining what it means that a stimulus can be detected --- what if you can't quite detect it all the time, but still most of the time --- does that count?  The convention is actually to push all the way down to a 50% probability of detection.

We can also determine the **discrimination threshold** or **just-noticeable difference** (*JND*), which is how big of a *difference* between two different stimuli that can be reliably detected (again typically at the 50% probability level).  One of the most exciting results in psychophysics is that this JND is a function of the intensity of the stimuli, and the famous **Weber's Law** says that it is a constant proportion of the intensity.  For example, if people can detect differences in fairly dim lights of a few percent, then to detect differences in much brighter lights, the raw differences must also be much larger, exactly in proportion to the intensity of the lights.  This proportion is known as the **Weber fraction**.

One of the reasons we have to cover this (btw, it is all over --- not so bad after all!) is that psychophysics was one of the earliest examples of scientific psychology, starting with Ernst Weber's work in the 1830's and fully established by Gustav Fechner in 1860.  

## Summary

This chapter has a lot of detailed information, but the overarching theme of *compression* and *contrast* hopefully comes through.  Your brain is wired to be a *simplicity filter* and a *novelty filter*, delivering the simplest interpretation of an complex pattern of sensory input, and focusing on what is new and different.  These same processes, which can be tied directly back to the properties of neurons as explained in Chapter 2, operate throughout your brain, at all levels, shaping how you perceive other people (in terms of simplifying stereotypes) and the world (always seeing out news, and quickly discounting the past).  Thus, perception truly is a window onto the soul, and we return to these lessons throughout the remainder of the textbook.

## Summary of Key Terms

This is a checklist of key terms / concepts that you should know about from this chapter.  As we'll learn in the memory chapter, it is a great idea to test yourself on what was said about each of these terms, and then go back and double-check --- that provides both beneficial repetition and also the *testing effect*.

* Hierarchical compression
    + Filtering
    + Raw sensation vs. subjective perception

* Perceptual constancy
    + Color constancy
    + Size constancy
    + Assumptions, illusions

* Sensory systems
    + Physical stimulus, transduction, subcortical preprocessing
    + Vision, rods, cones, retina, LGN, V1
    + Audition, hair cells, cochlea, MGN, A1
    + Olfaction, hair cells, ofactory epithelium, olfactory cortex
    + Gustation, taste buds, papillae, VPN, Insula
    + Somesthesis, free nerve endings

* Vision
    + Retina, periphery, fovea, photoreceptors, blind spot
    + Cones for color, rods for motion
    + Center-surround contrast detectors
    + R,G,B primary color photoreceptors (L,M,S)
    + Red-green, blue-yellow opponent color coding
    + Binocular depth cues: retinal disparity, convergence
    + Monocular depth cues: occlusion, relative size, texture, linear perspective
    + Object, top-down biases, hallucinations, Kanisza triangle
    + Gestalt principles
    + Time contrast: novelty filter

* Audition
    + Amplitude, frequency, wavelength
    + Ear drum, cochlea, hair cells
    + Place coding, spectrogram
    
* Attention
    + Spotlight
    + Spatial attention in parietal lobe
    + Hemispatial neglect
    + Posner spatial cuing task

* Psychophysics
    + Absolute threshold
    + Discrimination threshold, just-noticeable-difference (JND)
    + Weber's law, Weber fraction
