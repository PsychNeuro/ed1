# Chapter 9: Origins: Evolution, Genetics, and Development

Having now explored the full scope of human cognition, from perception to the highest levels of intelligent reasoning, we now circle back and consider the incredibly fascinating and challenging question of *origins* -- where does everything a mature adult can do come from in the first place?  In the learning chapter, we touched a bit on this question, arguing that the genetic code could not possibly contain enough information to directly specify the synaptic connection strengths of even a small fraction of the billions of cortical neurons that support all of our cognitive functions.  Thus, learning must play a critical role.  And yet, we don't yet have fully satisfying, widely-accepted accounts of how this learning unfolds, and how the considerable genetic shaping of the basic organization and wiring of the brain interacts with these learning processes to determine how we develop.

Thus, development remains more of a mystery than most of the other topics that we cover.  But certainly much is known about the *phenomenology* of development, e.g., the general chronology of when different capacities begin to emerge.  Furthermore, we also know a great deal about genetics and how the blueprints of life are encoded in our genes, and understanding the nature of these mechanisms is essential for having a more complete picture of how biology can shape and constrain our developmental processes.  We begin our exploration of these issues at the natural beginning: with the process of evolution -- the origin of everything!

## Evolution

Evolution is an absurd theory on the face of it.  How is it even remotely possible that a beast such as a fish could *ever* give birth to something that was *not* a fish?  And even if it did, how could that freak ever mate with anything else to propagate its non-fishiness?  Two freaky non-fish emerging at the same time, in close enough proximity to propagate this new species?  It just doesn't add up.  Presumably everyone has heard the basic principle of *survival (and reproduction) of the fittest* as the primary engine of **adaptation** in evolution, but it seems that this alone does not really explain things.  Undoubtedly, the sheer implausibility of the ideas at this basic level contributes at least something to the continued reluctance that people have toward this theory.

There are several things we can work through to help resolve some of these conundrums.  First, we need a clear overall framework for thinking about biology, and what we are actually made of -- fortunately, LEGO provides a really nice, familiar analogy.  Second, we need to think more carefully about the actual nature of evolutionary change: it is very incremental, and yet somehow can produce large changes over time -- reconciling those two points is difficult within our limited scope of experience.  Here, computational models can be immensely useful in providing the missing "long timeframe" perspective, by simulating many generations of evolution within a few minutes of human time.  Furthermore, a few key principles can go a long way toward resolving the basic problems stated above.

![Fig 9-1: Which picture shows cells from a human and which from a frog?  At the cellular level, all animals are essentially identical, built from the same building blocks -- just like you can build so many different things using the same set of LEGO blocks.  Understanding this makes it easier to see how different bodily forms can emerge through evolution.](figures/fig_human_vs_frog_cells.png){ width=60% }

![Fig 9-2: Biology is remarkably like LEGO: the same small set of basic building blocks can be recombined to produce many different animals.  This continuity across all animals makes it clearer how evolution can work: you just need to tweak the instructions a bit and you can end up with entirely different animals, as in this 3-in-1 kit that uses the same parts, with different instructions, to make very different animals.  In biology, genes are the instruction set, and these instructions are constantly subject to random changes, producing novel "experiments of nature". ](figures/fig_lego_animals.png){ width=60% }

When you think of the manifold differences between a fish and a human being, evolution seems impossible.  However, these differences disappear when you zoom in to the cellular level, where it immediately becomes clear that all animals are made from the same basic building blocks (Figure 9-1).  This is really the level at which evolution is operating -- the macroscopic forms of organisms (which is what we perceive) are more of an emergent result of cellular and sub-cellular processes unfolding over the developmental process, just like our cognition is an emergent product of our neurons interacting in complex ways.

The easiest way to understand this is in terms of the more familiar process of building different things out of LEGO blocks (Figure 9-2).  A very small set of basic building blocks can be used to make an essentially infinite number of different objects.  Furthermore, the developmental process is akin to the process of following the step-by-step instructions to assemble the final product.  As you may have experienced, small random errors at an early stage can have major implications later.  Likewise in biology, relatively small changes in the developmental process can lead to major changes in the overall shape and function of the organism.  To make a bigger brain, you just have to tweak the process that controls how long nerve cells divide and proliferate, in the same way you would just keep adding blocks to a LEGO wall to make it bigger.  Thus, major macroscopic changes can emerge from relatively minor changes in the program.

In biology, the genetic code is the equivalent of the LEGO instruction booklet, and we'll see below that much of the genetic information is devoted to controlling the timing and coordination of the building process (as compared to the raw bricks themselves), and this is likely where much of the action has taken place over the course of evolution.

Perhaps it is now clearer how very different-looking organisms can emerge from relatively minor tweaks to the genetic code, and how in fact a fish very likely could produce some rather different offspring even in a single generation.  Indeed, there are plenty of examples of "genetic freaks of nature" that can be found on the internet (e.g., [list25.com 25 disturbing freaks](https://list25.com/25-disturbing-freaks-of-nature-you-have-to-see-to-believe/)).  But the other puzzles remain: how could that weird offspring itself procreate, and lead to the origin of new species, and more complex, sophisticated biological machinery?  This is really the hard problem of evolution that Darwin tackled, and later theorists such as *Stephen J. Gould* wrestled with.  

One key idea here is that gradual, continual change from one generation to the next is much easier to understand than some kind of entirely new beast emerging whole within a single generation -- even though random changes in the genetic code can produce such things, they indeed aren't likely to have survived and procreated.  But how does gradual, incremental change ever lead to the emergence of something dramatically, qualitatively different?  We can consider two cases: emergence of air breathing, and moving onto land from a fish, and the emergence of flying birds from dinosaurs.

In the case of the air breathing, one could easily imagine that there was some gradual advantage to being able to gulp some extra oxygen from the surface of the water -- such fish would have more energy, and this would be especially important if the water ended up being less oxygenated, e.g., as happens in algae blooms.  Also, there is plenty of evidence that proto-lungs emerged for various other digestive and boyancy-control functions.  Then, as this ability was selected for over time, more and more oxygen could be processed, until at some point, such animals could actually survive by breathing air.  If they happened to get trapped on land, as often happens during tidal cycles, then you end up with all the right ingredients for one of the most dramatic events in evolutionary history: the transition from the sea to land.  Of course, we don't know how it actually happened for sure, but by thinking about how gradual *quantitative* processes can eventually turn into a major *qualitative* changes, at least it may seem more plausible.

In the case of bird feathers and flight, we now know that many dinosaurs had feathers, presumably for temperature regulation.  Thus, like the oxygen example, something can be adaptive in one way, before it becomes "co-opted" for something else entirely -- Gould referred to this as an **exaptation**, while Darwin referred to it as *preadaptation* (which sounded a bit too "prescient" for Gould).  Thus, the dinosaurs that climbed trees, and had feathers for keeping cool, eventually discovered that they could glide their way to safety using these same feathers, and a whole new adaptive function and corresponding gradual selection pressure emerged, to make better and better wings.

Putting all this together, this initially crazy-sounding idea perhaps makes more sense.  It just takes a lot of time and a lot of random accidents and amazing stories of survival -- those challenges are the "desirable difficulties" of evolution that have driven survival of the fittest to select some radical new innovations.  It took many such apocalypses to get us where we are today, and it is thoroughly mind-blowing to try to grasp this idea that we are just the latest spawn in a "great chain of being" stretching back billions of years.  But look again at Figure 9-1: at this level, not that much has really changed in all those years!

To really see evolution happening in "real time" that we can actually comprehend, people have developed computer simulations of evolution, and it definitely works!  Indeed, there is a quote that neural network algorithms (modeled on the functioning of the brain) are the second best solution to any complex problem, and the third best solution is a **genetic algorithm**, which is the general computer-science version of evolution.  Randomly searching a complex, high-dimensional space, and combining the features of the best-functioning exemplars, works really well for finding novel, previously-unimagined solutions to complex problems.  Interestingly, both genetic algorithms and neural networks have the same property of following *gradients* (i.e., hill climbing, as was discussed in the case of problem solving strategies earlier) -- this is the core property of evolution where some adaptive property gets "optimized" over successive generations.

*Jeff Clune* and colleagues have developed particularly compelling, bio-mimetic simulations of the evolution of organisms, often with very funny but functional properties: [YouTube video of Evolving Soft Robots](https://www.youtube.com/watch?v=z9ptOeByLA4) -- this is really a must-see video, and should hopefully make evolution come to life in a unique and compelling way.


## Genetics

![Fig 9-3: The molecular structure of DNA, which has an alphabet of only *4* different letters: G, C, T, A.  They are paired with each-other as shown, but any given strand could have any one of these different letters in any order.](figures/fig_dna.png){ width=50% }

As we mentioned above, the biological equivalent of the LEGO instruction booklet is the *genome* -- the collection of genes that determine how everything in our bodies (and every other living organism) is built.  It is amazing how much progress has been made in understanding the genetic basis of biology since the structure of DNA was discovered in the early 1950's, and published in 1953 by *James Watson* and *Francis Crick* (who later became somewhat of a neuroscientist, confirming that the brain is the most fascinating thing in the universe, and the "last refuge of scoundrels").  And it is also amazing that this all happened so recently -- we are the first few generations of beings that now know (more or less) how that great chain of life actually works.

At its base, the genetic program is remarkably simple: there are only *4* different letters in the language of life: G, C, T, A, which are always paired GC and TA (Figure 9-3).  These 4 **base pairs** don't do much by themselves -- it takes 3 of them in sequence to determine a corresponding **amino acid** (of which there are 20 different varieties coded in the DNA).  Interestingly one of these amino acids is *Glutamic acid*, which is the basis of both the ubiquitous excitatory neuotransmitter glutamate, and MSG, and another is *Tyrosine*, which is the direct chemical precursor of the major neuromodulators dopamine, epinepherine, and norepinepherine.  Thus, like the sodium and chloride ions, the key ingredients that make the brain tick are really basic and universal in biology, coded directly by just 3 base pairs in our DNA.

The more complex building blocks in biology are composed from sequences or polymers of these amino acids, i.e., **proteins**.  These are the LEGO blocks of the body, and our DNA directly codes for how to build these blocks via those sequences of 3 base pairs.  However, only a tiny fraction (about 1.5%) of our total DNA actually codes for these proteins, of which there are about 20,000 different types.  Proteins come in various lengths, from a few hundred up to 20,000 amino acids in size.  However, the *genes* that code for these proteins are typically *much* longer than the minimal number required to code for the literal amino acid sequence -- there is clearly a lot more going on in the genome than just the literal coding of amino acids to build proteins, even among the tiny fraction of genes that actually code for proteins in the first place.

Thus, there is quite a gap between the very concrete, well-understood level of DNA, amino acids, and proteins, and the rather fuzzier notion of **genes**.  Genes are defined functionally as units of **heredity** -- the basic elements that we can inherit from our parents, and the simple idea that each gene codes for a different protein holds in some cases, but only a tiny minority.  Again, LEGO is incredibly helpful in understanding why this might be the case.  When you're building a lego kit, the vast majority of the information in the instruction booklet concerns *where* and *when* to place the bricks, with only a relatively tiny bit of information concerning the different bricks that are available to build with.  For example, there may be around 100-200 different types of bricks in a typical reasonably complex LEGO kit, but the amount of information it takes to specify exactly where to place those bricks is much greater (and somewhat difficult to quantify, given its visual nature).

In short, although the remaining 98% of the human genome that does not code for proteins was originally characterized as "junk DNA", it is highly likely that most of it is serving a vital function akin to the bulk of the LEGO instruction booklet: determining when and where to build all those proteins.  For example, we know that much of the extra "junk" within a given protein-coding gene plays a *regulatory* role, shaping the complex process that actually *transcribes* the DNA sequence into a corresponding amino acid sequence to make up the proteins.  Perhaps we will ultimately find the equivalent of a computer programming language embedded in all this regulatory DNA, complete with `if-then` rules and `for` loops, etc.  As with the LEGO instruction booklet, most of the power and functionality in a computer program comes from these kinds of control structures, rather than the raw "proteins" (e.g., numbers, characters or other data) that is being manipulated.


### Sexual Reproduction

Aside from the pure intellectual fascination of understanding the amazing genetic machinery that makes us tick, the practical relevance to psychology and neuroscience comes in understanding how our genetic information is inherited from our parents, and how much of our overall brain function it ends up determining.  This is the domain of **behavioral genetics**, which traditionally has been performed by comparing **identical (monozygotic)** versus **fraternal (dizygotic)** twins, and is now also able to leverage the advances in *molecular genetics* to directly compare genetic material across people.  To understand how all this works, we first need to understand how **sexual reproduction** works!

Monozygotic twins started out from the same **zygote** (fertilized egg cell), and they thus share 100% of their DNA, whereas dizygotic twins came from two separate egg cells, and have the same genetic similarity on average as any siblings born from the same parents (i.e., 50%).  The exact process by which this 50% genetic similarity arises is surprisingly complex.  Likely everyone remembers hearing about _meiosis_ and _mitosis_ from high-school biology, but you probably don't remember all the crazy details.

![Fig 9-4: The source of genetic randomness in sexual reproduction comes in the making of the gametes during crossover in meiosis (not during fertilization).](figures/fig_sexual_reproduction_meiosis.png){ width=80% }

Interestingly, all of the genetic shuffling responsible for producing the essential randomness that powers evolution happens *before fertilization!*, despite the emphasis on sexual reproduction being the source of this randomness -- no further mixing-up of genetic material occurs when the egg and sperm fuse to form the zygote (Figure 9-4).  Specifically, each **gamete** (germ cell -- egg or sperm) undergoes **meiosis**, and this is where *the future parent's own genetic material* that was inherited from *their* parents is shuffled.  Thus, each parent is actually re-shuffling the genes from their own parents (the grandparents of the future child) to create some new random genetic sequences.  This shuffling process occurs as the two copies of each **chromosome** (one from grandpa and one from grandma -- the parents of the future parent) are split apart, such that each gamete only has *one* copy of each chromosome (i.e., it is "haploid", compared to the normal "diploid" with 2 copies).

As you likely know, there are 23 distinct chromosomes, which are large collections of DNA.  When the egg and sperm join to form the zygote, the separate collections 23 chromosomes from each parent are simply "added" back together to form a "full deck" of 46 chromosomes, without any further mixing.  Thus, you are really the *sum* of random combinations of genes from each of your two sets of grandparents, and your two parents's gene sets won't really mix until you have your own kids!

After fertilization, the zygote only divides via **mitosis**, which is the "normal" form of cell division that preserves the full deck of chromosomes in each of the two new *daughter* cells (each chromosome, one inherited from each parent, splits and replicates, again with no further recombination).   Thus, your two parent's chromosomes are preserved intact from that point onward, and the primary form of interaction between them is in terms of the relative *dominance* vs. _rescessiveness_ of the genes inherited from each parent.  If both resulting copies of a given gene across the two chromosomes are the same, then there is nothing further of interest to discuss -- that gene will do whatever it does, in the same way, all the time.  This is actually the default case: over 99% of our genes are identical across all people, and thus across your two parents.

![Fig 9-5: The logic of dominant vs. recessive genes, in the case of eye color, where blue is recessive compared to brown.  Two people with brown eyes *can* give birth to a blue-eyed baby, if they are both *carriers* of the recessive blue gene.](figures/fig_dominant_recessive_eyes.png){ width=40% }

However, for the roughly 0.6% of our genes that do differ across people, you may end up with a different version of that gene in each of your different chromosomes.  These different versions are called **alleles**, and they are entire focus of interest in behavioral genetics and the study of heritability more generally.  Some versions of a given gene are more likely to be transcribed and **expressed**, or to produce a functional protein product, and this is what is meant by **dominant** vs. **recessive** (and like most things, it is a continuum, not a dichotomy).  Thus, only if you end up having both copies of a gene in the recessive (non-dominant) form, will that recessive version actually do its thing (or fail to do the thing that the dominant gene would otherwise do).  Otherwise, having a recessive form of a gene typically doesn't make much of a difference in the overall function of the organism.

This presence of recessive genes is the reason we (still) have genetic disorders at any significant rate in the population.  Any allele (genetic variant) that is dominant *and* produces bad effects, is quickly driven out of the population through natural selection -- people with that gene version don't tend to survive and reproduce.  However, a recessive gene can fly under the radar and persist in the population, because the odds of two people having the *same* recessive gene variant is really quite low on average (only 0.6% of genes vary at all, and most recessive alleles are relatively rare on top of that).  Except, of course, if they are siblings or otherwise closely related, which is why incest is generally frowned upon.

### Heritability and Individual Differences

Now we can actually talk about how behavioral genetics works.  Basically, it amounts to comparing the genetic similarity of people against their *phenotypic* similarity, where the **phenotype** is just a complex word for the thing you are actually interested in, such as IQ, height, eye color, etc.  For a very small set of phenotypes, your genes essentially determine 100% of how you'll end up.  For example, there are specific recessive genes that cause Huntington's disease, and cystic fibrosis.  But for almost everything else, the relationship between genetic differences and phenotypic differences is much more complex, and actually determining how much can be attributed to genes is surprisingly challenging.

Let's take the case of IQ.  As we discussed in the chapter on intelligence, there is ample evidence that your IQ is a function of learning, motivation, and the wealth and general **socio-economic status (SES)** of your parents.  Thus, any dependence on genes is likely to be at least somewhat indirect.  But how can we measure it?  The simplest way would be to compare the IQ's of identical (monozygotic) versus fraternal (dizygotic) twins, and somehow use their known overall genetic similarity differences to compute how much of their measured IQ differences can be accounted for by those known genetic factors.

![Fig 9-6: How heritability is computed from identical (MZ = monozygotic) and fraternal (DZ = dizygotic) twins.  P = phenotype variance; A = additive genetic variance; C = common (shared) environment; E = unique (non-shared) environment (and everything else).](figures/fig_heritability_ace.jpg){ width=60% }

Critically, we need to also include some kind of factor that can account for non-genetic influences on IQ, which goes under the general category of **environmental** factors.  To, make things more interesting, this latter category is typically split into **shared / common** and **non-shared / unique** environmental contributions, determined by whether the children were reared in the same family environment or not. Thus, there is a three-way tug-of-war dynamic between genetic factors (which are labeled with the letter *A* for additive genetic factors), and these two environmental factors (*C* and *E*), comprising the ACE model shown in Figure 9-6.  The overall genetically-associated portion is called **heritability**, and is denoted with the letter *h*.

The comparative nature of heritability is a source of major interpretational problems.  Just as we saw back in the neurons chapter, this is a fundamentally *Contrast*-based dynamic, and the *relative* balance between genetic and non-genetic factors can be affected by increases or decreases in our measurements of *either* of these factors.  In particular, the apparent heritability of IQ could increase just by *decreasing* the strength of environmental factors.  Indeed, there is considerable evidence for exactly this effect happening in **WEIRD (Western, Educated, Industrialized, Rich, Democratic)** societies, where the majority of the population has essentially comparable levels of health, nutrition, education, etc.  In this case, the impact of environmental factors is greatly reduced compared to cases where some people are severely malnurished and have little access to education.  

![Fig 9-7: Idealized estimate of the effect of parental education on environmental (E and C) versus genetic contributions to the phenotype of reading (word recognition).  Environmental influences go down with greater parental education, increasing the apparent amount of genetic heritability according to *h* measures, *without any actual change in genetic influence*.  This is a consequence of the contrast-based, relative way that heritability is measured -- it does not give an absolute value of genetic contribution (which is extremely difficult to measure -- that is why this figure is just an idealization).  From Kremen et al (2005).](figures/fig_heritability_variance_kremen_05.png){ width=70% }

Figure 9-7 shows an idealized representation of the overall results of an attempt to *independently* estimate the amount of environmental versus genetic contributions to the phenotype of reading ability, as a function of the parent's education level [@KremenJacobsonXianEtAl05].  As you can see, they found evidence that the amount of environmental variance went way down with increasing parental education, directly consistent with this broader WEIRD effect.  Parents with more education generally are wealthier, and provide more enriched educational opportunities to their children -- thus eliminating variability in these factors across individuals.  What is left over is the raw genetic variability, which, due to the random mixing processes at work (Figure 9-4), remains relatively constant.  Thus, the bottom line is that the raw magnitudes of heritability scores cannot be taken as a direct measure of absolute genetic influence -- just like absolute, perfect pitch is very difficult due to our contrast-based perceptual systems, it is extremely difficult to quantify the absolute level of genetic influence in any meaningful way, because it is always relative to the environment, which is even harder to measure directly than genetic differences are.

![Fig 9-8: Heritability estimates of various phenotypes, computed either from twins or directly from the genome (GCTA = genome-wide complex trait analysis).  There is a *missing heritability* in the direct genetic measures (which are about half as big as the twins), which could be due to rare genetic variation not measured, or by gene-by-environment interactions present in the twin sample (who are by definition related), versus the unrelated people used for GCTA.  From Trzaskowski et al (2013).](figures/fig_heritability_twin_vs_gcta.png){ width=90% }

Most studies of heritability take place in WEIRD societies, and have produced estimates of heritability around 0.5 for almost every phenotype you can think of (Figure 9-8).  In addition to those shown, personality factors of *neuroticism* and *openness* (which we'll learn more about in the next chapter) have a measured heritability of 0.4 to 0.6 [@PowerPluess15].  Interestingly, Figure 9-8 also shows comparable results from a newer technique based on direct measurements of the genomes of a large number of *unrelated* people, known as **Genome-wide Complex Trait Analysis (GCTA)** [@TrzaskowskiDalePlomin13] (see Figure 9-9 for more detailed results on this technique, for IQ; [@SniekersStringerWatanabeEtAl17]).  Across many applications of this and related techniques, there is a consistent finding that heritability estimates are about half those based on twins!  This is the latest version of the **missing heritability** problem that has come up repeatedly over many years of attempts to use direct genetic measurements to predict phenotypic variance across people  [@Turkheimer00; @Turkheimer11; @PlominDeary15].

![Fig 9-9: The latest Genome-Wide Association Study (GWAS) results for IQ (Sniekers et al, 2017).  A total of 336 locations across the genome exceed the statistical threshold for being significantly correlated with IQ.  No single gene accounts for much of the variance in IQ, and all together they only account for 4.8%.  Most of the relevant genes are regulatory, and very few are clearly protein coding, consistent with the idea that the "junk" DNA is where all the action is.](figures/fig_gwas_iq_2017.png){ width=100% }

There are at least two potential explanations for this missing heritability.  One is that the current genome-based analyses are missing *rare genetic variants*, so their ability to capture the full scope of genetic differences among people is thus limited.  However, recent analyses based on full genetic sequencing instead of the much sparser (and less expensive) sampling techniques used previously have estimated that these rare variants are likely to only contribute about 5% of this missing heritability [@EvansTahmasbiVriezeEtAl18].  Thus, the remaining missing heritability points instead to a range of potential differences between twins and samples of unrelated people, which can inflate the heritability estimates generated by twin studies, which we'll explore in a moment, and point to interesting ways that genes can interact with the environment.

The bottom line at this point is that our current best guess as to the "true" level of genetic influence on various phenotypes is more like the right-hand side of Figure 9-8 based on direct genetic measurements, rather than the traditional twin-based estimates.  Thus, genes probably account for a more plausible 25% of overall variance on average, instead of 50%.  And this is still with all the inflation produced by the WEIRD reduction in environmental variance, so if we actually measured all of humanity, those heritability estimates would be much lower.  Overall, this is consistent with the importance of learning for shaping the brain, as emphasized previously in the learning chapter.

### Shared Environment and Parental Influences

Another striking finding from twin studies is that the estimates of the *C* factor in the ACE model, representing shared environmental influences due to children being reared in the same household, are almost always near zero.  *Judith Rich Harris* has interpreted these pervasive findings to argue that *parents don't matter* in shaping how their children turn out, beyond of course contributing their genes to their children [@Harris11].  This striking conclusion flies in the face of most people's deeply-held beliefs about the importance of parents in shaping their kids, and Harris's book raises many fascinating points about why this idea might in fact be wrong.  For example, children of immigrants typically become most proficient in the language of their new home, not the one spoken by their parents, and in general seem to be much more strongly influenced by their peers than by their parents.  Indeed, probably most parents can recognize that their kids do seem to take them for granted, and are typically much more sensitive to what their friends think and do.

Another thing that parents with multiple kids are always struck by is how *different* their kids can be.  That genetic crossover mixing stuff really works!  Thus, Harris emphasizes that the environment for each such kid is very much an *interaction* between the parent and the child, with perhaps relatively little of a "main effect" of the parent overall across all the kids.  In other words, the child shapes their own environment as much as the parent does.  This is consistent with our focus on the importance of *Control* in the individual: just as we cannot convince our friends to change their beliefs, neither can a parent really control their child nearly as much as we often wish we could!  The developmental transitions starting with the "terrible two's" mark the real onset of an independent, willful being, and from that point onward, the parent's influence is on a consistent downward slide.  Of course, some kids turn out very much like their parents, but a roughly equal portion end up rebelling and try to be as different from their parents as possible.  Thus, when looking for an overall consistent statistical effect of parents, perhaps you could see how it might be hard to find.

Another way to put this problem is that we have a highly accurate, reliable ways of estimating genetic differences among different pairs of people (e.g., identical vs. fraternal twins), but our ability to measure the similarity of *everything else* about people, including the actual nature of their individual experiences within their shared family environment, is significantly worse [@Turkheimer00].  Thus, when you pit a really solid estimate of genetic similarity against a really noisy estimate of shared environment, it is perhaps no surprise that the genetic effects are consequently over-estimated, while the shared environmental effects are under-estimated.  This is in fact one of the major potential sources of the missing heritability present in twin studies relative to the direct genome-based studies (we should more accurately refer to this as the **excess heritability** in twin studies, actually).  In other words, there is in fact a significant contribution of shared environment (parents really *do* matter!), but because we can't measure it very well, this contribution ends up getting soaked up by the much stronger genetic factor, thereby artificially inflating it [@Turkheimer00].

In addition, it turns out that various other factors are also hard to disentangle in twin studies, and could also account for the excess heritability [@KellerCoventry05; @EvansTahmasbiVriezeEtAl18].  For example, people who share various traits are more likely to marry and have children (known as *assortative mating*), which thus inflates the overall genetic similarity of even the fraternal twins, beyond what is assumed by the simple twin model.  In addition, genes don't combine additively, as assumed by the model -- instead they interact through the dominant vs. recessive dynamic discussed earlier (and other similar non-additive interactions across genes, known as *epistasis*), and this can significantly reduce the expected genetic similarity of DZ twins (because the MZ twins have the same genes, this factor affects them both in the same way).

Thus, in summary, despite the chemical precision of our knowledge about the molecular basis of life and the nature of DNA, once you get up to the level of the entire organism, there is a great deal of uncertainty in estimating the contributions that our genes make to the kinds of traits that we care about as psychologists (e.g., IQ and personality).  Despite all these difficulties, there is no doubt that genes are making an important contribution, but it may have been overestimated by twin studies, and is likely to account for roughly a quarter of the overall differences across individuals (in WEIRD societies).  Furthermore, the ability to predict *anything* at all about a *single individual* based on their genetic profile is *extremely limited* and generally nonexistent, outside of the few strong recessive genetic disorders where one or a few genes can make a huge difference.  There are simply way too many complex interactions both within the biology, and between a person and their environment, to make any kind of predictions at the individual level.  Movies such as *Gattaca* which depict a dystopian future where everyone's future is predicted from their genes will almost certainly remain the stuff of science fiction.


## Development

People are fascinated by butterflies because we *are* butterflies!  The dramatic transformations that occur over the course of our development are truly astounding, and it is not too much of a stretch to say that we start out as something resembling a larva, and seemingly magically transform into a fully-functioning being with capacities unparalled in the known universe.  The first three months of life have been described as the "fourth trimester" -- essentially we should still be in the womb but then we would be too big to ever get out, so we complete the last part of our gestation outside the womb.

After about four months, the pace of progress starts to increase, and after about six months, you can really start to see some real signs of neural activity going on under the hood!  Babies at this point can actually recognize their parents, and start babbling in a way that is distinguishable from mere drooling and gurgling.  They are obsessed with putting things in their mouths, and generally can reach for things with a non-zero chance of grabbing them.  After all this time, they can finally start to sit up and support their own huge heads, and maybe start crawling.  With another 6 months, babies can start saying "mama" and "dada" and learn some sign language, and follow simple directions -- language is really starting to happen.  They can drink from actual cups (though most parents will stick with sippy cups for a while longer), and have recognizable hand-eye coordination for grabbing stuff and putting things where they want (though they will not be able to catch anything for years to come).  Standing is starting, walking with support may be getting underway.  Peek-a-boo may still be interesting, but you can see that bigger things are on the horizon.

By their second birthday, kiddos are recognizably human beings.  They have basic mastery of their sensory and motor systems (though still have a long way to go for catching things, and potty training is a major issue at this age), and language learning has entered that exciting **naming explosion** period when 10's of new words can be learned in a single day.  Most importantly, kids start using language to say the most important word: "no"!  This is the onset of the **terrible twos**, when *cognitive control* really emerges, and a real sense of independent motivation takes hold.  In other words, the basic elements of all of our special human capacities are at least minimally present, and from this point onward, it is just more, better, faster, smoother, etc (as a gross simplification).

### Piaget and the Development of the Neural CPU

![Fig 9-10: Piaget's four stages of cognitive development.  After the first 2 years, the notion of "operational" is key: this is really about how capable our neural CPU is at supporting increasingly abstract, symbolic-like processing.](figures/fig_piaget_stages.png){ width=80% }

**Jean Piaget**, a founding figure in developmental psychology, ended his first stage of development, which he labeled the the **sensorimotor stage**, at this two-year mark (Figure 9-10).  Everything beyond this first two years was focused on further stages of progress toward something he called **formal operations** (emerging fully after 11 years of age), which is the ability to perform abstract, logical reasoning, and engage in effective planning and strategizing.  Furthermore, he emphasized the ability to transfer knowledge across different domains at this level.  Basically, Piaget is describing the function of the **neural CPU**, supported by the **prefrontal cortex** and **basal ganglia**, as we discussed in the thinking and control chapter.  These systems work together to enable information to be juggled in **working memory**, and behavior to be controlled by something approximating a **program** of steps to execute in sequence over time.

The fact that it takes roughly 11 years for this capacity to develop is consistent with the idea that the natural function of the neural networks of the brain is *not* intrinsically based on logic or symbolic processing like in a digital computer.  Thus, per Piaget, it takes a *long* developmental progression for our limited abilities to perform logical, abstract reasoning to emerge [@InhelderPiaget58].  One of the main critiques of Piaget's work was that he only *described* the trajectory of steps toward this ultimate formal reasoning ability (as shown in Figure 9-10), without providing a clear, testable proposal as to *how exactly this reasoning ability develops* [@LourencoMachado96].  However, we *still* don't have a very good idea how this kind of thing emerges over the course of learning, experience, and brain development, so perhaps we shouldn't be too hard on Piaget.  His proposal was at least based on a number of very specific behavioral tasks that characterize the level of competence at each of his stages, and those tasks have been widely tested.

![Fig 9-11: Siegler (1981)'s results on the extent to which people at different ages are able to integrate across multiple dimensions in three related tasks (brighter, more yellow = higher % of rule use across participants).  The different rules (1-4) involve a progression of ability to integrate the different dimensions (e.g., weight vs. distance in the balance scale), as tested in problems like those shown at the top, where these dimensions are differentially relevant.  While there is clearly a developmental progression, at each age there is considerable variability within and between tasks, and even adults are not perfectly "logical" (rule 4).](figures/fig_siegler81_balance_rules.png){ width=50% }

Piaget's **preoperational stage** (ages 2-7) marks the emergence of *symbolic*, but not logical thinking (a clearer term would thus perhaps be the *symbolic stage*).  During this time, children build on their initial language competence to use words to refer to non-present objects, and develop more complex symbolic mental abilities -- e.g., imagining entire scenarios and events unfolding over time.  One of the signature developments during this time is the increasing ability to deal with *multiple factor relationships*.  For example, on the **balance scale task**, there are two relevant factors or dimensions: distance and weight.  Children start out only being able to focus on one of these at a time, and time, and then gradually become better at integrating across these two dimensions (Figure 9-11).  As *Robert Siegler* has emphasized [@Siegler81], children's use of different levels of this integration is highly variable both between and within different tasks that are otherwise formally equivalent, as shown in Figure 9-11.  This variability, which persists into adulthood, is inconsistent with the idea that people's behavior is strongly and consistently driven by different levels of logical reasoning and rules, which a simple reading of Piaget's theory would suggest [@LourencoMachado96].  On the other hand, it was possible to characterize any given person's reasoning on a given task, at a given point in time, according to specific, enumerable rules.  Thus, unlike the 3-year-olds, people *do* appear to be using some kind of explicit rule-like reasoning process.  It is just not very systematic.  

Another widely-studied example is the **conservation task**, where two identical glasses initially have the same amount of water, and the child can recognize that.  Then, a taller, thinner glass is introduced, and the child watches as water is poured entirely from one of the two identical glasses into the tall, thin one.  Logically, the tall glass must have the same amount of water, but a younger child will typically say that the tall one has more, because it looks like it does. [YouTube video of conservation tasks](https://www.youtube.com/watch?v=gnArvcWaH6I).  Siegler also demonstrated a similar level of variability in the developmental trajectory of more logical reasoning in these tasks, that take into account the shape and height dimensions [@Siegler81].  Even in adults, bartenders can still get away with this trick (as long as we don't see them actually do the pouring)!

In Piaget's **concrete operational stage** (ages 7-11), kids can now master some feats of logical reasoning, but only in concrete, specific situations.  Thus, some can solve the conservation tasks, but not because they have the general abstract principle of conservation of mass (or energy), but rather because they've had enough physical experience and common sense to recognize that the water isn't going anywhere else.  This is much like the Wason card selection task you tried to solve in the thinking chapter -- you could nail the task when it was posed in a concrete, familiar situation (carding underage drinkers), but likely failed when it was novel and abstract in the card version.  Thus, our brain's preference for concrete, familiar situations does not magically disappear after age 11 -- it is always there, and only our ability to overcome it gets progressively better (but clearly never achieves anything like perfection).

### The Development of the Prefrontal Cortex

![Fig 9-12: Synaptic pruning over the course of development across different brain areas. Blue colors indicate thinner brain areas where synaptic pruning has already taken place, while yellow and red areas are thicker and have yet to be pruned.  The prefrontal cortex is one of the last areas to complete its pruning, along with the superior temporal lobe.  From Gogtay et al., (2004)](figures/fig_synaptic_pruning.png){ width=70% }

Consistent with Piaget's focus on this protracted development of our logical, symbolic reasoning abilities that depend on our neural CPU, there is considerable evidence that the prefrontal cortex is one of the last brain areas to fully mature [@GogtayGieddLuskEtAl04].  This evidence comes from measurements of cortical thickness -- your brain becomes progressively thinner as synapses are pruned over the course of development (Figure 9-12).  This pruning is associated with patterns of neural connectivity stabilizing and thus extra unnecessary connections can be eliminated, making neural information processing (i.e., detection, compression) more efficient.  But also less flexible -- this is one of the reasons it is difficult to "teach an old dog new tricks."

There are many other indications of the protracted nature of prefrontal development.  In addition to being important for supporting abstract, logical, computer-like processing, the prefrontal cortex is important for enabling *controlled processing* to overcome stronger, habitual automatic processing pathways.  As we saw in the thinking chapter, there are a number of problem-solving tasks and puzzles that depend on having to overcome the "obvious" but wrong solution.  Piaget actually developed a version of such a task that kids initially struggle with at around a year of age, and then master in the subsequent few months, known as the **A-not-B task**.  In this task, a toy is repeatedly hidden in one location (*A*), and the infant is allowed to reach for it there.  Then, it is hidden in a new location (*B*), but the child tends to reach back to the original "habitual" location, hence the name: A, not B. [YouTube video of A-not-B](https://www.youtube.com/watch?v=lhHkJ3InQOE). Success in this task is thought to depend on the increasing ability to maintain neural activity representing the actual hiding location, dependent in part on the developing prefrontal cortex  [@Munakata98].  

Well after children succeed at this A-not-B task, they still fail at a related task involving sorting cards according to different dimensions (e.g., color vs. shape) [@ZelazoFryeRapus96].  [YouTube video of DCCS](https://www.youtube.com/watch?v=0L7xzcvJzZc).  Again, the explanation is that prefrontal cortex is continuing to develop throughout this long developmental period.  But why does this manifest in different tasks at different time?  One important factor is the relative familiarity and concreteness of the situation.  In the A-not-B case, kids get a lot of experience tracking and reaching for objects, and thus it becomes "second nature" to track the hiding location of the toy.  For the card sorting task, kids of this age have much less experience applying relatively arbitrary rules to sorting cards, even though the dimensions of color and shape are presumably quite familiar.  Thus, these weaker mental states can be more strongly influenced by the new experience, and people get "stuck" in a mental set associated with the first sorting rule [@YerysMunakata06].  Likewise, when adults confront the Wason card selection task, we still don't have much experience applying those kinds of arbitrary if / then rules in that way, and fall back on the raw perceptual match of the cards and the terms in the rule.

![Fig 9-13: Heritability of IQ goes *up* as a function of age (based on twin studies; Haworth et al, 2009).  One explanation is that IQ reflects in part the motivational and integrated learning contributions of the prefrontal cortex, and this takes a while to manifest over the protracted development of this brain area.](figures/fig_heritability_iq_age.jpg){ width=50% }

Another interesting potential indication of extended prefrontal development is that the genetic heritability of IQ actually appears to *increase* over time (Figure 9-13) [@HaworthWrightLucianoEtAl09].  This is not consistent with the idea that the genes are directly shaping the raw computational power of the neural CPU, but rather that they influence factors that shape the nature of learning over the relatively long developmental timecourse.  As we discussed earlier, it is likely that motivation plays an especially large role in shaping overall measured IQ, and this is consistent with the idea that it the sustained influence of motivation over a relatively long time that produces increased genetic differences.  Furthermore, other work has shown that people who have the highest measured IQ levels exhibit a longer period of greater environmental vs. genetic influence over IQ [@BrantMunakataBoomsmaEtAl13].

### Crystallized vs. Fluid Intelligence Development

From everything that we just discussed, you might conclude that cognitive development is mostly about the improvement of the neural CPU *hardware*, in particular via development of the prefrontal cortex.  We can think of this as improvements in the raw capacity for *fluid intelligence*.  But in fact, it could just as well be more about the *software* that drives this neural CPU -- that is, the specific knowledge that is learned via synaptic plasticity taking place through learning.  This corresponds with crystallized intelligence, which as noted in the intelligence chapter is more generally thought to develop over time.  Indeed, various evidence suggests that the raw capacity constraints of our working memory and other elements of the neural CPU are likely to be relatively fixed over time, and across different content domains.

For example, as we reviewed in the memory chapter, the same strong constraint of 4 items applies across a wide range of domains [@Cowan01; @LuckVogel97].  Furthermore, the main way that memory capacity is increased is by forming new *chunks* that integrate previously separate items.  These chunks are really just integrated knowledge representations shaped over learning, through synaptic plasticity mechanisms.  This then suggests that perhaps the best way to improve the capacity and function of our neural CPU is similarly by developing more powerful knowledge structures that enable more sophisticated kinds of abstract and formal reasoning, using essentially the same raw hardware capacity.  Likewise, we reviewed in the thinking chapter that brain training programs have been remarkably ineffective in improving people's general cognitive capacity, and mostly just improve their abilities to solve specific problems [@SimonsBootCharnessEtAl16].  Furthermore, Siegler's data on people's application of logical rules to various of Piaget's tasks shows a high level of variability and inconsistency, which again is consistent with the idea that specific experience-driven knowledge representations are playing a large role in shaping our reasoning abilities.

To summarize, it is likely that cognitive development is mostly about learning, learning, and more learning, and that through this process, synapses throughout the brain are shaped (and pruned), and this ultimately results in better chunks and abstractions that we can use to drive the limited capacity of our neural CPU.  At every age, we *always* find it challenging when we are pushed outside of the zone of the concrete and familiar, and adults really don't seem to ever achieve anything like perfectly logical formal reasoning abilities.  Nevertheless, we do have this neural CPU capability that apparently no other animal does, and with enough experience and learning, we can make it do some pretty amazing things.

### Social, Personality and Moral Development

![Fig 9-14: Erikson's stages of psychosocial development.  Each stage represents a fundamental challenge between two conflicting forces, and if the positive outcome is achieved, the corresponding virtue is obtained.  It was inspired by Freudian principles.](figures/fig_erikson_stages.jpg){ width=100% }

While Piaget focused mostly on cognitive development, other theorists focused on broader social, moral, and personality stages of development.  For example, **Erik Erikson** articulated a set of stages that were partly inspired by Freudian psychoanalytic principles (Figure 9-14), and feature a dramatic, almost literary battle between opposing forces at each stage [@Erikson56; @EriksonErikson98].  Like Freudian theory, and unlike Piaget, Erikson's ideas were based on observation and conjecture, not experimental data.  Nevertheless, they certainly resonate with many of the themes that people wrestle with across the lifespan, and are indeed often the subject of great works of literature.

Up through adolescence the overriding issues center around personal control, and a broader social sense of belonging.  The focus on individual control, autonomy, initiative, and industry are all very compatible with the idea that prefrontal cortex, which plays such a critical role in cognitive control, is among the brain areas developing throughout this same time period (Figure 9-12).  Furthermore, the motivational and emotional areas in ventral and medial prefrontal cortex also seem to undergo extensive periods of development.  Thus, in fact, Piaget and Erikson can be seen as describing two sides of the same developing coin!  This coin is the third of our three C's, *Control*, and again it emerges as the central feature of human cognition, motivation, and social orientation.

Erikson's focus on the **identity crisis** in adolescence is one of the most impactful aspects of his framework.  From an evolutionary psychology perspective, adolescents have to decide which tribe or group they will join -- will they stay with their parental group, or break away and start off in a new direction?  Will they try to become a leader or a lone wolf?  Likewise, adolescent humans face dramatic choices about which direction their lives will take, and what role they will play in society.  This is really the final step of the long process of asserting full independence and autonomy, that started back in the terrible twos (Erikson's 2nd stage).

In the sphere of personality development, **attachment theory** has been very influential, and it corresponds mostly with Erikson's first stage, about establishing a sense of trust or mistrust.  An early inspiration for this theory was the work of **Harry Harlow**, who raised monkeys without their mothers, and found that they instead became very attached to cloth surrogate mothers, and ended up treating the cloth like children often treat their *attachment object* (e.g., a favorite stuffed animal or blanket) -- they needed it around to feel comfortable, and became distressed when it was removed.  **Mary Ainsworth** studied infant attachment to their mothers, using a similar **strange situation** where the mother left the child alone with a stranger.

Infants exhibited three characteristic patterns of behavior: **Secure attachment**, where they were comfortable exploring when their mother was around, and distressed when the mother was absent; **Insecure-avoidant attachment** where they had little interest in the mother or distress at her absence; and **Insecure-ambivalent attachment**, where the infant sought maternal attention and did not explore the environment as much, and exhibited high levels of distress when the mother left.  Interestingly, in this latter case, the infants subsequently expressed ambivalence toward the mother when she returned, alternately seeking closeness and pushing her away.  Although evidence has shown that these early attachment behaviors are correlated with various factors later in life, it remains unclear to what extent these are actually reflecting individual differences in personality, and genetically inherited personality factors from the mother [@Harris09].  We'll learn more about this in the next chapter.

![Fig 9-15: Kohlberg's stages of moral development.](figures/fig_kohlberg_moral_stages.png){ width=50% }

**Lawrence Kohlberg** developed another stage-based developmental theory, in this case about *moral* development [@KohlbergHersh77] (Figure 9-15).  This framework was originally based on some ideas of Piaget's, and it features three major levels with sub-levels within each.  The first **preconventional** level characterizes the moral level of children: avoiding punishment and seeking rewards (self-interest).  This level directly corresponds to the dopamine-based learning of the basal ganglia, and can thus be considered the biological foundation of all human (and animal) decision making.  The **conventional** level is hypothesized to characterize the typical moral reasoning of a normal adult, based on both a seeking of social approval, and a respect for laws and a sense of duty to uphold them, to maintain social order.  Thus, the conventional level is could be considered the *social* level (i.e., social conventions), whereas the preconventional level is clearly the *individual* level. The highest level is **postconventional**, and involves moral reasoning based on a more abstract understanding of the necessity to preserve a social contract, and on abstract principles of basic human rights and ethics.

The correspondence with Piaget's focus on a progression toward more abstract, logical, rational thinking is evident in Kohlberg's stages.  Consistent with the challenges that people have with actually thinking logically and rationally, a major critique of Kohlberg's framework is that it is very difficult to find any people who apply his highest level of moral reasoning in their daily lives.

One particularly interesting feature of Kohlberg's approach is that he employed a structured interview format based on moral dilemmas, in particular this famous *Heinz dilemma*:

> A woman was on her deathbed. There was one drug that the doctors thought might save her. It was a form of radium that a druggist in the same town had recently discovered. The drug was expensive to make, but the druggist was charging ten times what the drug cost him to produce. He paid $200 for the radium and charged $2,000 for a small dose of the drug. The sick woman's husband, Heinz, went to everyone he knew to borrow the money, but he could only get together about $1,000 which is half of what it cost. He told the druggist that his wife was dying and asked him to sell it cheaper or let him pay later. But the druggist said: "No, I discovered the drug and I'm going to make money from it." So Heinz got desperate and broke into the man's laboratory to steal the drug for his wife. Should Heinz have broken into the laboratory to steal the drug for his wife? Why or why not?

Critically, there is no right answer to this dilemma -- Kohlberg instead was interested in what kind of reasoning people used in justifying their answers.  Those reasons typically aligned with his different stages, e.g., "Heinz should not have stolen the drug because that is breaking the law", consistent with the law & order conventional stage.

### Lifespan development

Erikson's stages were among the first to consider the full scope of the lifespan, beyond early development up to the start of adulthood.  Many challenges and issues remain even after the turbulence of adolescence!  There are now many scientists actively researching the cognitive, social, and emotional aspects of aging.  

<!--- todo: dopamine, suicide -->

## Summary of Key Terms

This is a checklist of key terms / concepts that you should know about from this chapter.

* Evolution:
    + adaptation: survival and reproduction of the fittest
    + exaptation: selection of something for one function later turns out to be useful for something else (e.g., an oxygen boost for then living on land, or feathers for cooling then flight).
    + genetic algorithm: principle of evolution implemented on a computer

* Genetics:
    + base pairs (G, C, T, A)
    + amino acid = 3 base pair sequence
    + protein = sequence of amino acids
    + 1.5% of genome codes directly for proteins
    + genes are units of heredity -- somewhat complicated to define

* Sexual reproduction:
    + zygote = fertilized egg cell
    + identical twins: monozygotic = same fertilized egg = 100% shared genes
    + fraternal twins: dizygotic = two different eggs = 50% shared genes on average
    + gamete: sperm and egg
    + meiosis: gametes go from 46 to 23 chromosomes, and crossover shuffling of parents genes
    + mitosis: normal cell division, preserves all 46 chromosomes
    + alleles: different versions of genes (most are the same in all people)
    + dominant vs. recessive and rare recessive diseases

* Heritability:
    + phenotype: overall property of organism (height, weight, IQ, etc..)
    + environment (shared, non-shared) vs. additive genetic contributions (ACE)
    + heritability as proportion of variance due to genes *relative* to environmental factors
    + reduced environmental variance in WEIRD societies
    + GCTA: directly based on genes of *random* people, not twins
    + missing heritability: GCTA and other direct techniques have about 1/2 the heritability
    + shared / common environment (C in ACE) is typically near 0: Judith Rich Harris: parents don't matter
	
* Development:
    + Piaget stages: sensorimotor, preoperational, concrete operational, formal operations
    + formal operations = functioning neural CPU based on prefrontal cortex, basal ganglia
    + balance scale, conservation tasks and multiple factor relationships: people are not logical; are variable across time and tasks
    + brain development = synaptic pruning, thinning; prefrontal cortex develops over long time
    + A-not-B, card sorting tasks and controlled processing
    + crystallized intelligence develops, fluid intelligence capacity ("hardware") remains relatively constant
    + Erikson's stages of psychosocial development: control, autonomy, and identity crisis in adolescence
    + attachment theory: Harry Harlow's motherless monkeys get attached to cloth
    + strange situation: secure attachment, insecure-avoidant, insecure-ambivalent
    + Kohlberg's stages of moral development: preconventional, conventional, postconventional

