---
bibfile: ccnlab.bib
---

# Chapter 1: Science and Subjectivity --- The Fundamental Challenge of Psychology

Psychology is the science that attempts to understand the human mind.  The human mind is the most fascinating and amazing "thing" in the known universe, and the idea that you can actually attempt to study it using the basic reductionistic approach of science may seem a bit of a stretch.  And indeed it has been --- but at this point in the development of the field, many practicing scientists are likely to feel at least somewhat confident that significant progress has been made, without fundamental, obvious limitations to how far we can go.

Despite all this progress and optimism, we will see in this chapter that there are fundamental boundaries to what science can penetrate, and these boundaries have shaped the field from its inception.  Thus, understanding these limitations helps put the field of psychology and neuroscience into perspective in multiple ways, and in fact many of the limitations we discuss apply to science, and all human knowledge, more broadly.

The central issue we must confront head-on is the inescapable problem of *subjectivity*.  By subjectivity we mean not just the fact that different people have different opinions or perspectives on things, though that is a big part of it.  Instead, we need to step back a bit to look at the *really big picture* (i.e., Philosophy), starting with the fundamental problem of subjectivity as expressed by **Rene Descartes** (way back in 1637), in his famous statement: *Cogito Ergo Sum* --- *I think therefore I am.*

There are two essential implications of this statement --- we'll explore the first one in depth before turning to the second.  The first implication is that *subjective experience is primary*.  If you put yourself into the mindset of a very skeptical, doubting philosopher, you might find yourself questioning just about everything, *except* this one, primary fact: you are *here* (wherever you are), *thinking*.  If you really push it, you might appreciate that you can't really be sure that the world itself exists outside of your mind!  This very challenging train of thought is well-captured in several modern movies, perhaps most notably in the *Matrix* series, where, in fact (in the movies at least), there turns out to be every reason to have such doubts.  In philosophical circles, this line of thinking is known as *solipsism*, and lest you think that this is just an irrelevant and obscure way of thinking, one of the great innovators of our time, Elon Musk, is apparently convinced that we're all living in a giant simulation.  He also apparently smokes entirely too much dope, but be that as it may.

This is the kind of all-encompassing subjectivity that we want to more fully understand and appreciate.  What does this line of thinking mean for the study of psychology, or science more generally?  

This is where we can usefully bring in Descartes' second major implication from *Cogito Ergo Sum*: *dualism*.  Dualism is the idea that there are two fundamentally different "substances" in the universe: the regular physical stuff of the everyday world, and this entirely separate, magical transcendent thing called *mind*, which lives apart from that other, regular stuff.  The opposing view is called *materialism*, where the mind is seen as just a product of the material world like everything else, and in particular a product of the physical processes taking place within the *brain*, as widely embraced in modern neuroscientific approaches to psychology.

You might be somewhat surprised to hear that many modern-day philosophers still embrace dualism, and one of the most outspoken advocates is David Chalmers, who argues that understanding the nature of subjective experience, or *qualia*, is the *hard problem* of consciousness and simply cannot be explained in objective, materialistic, scientific terms [@Chalmers95].

You might also be surprised to hear that, despite being one of those modern materialistic neuroscientists, I actually agree with Chalmers, and Descartes (in spirit at least, so to speak)!  I think that there are two fundamentally different "somethings" in the universe, but, unlike Descartes and Chalmers, I don't think the dividing line is between *mind* and *matter*, but rather, between *subjective* and *objective* perspectives [@Nagel74].

Following Descartes (again), we can take subjective experience as primary --- it is the only thing I am fully certain of.  But it is also primary in another, essential way: it is uniquely, completely, definitionally, *mine*.  It is literally impossible for *you* to experience *my* subjective experience, because, by definition, *my* subjective experience is exactly the sum-total of what it "feels like" to be me.  If we somehow were to add *you* into my brain, my subjective experience would be irreparably altered.  If you are somehow sharing in my subjective experience as it is happening, you would have to have direct access to every level of my brain, and not just "objective" access as you might get from a super-hi-tech future brain scanner, but *direct*, *internal*, *subjective* access, "from the inside out".

In other words, you would have to literally be inside my brain.  And you can't be inside my brain because I'm already here.  From the materialist perspective, we can identify my subjective experience as emerging directly from my brain --- it is what it feels like to be my brain.  If you truly appreciate this equivalence, then it should be readily apparent than there can be only one "mind" for every brain (we'll look into the fascinating phenomenon of multiple personality disorder later, but it doesn't change this fundamental conclusion --- all those personalities are just as irrevocably trapped inside the one brain as you and I are, and in fact we all have something like multiple personalities too). 

Another way of thinking about this is in terms of identical twins.  Let's imagine we have the most identical of identical twins ever to exist.  Their brains are *completely identical* in every way possible.  Would those twins have the same subjective experience? No.  They might have a great deal in common, but, fundamentally, they would not, and could not, directly experience exactly what the other is experiencing.  Why not?

It all boils down to *perspective*.  Each physical thing in the universe has its own unique perspective, if we take this term to mean a particular spatial location, and a particular trajectory through space and time in the past (and going onward into the future), that is fundamentally *unique* to that thing.  This is why the twins cannot share their subjective experiences: they are two separate, distinct things, and, inevitably, they "see the world" from two different vantage points.  The only way they could share experiences is if they could somehow superimpose themselves into exactly the same point in space, and do so over a sufficiently long time period to synchronize their history of experience, which plays such a critical role in our subjective life, in addition to the immediate sensations coming in from the outside world.

Anyway, the key point of all this is that *if* you allow that subjective experience can never be shared among different brains, *then* it follows that there is a fundamental divide between this inner subjective world, and the "regular" outside *objective* world.  I believe this divide captures the essence of what Chalmers is talking about in terms of the irreducible nature of the qualia of consciousness --- the impossibility of trying to explain in objective terms "what it feels like" to experience things in our subjective, inner world.  Furthermore, it does so without introducing anything particularly magical or fundamentally at odds with materialism: subjective experience is not separate from the physical world in terms of some kind of magical "substance" that it is constituted from --- it is just separate in terms of this notion of *perspective* --- the unique point of view (literally, where they are standing / sitting / looking) that each subjective being has all to themselves.

## Subjectivity in Psychology: A Brief History

Stepping back from this big philosophical abyss, what does it all mean for the attempt to study psychology as a science?  The primary, obvious problem is that psychology is the study of *what it is like to be a human being*, and if this is fundamentally a subjective thing that can never be directly shared with any other human being, how can we possibly hope to arrive at some kind of objective, scientific understanding?  Well, the first step is to follow Chalmers and attempt to *partition the problems* --- we can carefully attempt to set aside the *hard problems* associated with the nature of subjective experience, and focus instead on the so-called *easy problems* that are left over.  *If* there is enough interesting stuff left over in this space of easy problems, then it probably makes pragmatic sense to just see how far we can get in trying to understand that stuff, and then, once we seem to have exhausted that space, perhaps we could circle back and start reconsidering some of those hard problems.

This overall approach provides a reasonable narrative for the history of psychology as a scientific discipline.  The person most widely credited with founding the science of psychology, **Willhelm Wundt**, had the innovative idea in the late 1800's that, after millenia of armchair speculation, you could actually apply the techniques of empirical science to understanding the human mind / brain.  Wundt made many groundbreaking contributions, but his legacy, at least at the level of introductory psychology texts, is as a founder of the *introspectionist* school of psychology, which also includes **William James**, who also made major lasting contributions to the field.  When the next major paradigm shift took place in the early 1900's, it emerged as as strong reaction and rejection of this introspectionist approach, which was characterized as being overly concerned with all those hard problems of subjective experience.  Introspectionists would try to systematize and characterize the contents of subjective experience, and the hard-nosed *behaviorists* who came next regarded these investigations as insufficiently objective, rigorous, and replicable.  Instead, they emphasized purely objective, externally-observable *behavior* as the only valid data in psychology (hence the term behaviorism).  The main figures in this era (e.g., **John B. Watson, B. F. Skinner, and Ivan Pavlov**) focused on how external, objective factors such as reward and punishment affected subsequent behavior through *conditioning*.

Thus, these first two epochs of scientific psychology embody exactly this tension between the subjective and objective worlds.  The next paradigm shift took place in the 1950's and 60's with the *Cognitive Revolution*, riding the wave of digital computers, which made it fashionable to start talking about internal mental operations in terms of the *information processing model* of the mind --- i.e., the mind as a computational device.  Scientists leading this new field, such as **Herbert Simon** and **Alan Newell**, started thinking about how the mind could perform complex mental operations such as scientific proofs, chess, and other challenging tasks [@NewellSimon72].  People created running computer models of how these internal thought processes might work, which provided a compelling way to render that formerly "loosey-goosey" internal world in a much more rigorous, objectively-characterizable way.

However, as parallel work in the field of Neuroscience continued to advance, it gradually became clear that the brain really doesn't work anything like a standard digital computer.  Instead, it is really a *massively parallel* computer with billions of computing elements (neurons) that combine the functions of computation and memory, which are otherwise separated in a standard digital computer.  Psychologists **David Rumelhart** and **James McClelland** published a ground-breaking pair of books in the mid 1980's that popularized this new understanding of how information processing might work in the brain [@RumelhartMcClelland86; @McClellandRumelhart86], and subsequent advances in the ability to take high-resolution pictures of the activity inside the human brain (*neuroimaging*) have led to the currently-dominant paradigm that integrates neuroscience and cognitive psychology (i.e., *cognitive neuroscience*) to come up with coherent understanding of how exactly the brain gives rise to the phenomena of the mind.

## Fundamentals of Cognitive Neuroscience

This book is grounded squarely in this new paradigm of cognitive neuroscience, and attempts to provide a coherent set of core principles that connect directly from the basic processing carried out by individual neurons, all the way up to the highest levels of mental life.  We are still largely avoiding significant consideration of the vast inner world of subjective life, but there is a robust field studying the *neural correlates of consciousness* (NCC) that we will discuss in depth in Chapter 3.  Slowly but surely, we are building bridges between the objectively-identifiable properties of the human brain, and the subjective experiences that tend to co-occur with particular such brain states.  Thus, we are developing a richer objective understanding about the kinds of neural mechanisms that give rise to our subjective mental life.  But even with all of these advances, I don't think we could ever explain to a non-human-brain lifeform what it feels like subjectively to be a human brain.  Thus, the subjective world remains our own private dominion, and literature, art, and movies provide the richest vehicles for sharing those experiences across the inevitable subjective gap between us all.

## Subjectivity and Science: Working with the Method

The challenges imposed by the primacy of subjectivity have far-reaching implications beyond the field of psychology.  First, given that some people can't even agree that there *is* an objective, external world outside the mind, how can we possibly even begin to start talking about *objective knowledge* and *facts*?  This appreciation for the primary nature of subjective experience forces us to recognize that objective knowledge itself is entirely dependent on the subjective motivation of individuals to entertain a strong enough belief in this notion of objective reality, to put up with all the effort it takes to make any progress in understanding and advancing objective knowledge.  

Those individuals are called "scientists", and they follow a particular method, the **scientific method**, which has the following basic steps:

1. Come up with a general question or problem, e.g., based on an informal **observation** about something of interest (e.g., Newton observes the apple falling on his head, which gets him thinking..)

2. Form a specific **hypothesis** about how that something might work, which makes testable **predictions** (e.g., there is an invisible force called *gravity* that causes all objects to experience the same acceleration, making the testable prediction that a feather and a hammer should fall at the same rate *in a perfect vacuum* so as to eliminate the "confound" of friction).

3. **Collect data** that could actually test the predictions of the hypothesis, in comparison to other possible hypotheses (e.g., measure how fast things fall, ideally in a vacuum if you happen to have one of those lying around).  It is essential that the data be collected using a well-specified procedure that could be **replicated** by other scientists. 

4. **Analyze the data** to determine whether any effects observed are strong enough to be clearly distinguishable from random chance and noise.

5. **Draw conclusions** --- how compelling are the data, what holes are there in the data that would allow other hypotheses to explain the observed effects, etc?

6. Iterate!  Plug the holes, think of other alternative explanations, test those, etc.

These steps can incrementally pull us out of our individual subjective fortresses through the critical lever of **consistency**.  If you articulate a clear sequence of steps to perform an experiment, and tell me exactly what you observe as results, and I do the same thing to the best of my ability, and get *consistent* results, then it seems like there might be something *real* and *objective* going on, or at least the world isn't completely random.  As more and more people do the same thing, and continue to get consistent results, the odds that each one of us is just being individually tricked by some kind of subjective illusion would seem to go down.

As this scientific process continues, ever broader networks of interconnected hypotheses and associated empirical data accumulate, and if all of these remain somehow consistent with each other, it really starts to seem like there might be some kind of *laws* governing the behavior of the outside world.  Furthermore, all this scientific knowledge makes its way into technology, which depends on those same laws, further bolstering the network of consistency.  Fast forward to the modern world, and we now have the *standard model* of physics that provides a single consistent framework for understanding virtually all physical phenomena that have been subject to experiment, and drives incredible technology that would have been considered pure magic in times past.  

Despite all this amazing progress made through the iterative application of the scientific method, you still have people like Elon Musk, one of the great *users* of physical laws, nevertheless concluding that it is all a giant simulation.  And still plenty of people who believe that the Earth is flat, etc.  And there is *nothing* you can do to convince these people otherwise.  Such is the ultimate primacy of our subjective perspective on the world: the *only* porthole we have onto that supposed objective reality out there is through our very own, individual, subjective lenses.  Because our subjective worlds are fundamentally uniquely our own, this also means that nobody can force anyone to believe anything that they aren't otherwise prepared to believe.  Objective reality really is a second-class citizen, and is entirely dependent on the patronage of the ruling, sovereign subjectivity, just as scientists are still to this day dependent on the hard work and wealth of others to have the luxury of time and resources to create this huge network of consistent hypotheses and data.

Even within the scope of the scientific method, subjectivity abounds.  Where, exactly are these hypotheses, or conclusions, supposed to come from?  How many scientists looking at the exact same empirical data draw the same conclusions?  You'd be surprised how subjective and inconsistent cutting-edge science really is.  History is full of examples where a visionary pioneer was ridiculed by their colleagues, until enough evidence accumulated, and enough old people in power died, to allow the new ideas to flourish ("science advances one funeral at a time", according to Max Planck).  The widely-accepted description of how science actually works, developed by Thomas Khun [@Kuhn62], emphasizes this sociological, psychological reality of science, with one major consequence being the strong suppression of ideas that are inconsistent with the current paradigm.  

We can understand this phenomenon in terms of the three C's principles.  Compression says that people crave simplicity, and the current paradigm embodies that: it is something that a large number of people know and agree about.  Having that overturned requires confronting a high level of uncertainty and complexity.  Control is paramount here: that challenge to a widely-believed paradigm is experienced as a direct, personal challenge to your entire mental fortress --- psychologically, it is really the same as challenging someone's belief in a particular religion.   Furthermore, the uncertainty directly undermines the feeling of control as well.  And control interacts with contrast --- the "paradigm believers" constitute a social in-group, and anyone challenging the paradigm is immediately a strongly-contrasting out-group member, and all the deep tribal motivations are aroused in this case, causing the challenger to be treated like a real outcast and pariah.

In other words, science is just people being people.  However, despite all our limitations and inevitable subjectivity, there is some indication that following some approximation of the scientific method really does seem to work, at least over the longer arc of history.

Before we get more into the nuts and bolts of actual experiments and statistical analysis techniques in psychology and neuroscience, there is one further perspective on the problem of subjectivity in science that bears mentioning.   This comes from Robert Pirsig, who wrote the famous book, *Zen and the Art of Motorcycle Maintenance*, which is actually more about philosophy of science and personal autobiography, rather than Zen per se.  Pirsig literally went insane (as in, institutionalized, electroconvulsive shock therapy, etc) in the course of struggling with the question of where hypotheses come from --- he realized that there was no rational explanation for how to come up with a good hypothesis, and it seems like there could easily be an infinite number of plausible hypotheses, so this throws a massive monkey wrench into the entire rational foundation of science.

Thus, subjectivity, creativity, and individual genius truly lie at the heart of science --- most scientists are reasonably capable of evaluating hypotheses in terms of their consistency with data and with the larger network of other validated hypotheses, but relatively few scientists are responsible for coming up with the major hypotheses in the first place.  Oh, and by the way, Pirsig suffered from Schizophrenia so that probably had more to do with his mental breakdown than the problem with hypotheses, but anyway it makes for a good story.

## Research Methods in Psychology and Neuroscience

The one thing that everyone in science can agree upon is that *data* is essential!  You can come up with cool-sounding new hypotheses all day long, but ultimately the data has the final say (despite the grumbling of many a theoretician).  Nobel prizes strongly favor those who generate data, and even Einstein never got one for his general relativity theory!  In this section, we'll discuss the specific types of data that psychologists and neuroscientists tend to collect, and what kinds of analyses are typically done with that data.  We will cover this succinctly because it all *sounds* perfectly logical, but actually applying it requires a good deal of practice and experience, which is beyond the scope of this book, and likely the course you're currently taking.

In psychology, there are three major ways in which data is collected, each with complementary trade-offs:

* **Descriptive Methods** --- these tend to be the least *invasive* techniques, involving various ways of capturing what is actually happening in human behavior, such as observation, case studies, and surveys.  A modern version employs cell phones with apps that ping people at random times during the day and ask them what they're doing, or thinking about, etc.  The disadvantage of these techniques is in their relative inability to inform you about *why* people might be behaving the way they are --- the other two techniques improve on that aspect of things, but, particularly with the experimental method, tend to require more artificial, less naturalistic kinds of experiments.

* **Correlational Studies** involve measuring multiple different **variables** (something that can be measured which varies across people, such as weight, IQ, vocabulary, diet, etc) and determining the extent to which these variables **correlate** or vary systematically in relationship to each other.  For example, people's weight and height tend to be positively correlated, because as one goes up, the other does too.  Critically, as with most real-world data, this is not a **perfect** correlation --- there are many exceptions in either direction --- but overall, on average, there is a relationship.  The single most important limitation of correlational studies, is that the presence of a **correlation does not imply causation**.  Often, however, it does work the other way around: causation *does* typically produce correlations of some sort.  Interestingly, psychologists have shown that we're particularly bad at overgeneralizing these kinds of relationships [@Wason68], and thus we tend to falsely conclude that correlation does imply causation.

  In short, the human brain relies on correlation as a kind of "quick and dirty" shortcut for finding causal relationships in the world, and we find it remarkably difficult to recognize that the two are not equivalent.  For example, most studies on the effects of diet on health are correlational, and yet the media and even scientific papers regularly interpret these as showing a causal link.  "Drink more coffee because you'll live longer!"  Well, what if in fact the observed correlation between coffee and longevity is due to the fact that more wealthy people drink more coffee, and it is really the wealth and all its associated benefits that is driving the longevity.  Coffee is just "along for the ride".  This is the **third variable problem** (in this case, the third variable is wealth), and it is the bane of correlational studies, because *there is always a third variable* (and a fourth, and a fifth, etc).  And it is typically very difficult to rule out the possibility that everything is being caused by one of these unmeasured "third variables".

![**Figure 1.1:** Logic of an experimental study, using random assignment to eliminate third variables from the study participants.  It is also essential to minimze all other differences between the experimental and control conditions (i.e., *confounds*, or additional "third variables"), to more precisely identify the single *independent variable* (i.e., the *causal* variable) as truly being responsible for the differences measured in the *dependent variable*](figures/fig_expt_design.png){ width=100% }

* **Experimental Studies** are the only way to truly establish a causal relationship, and even then it is still a major challenge to really accomplish this feat.  The key trick is to use *randomness* and careful designs to attempt to systematically eliminate all possible "third variables".  A huge source of third variables is each individual person participating in the study.  Like all the bacteria on your skin, you are crawling with third variables.  Your genes, your upbringing, your neighborhood, your schools, your friends, your... everything, is a teaming cesspool of third variables!  The key trick in an experimental study is to use the cleansing power of randomness to wash away all those third variables, by **randomly assigning people to different conditions**.  No third variable can withstand the incredible power of such random assignment --- if we find a systematic difference between two completely random samples of the population, it cannot be due to their pre-existing conditions!

  However, random assignment is also the achilles heel of experimental studies, because it is often impossible to use random assignment for many questions of interest.  Can you really look at the effects of parenting style on subsequent emotional development, by randomly assigning kids to parents!?  Same goes with any long-term study on things like diet and lifestyle --- you can sometimes sorta force people to eat some particular diet over a period of a few months or so, but that just isn't going to work for the decades it likely takes for most diet effects to really impact overall health outcomes.  There are also other important ways of eliminating further possible third variables (typically called **confounds** in this context) from experiments, but random assignment is the most important (see Figure 1.1 for a diagram of the overall logic). 

In summary, each of these different techniques is most appropriate for different kinds of questions, given the different tradeoffs.  The key thing as a student and a citizen is to understand the limitations of any given study, so you can make an informed decision about what it really means.  And don't expect the media to do this for you.  Seriously, look at *any* correlational study on health / diet / etc and see how clearly the story, or the original article, discusses the limitations on any kind of causal implications from the study.

### Neuroscience methods

Methods in neuroscience (and cognitive neuroscience) tend to be either correlational or experimental.  The vast majority of **neuroimaging** studies are purely correlational, measuring the neural correlates of various different tasks or other manipulations performed while participants are in the brain scanner (we'll learn more about these scanners in the next chapter).  By now, the neural correlates of just about every possible human activity (yes, including sex) have been measured in a scanner.  But because of the correlational nature of these results, it is difficult to know whether the recorded brain activity is just *epiphenomenal* (i.e., just along for the ride), or whether it is really causal and somehow *responsible* for the behavior in question.

To attempt to address this causality question, scientists have used various forms of electrical and magnetic stimulation, which can disrupt or enhance neural firing in a relatively localized region of the brain.  For example, **transcranial magnetic stimulation (TMS)** applied over the primary motor cortex can cause your muscles to flinch.  However, just as with other experimental studies, the resulting brain states after TMS are not very "naturalistic", and it becomes difficult to interpret whether any changes in observed behavior are due to the disruption of the "normal" functioning of that brain area, or whether they just reflect the weird stuff that happens when you tweak that brain area in a completely unnatural way.

In animal neuroscience, much more precise causal inferences can be made by employing "invasive" techniques, such as directly cutting out different parts of the brain, or using modern **optogenetic** techniques to instantly and reversibly activate or deactivate a given population of neurons.  These optogenetic techniques allow very specific populations of neurons to be targeted, and have produced a powerful new wave of causal empirical data, showing that very precise manipulations to very specific neural populations can sometimes have impressive overall effects.  However, often even these results are over-interpreted and one must look very carefully for confounds in the resulting activity of other neural populations.  Virtually every neuron in the brain is within a few synapses of every other neuron (i.e., the "6 degrees of separation" (from Kevin Bacon) phenomenon), so it remains very difficult to isolate what each specific subset of neurons is uniquely contributing.  Indeed, as we'll see in the next chapter, the very premise of isolating specific functions may be entirely misguided.

Finally, animal neuroscience also affords much higher-resolution neuroimaging techniques which can resolve the activity of individual neurons, while also recording many such neurons at the same time.  Such techniques provide the most powerful descriptive methods for characterizing what neurons actually do, and historically have been some of the most important data for fueling our theorizing and understanding of how the brain works.

Thus, truly each different type of technique plays a critical role in the overall arsenal of science.

## Statistics

Finally, it is useful to be aware of the most widely used statistical techniques in psychology and neuroscience.  Here is a brief overview:

* **Descriptive Statistics** --- like descriptive methods, descriptive statistics are used to describe data, and differ from **inferential** statistics which are used to *infer* causality or correlation, as described below.  The primary descriptive statistics are probably familiar to you: *mean*, *median*, *mode*, *range* and *standard deviation*.  For a *normal* (bell-shaped, *gaussian*) distribution, the mean, median, and mode are all the same, and they tell you where the *middle* of the distribution is (i.e., the "average" person, etc).  It is only when the distribution is *skewed* that they differ, with the mode and median being less "pulled" by the long-tailed side of the distribution.  You may have heard of income being reported in terms of medians --- this is because income is a skewed distribution, with progressively fewer people making a *lot* more money than the mass of the "middle class" and below.  The median and the mode more accurately capture this "middle class" salary because they don't get pulled upwards as much by all the rich people (Figure 1.2).  

![**Figure 1.2:** Mean, Median, and Mode tell different stories when the distribution is skewed (in this case, it is *right*-skewed --- the skewer is the long tail to the right).  The mean is pulled up by the tail much more than the median or mode, which do a better job of capturing the "middle class" income.](figures/fig_income_skew.png){ width=75% }

* **Correlation Coefficient and Scatterplots** --- these are the primary tools for correlational studies.  The correlation coefficient is a number, typically labeled $r$, which goes between -1 and 1, where -1 represents a perfect negative correlation, 0 is the complete absence of a correlation, and 1 is a perfect positive correlation.  Importantly, both a strong negative and a strong positive correlation are equally important statistically, and indeed you can almost always just flip one of your variables around and turn one into the other (e.g., height vs. weight is positive, but "shortness" vs. weight is negative).  A scatterplot simply plots the value along each variable (one on the X or horizontal axis, and the other on the Y or vertical axis), with each dot representing a different person (or whatever else is being measured).  Thus, you can usually directly see the strength of the correlation in the shape of the "cloud" of such points (Figure 1.3).

  One critical "pro tip" for looking at such scatter plots is finding "outlier" points that might be carrying a huge amount of weight. Just as a person sitting further out on a see-saw has more impact than one sitting further in, data points that are far away from the center of the cloud carry much stronger weight, and if they happen to lie along one of the positive or negative diagonals, they can produce a strong apparent correlation, even when all the rest of the points in the middle are clearly just milling about and going nowhere in relation to each other.

![**Figure 1.3:** Scatterplot showing the positive correlation between length of gestation in the womb and overall lifespan, for different species of animals.  The Elephant in the figure is the outlier, carrying undue amount of weight on the overall correlation coefficient.  In this case, it is actually consistent with the rest of the data, but sometimes it is not, and yet the correlation still looks positive according to the $r$ value.  Thus, it is *essential* to *always* plot your raw data and ensure that the summary statistics are reflective of real aggregate effects!](figures/fig_scatterplot_outlier.png){ width=50% }

* **t-test, F-test (ANOVA) and the GLM** The "Student's" t-test is the most basic of the *inferential* statistics used in experimental studies.  It is *not* so-named because it is only for use by students, but rather it was the pen-name of the guy who invented it (William Gosset), to improve the quality of beer brewed by Guiness brewery in Ireland, no less!  Too bad it isn't called the "Stout" t-test.  Anyway, it basically tells you if the difference between your experimental group and your control group is big enough to *not* be due to random chance.  Thus, in applying this test, we "reject the null hypothesis" that our data is just random noise, but, critically, we're not actually *proving* that our favored hypothesis is correct.  We're just saying it is relatively unlikely to be pure noise.

  There are more "advanced" versions of this test, specifically the F-test used in the ANOVA (analysis of variance) procedure, and the full *generalized linear model (GLM)*, which can tell you about the importance of multiple different factors and their potential interactions.

  You may have heard about the *replicability crisis* in various fields of science, including psychology, where many results that were thought to be "true" have "failed to replicate" --- meaning that the original paper(s) reported a *significant* t-test result, and the subsequent ones did not (they instead found results consistent with pure noise).  This is actually to be expected about 5% of the time, given the standard for publication is set at this 5% level.  However, when you take into account how science is *actually* done, there are major systematic biases that enter into the process, which are not taken into account by these statistical tests, such that the actual effective probability of publishing garbage is closer to 50%!

  There are now important changes afoot to combat the worst of these biases, and help ensure that this garbage probability goes back down to closer to 5%.  But 5% itself is still a rather large number --- in physics the standard is one in 3.5 million!  And, amazingly, results that end up going into the "garbage" pile appear significant at levels below this standard, so randomness can sometimes be a challenging foe.

## Conclusions

All of science, but especially psychology, is challenged by the primacy of human subjectivity.  We are each sovereign nations unto ourselves, and science is more like policy making at the UN: it relies on slowly building up concensus among fundamentally capricious actors.  The scientific method, however, provides a recipe for attempting to find the key *consistency* across people and across time that builds the foundation for the growing objective understanding of our world.  In practice, if you don't worry too much about all these philosophical issues, progress is being made.  In the next chapter, we dive into the wealth of relatively new knowledge we have gained about how the brain works, building up a significant objective portrait to pair with the one painted by our own subjective worlds.

## Summary of Key Terms

This is a checklist of key terms / concepts that you should know about from this chapter.  As we'll learn in the memory chapter, it is a great idea to test yourself on what was said about each of these terms, and then go back and double-check --- that provides both beneficial repetition and also the *testing effect*.

* Subjective vs. Objective perspectives and the history of psychology
    + Rene Descartes, Cogito ergo sum, dualism
    + Materialism, objective reality
    + Wundt, James and *introspectionism*
    + Watson, Skinner, Pavlov and *behaviorism*
    + Newell & Simon and *cognitive, information processing* approach
    + Rumelhart & McClelland and the *cognitive neuroscience* approach
    + Neural correlates of consciousness

* Scientific Method
    + Observation, hypothesis, data, replication, analysis, conclusions
    + Consistency over time, across people
    + Paradigms, social, psychological forces in science
	
* Methods:
    + Descriptive, Correlational, Experimental: pros and cons
    + Third variable problem
    + Power of random assignment

* Statistics:
    + Descriptive: mean, median, mode, range, standard deviation.
    + Correlational: $r$, scatterplot
    + Inferential: t-test, F-test (ANOVA)
	

