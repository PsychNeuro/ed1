# Chapter 7: Thinking, Control and Intelligence

What is *smart*?  This is the fundamental question for this chapter, with many profound personal and societal implications.  Is there just one kind of smart, or are there multiple different forms of intelligence?  How can we reconcile any form of *general* intelligence with everything we've learned up to this point, about how the brain works at a biological level?  The brain is composed of billions of neurons, interconnected by vast networks of synapses, wherein all of our knowledge, and, presumably, intelligence, must lie.  Do "smart" people have more neurons or synapses?  Or, perhaps, *fewer* synapses?  Are their neurons somehow fundamentally different from other people who measure as less smart according to standard intelligence tests?  And what are those intelligence tests measuring anyway?  Are they really some kind of "pure" measure of intelligence, or do they just reflect the degree of western-style education (and health and wealth) that a person has?  What does your IQ score really tell us about you as a thinker, and about your prospects for future success in school and the real world?  So many important questions!

If our brains were more like digital computers, these questions would have much simpler answers.  It is relatively easy to measure the power and speed of a computer, and many people tend to think of human intelligence in these terms.  As we discussed in the previous chapter, a computer has discrete parts (the CPU, RAM, and hard drive), and each of these parts can be directly quantified in terms of its capacity and speed.  If you're at all savvy about these things, you can obsess about getting the best value for your money along each of these dimensions, and, generally speaking, the faster the CPU and the more RAM and hard-drive storage, the more you can achieve with your computer.  Computers really do come in obvious degrees of "smartness".

But our brains are nothing like that of a digital computer.  We do *not* have a CPU (at least, not one like that in a standard computer -- more on this in a moment).  Cognition emerges out of the interactions of billions of chattering neurons, which are fundamentally shaped by learning processes over an extended period of time.  As we will explore in the development chapter, we start out with virtually no discernible intelligence (despite how cute and special our parents think we are), and it takes most people a few *years* to even learn how to control their own bowels!  Wow.  The rest of the animal kingdom must think we are complete idiots, which comports with an amusing *Onion* headline to that effect.

Given that we clearly don't start out with much in the way of intelligence, it seems hard to escape the conclusion that intelligence is fundamentally a product of learning (in concert with other developmental / maturational changes).  And this view is also hard to avoid when you think about all those synapses that need to get wired up in just the right way to produce whatever cognitive abilities we end up with.

So are "smart" people just better learners then?  If so, what makes some people better at learning than others?  When we explored this question in the Learning chapter, one of the major conclusions is that learning is driven fundamentally by *motivation*, and all that dopamine and related machinery that gets us up in the morning and ready to pursue our daily goals, etc.

Indeed, we will review various sources of evidence that are consistent with the overall idea that motivational differences play an outsized role in determining measured level of intelligence.  Of course, there are many, many complex factors that shape an individual's trajectory of learning and development, and motivation is itself a multi-faceted thing, so perhaps we aren't explaining too much when we say that motivation plays an important role.

But understanding the major factors shaping intelligence may affect how we think about ourselves, and others, in important ways.  If we view intelligence as a product of learning and motivation, then it is more obviously malleable.  This is the critical difference between a **fixed mindset** about intelligence, versus a **growth mindset**, as emphasized by *Carol Dweck* and colleagues [@Dweck08], in an increasingly influential body of work.  The growth mindset emphasizes that intelligence is not something that people "have", but rather, something they have to cultivate -- something that grows over time.  Increasingly, schools and teachers are recognizing that motivational factors have a huge impact on educational success, and they are developing innovative ways of motivating students to learn, and making the material more obviously self-relevant.

Fundamentally, the idea that intelligence is largely the product of time spent learning means that **anyone can learn anything**, if they only have sufficient motivation and time to invest into it.  This open-ended, ambitious view of intelligence surely has the effect of opening up your individual horizons and sense of what is possible.  Personally, I have always had this belief, and I have learned lots of complicated things, often slowly and with great difficulty.  Eventually, things that once seemed impenetrable become just another familiar part of my mental toolkit.  I have a very salient early memory of spending far longer than my peers figuring out how to simply connect a battery to some gadget in a summer school class as a kid.  I felt like an idiot.  But eventually, I figured it out, and learned this valuable lesson that, with sufficient effort, I could succeed.

Hopefully, you are now motivated to learn more about the history and current state of understanding about the nature of human intelligence, and the thinking processes that underlie it!  We'll start off by exploring the core questions of what "thinking" is, and what kinds of brain mechanisms are particularly important for it.  The conclusion from this may seem to contradict what was just said above: maybe we *do* have something like a CPU in our heads after all -- except it is a CPU made out of neurons and brain systems, and it runs on dopamine!  This is an important example of an *emergent* system, like the gears we talked about in the neuroscience chapter: the overall function of a CPU can be supported by various different "substances", just like the gears can be made of many different materials, and yet still function more-or-less the same.

Nevertheless, our neural CPU has major differences from a computer CPU, and the fact that it is made of neurons does have important implications for how it works.  Indeed, one can understand a lot about the particular strengths and limitations of human cognitive function, in terms of the overall idea that we can do both neuron-like computation, *and* something that approximates the function of a digital CPU.  We have yet to develop powerful AI (artificial intelligence) systems that capture this unique combination of both forms of computation, and perhaps once we do, we will unlock the real magic of our brains!

After gaining a better understanding of the "mechanics" of intelligence, we'll review the history of thought about the nature of intelligence, and how it has been measured.  Furthermore, we'll examine the data about the real-world implications of IQ test scores, and circle back to these big questions about the relationship between intelligence and motivation.

Another way of thinking about all of these issues, is in terms of the *control* component of our three-C's.  Our neural CPU serves as a kind of overall control system for the rest of our brain, and, as we have emphasized, this is fundamentally a *motivated* form of control, focused on getting us the things we need and want, and avoiding all the bad stuff.  Thus, the idea that motivation and intelligence are inextricably intertwined makes perfect sense from this perspective: the brain systems supporting our control systems (in the prefrontal cortex and basal ganglia) are the very same ones that directly interface with lower-level motivational and emotional pathways in the amygdala and dopamine system.

## The Neural CPU in the Prefrontal Cortex and Basal Ganglia

![Fig 7-1: The components of a Turing machine: with just three basic components, any computation can be performed!](figures/fig_turing_machine.jpg){ width=50% }

If we are to have something like a computer CPU in our brains, we at least have a pretty good idea of the kinds of properties such a system should have.  At the most abstract level, *Alan Turing* and *John Von Neumann* worked out the basic principles of a *universal* computational device in the 1930's and 40's [@Turing36; @vonNeumann45] -- something that could in principle do *anything*.  Amazingly, this device is remarkably simple (Figure 7-1).  It only requires a way of reading and writing information from a memory system (conceptualized as a *tape* by Turing), and a program that determines how this information is transformed in between being read and written.  It also requires some *active* memory where things can be temporarily cached, for the program to refer to.  These elements were elaborated by Von Neumann, in one of the most important unpublished papers of all time [@vonNeumann45], creating the foundation for modern digital computers.  Now days, we take it for granted that computers can do almost anything, but this was just theory not so long ago.

![Fig 7-2: Computers solve problems by breaking them down into many small *sequential* steps, each one involving a specific, well-defined operation such as adding numbers, writing them down somewhere, and reading them back in for use later.  Just like you do when performing multi-digit arithmetic.  Alan Turing showed that these basic processes can be used to solve *any* problem.](figures/fig_mental_multiplication.jpg){ width=80% }

You can get a good feel for how a computer works, and why it can do anything, by considering the traditional strategies for performing multi-digit arithmetic (Figure 7-2).  Instead of just staring at those big numbers, you break the problem down into a sequence of simple, discrete steps.  That sequence of steps is the *program* or **algorithm**, and each individual *operation* involves one of a small set of different processes, such as adding or multiplying single-digit numbers, writing down some numbers for later use (i.e., storing onto the tape in a Turing machine), and reading those numbers back in at the appropriate time (as you move to the next column of digits).

This kind of sequential, discrete, step-wise processing is entirely different from how our neurons work.  Neurons also break down a problem in to simpler components, but a critical difference is that they all work together in *parallel* instead of the fundamentally sequential, *serial* processing required for a universal computer.  The major advantage of serial processing is that it is much more flexible -- any arbitrary combination of steps can be executed in sequence, but the same is *not* true for parallel computation.  In the case of multi-digit multiplication, you have to do the tens-place part of the problem first, before you know how much to carry over to the higher digits, etc -- you can't just do everything all in one step.  More generally, parallel systems are really good at doing the same kind of thing over and over again really fast (e.g., detecting patterns), but they are not so good at doing fundamentally *different* things.  I.e., they are not naturally *universal* computational systems.

A critical implication of this is that you need to use your "mental CPU"-like capacity whenever you take on a fundamentally novel task.  For example, when you first learn to drive a car, you rely on this same kind of sequential, deliberate process that consumes all of your attention, as you keep reminding yourself of what you are supposed to be doing at each point in time.  However, with sufficient practice over time, these slow effortful processes gradually become **automated**, and you may now find yourself driving down the freeway with very little awareness of any of the underlying steps you're effortlessly performing.  This difference between the initial effortful __controlled processing__ and the subsequent __automatic processing__ was captured in a highly influential pair of papers by *Walter Schneider* and *Richard Shiffrin* (the same one who published the famous paper on the modal model of memory from the previous chapter) [@SchneiderShiffrin77; @ShiffrinSchneider77].

Thus, a major, unique feature of human intelligence is our ability to use this deliberate, sequential, controlled processing when taking on novel, challenging tasks, and then gradually learn how to "compile" the elements of these tasks into much more efficient, automatic forms of processing that more directly leverage the parallel power of our underlying neural networks.  Other animals only have this more automatic, parallel, neural form of processing -- we are unique in being able to act somewhat like a flexible, universal digital computer when needed.

### What it takes to be a Computer

The reason we can function like a computer is that we have some special capacities lacking in other types of brains, supplying the key ingredients of a Turing machine:

* _Program:_  we use oure *natural language* as a kind of programming language.  There is abundant evidence that we routinely use verbal self-instruction to remind ourselves of what we're supposed to do next in a complex, novel task.  We literally talk ourselves through the problem, and this capacity for stringing together different such verbal programs is an essential element of flexible, universal computation.

* _Active Memory_ (registers, cache memory): special properties of our frontal cortex and basal ganglia give us the ability to maintain a small amount of information in active, **working memory**, as mentioned in the previous chapter.  This is what you use when solving a mental arithmetic problem, by constantly juggling the digits around in your working memory.

* _Controlled Memory Storage and Retrieval_: we also have the ability to take control over our hippopcampal episodic memory system, to deliberately encode and retrieve task-relevant information as needed, playing the role of the memory tape system in the Turing machine.



Thus, we are left with this rather startling conclusion: our super huge brains packed with neurons are in many ways no match for a simple serial computational device composed of just a few basic parts.  Indeed, growing up in the era when cheap digital calculators first became widely available, it became very clear that while our brains are impressive in some ways, they pale in comparison to a dime-store calculator for doing basic arithmetic!





* Baddeley working memory

* PFC

* active maintenance

* "juggling" of information and bg "gating"

* Turing machines and CPU's: The inherent flexibility of sequential processing: universal computation

* Controlled vs. automatic processing, stroop task


## Differences between Computers and our Neural CPU's

* computers can do prodigious amounts of math & statistics, very quickly

* we kinda suck at that stuff

* but we have neurons!!  those neurons can use *parallel* pattern recognition kinds of strategies to solve problems in a "pragmatic" way..

* heuristics and biases..


## Measuring Intelligence and its Implications

* IQ tests

* Multiple intelligences

* real-world implications

* Dweck etc 

* Vogel WM study

* Miyake & Friedman and the genetic basis of IQ: gets stronger over time, just like a learning system.

* important considerations in interpreting genetic results..




