# Chapter 7: Thinking, Control and Intelligence

What is *smart*?  This is the fundamental question for this chapter, with many profound personal and societal implications.  Is there just one kind of smart, or are there multiple different forms of intelligence?  How can we reconcile any form of *general* intelligence with everything we've learned up to this point, about how the brain works at a biological level?  The brain is composed of billions of neurons, interconnected by vast networks of synapses, wherein all of our knowledge, and, presumably, intelligence, must lie.  Do "smart" people have more neurons or synapses?  Or, perhaps, *fewer* synapses?  Are their neurons somehow fundamentally different from other people who measure as less smart according to standard intelligence tests?  And what are those intelligence tests measuring anyway?  Are they really some kind of "pure" measure of intelligence, or do they just reflect the degree of western-style education (and health and wealth) that a person has?  What does your IQ score really tell us about you as a thinker, and about your prospects for future success in school and the real world?  So many important questions!

If our brains were more like digital computers, these questions would have much simpler answers.  It is relatively easy to measure the power and speed of a computer, and many people tend to think of human intelligence in these terms.  As we discussed in the previous chapter, a computer has discrete parts (the CPU, RAM, and hard drive), and each of these parts can be directly quantified in terms of its capacity and speed.  If you're at all savvy about these things, you can obsess about getting the best value for your money along each of these dimensions, and, generally speaking, the faster the CPU and the more RAM and hard-drive storage, the more you can achieve with your computer.  Computers really do come in obvious degrees of "smartness".

But our brains are nothing like that of a digital computer.  We do *not* have a CPU (at least, not one like that in a standard computer -- more on this in a moment).  Cognition emerges out of the interactions of billions of chattering neurons, which are fundamentally shaped by learning processes over an extended period of time.  As we will explore in the development chapter, we start out with virtually no discernible intelligence (despite how cute and special our parents think we are), and it takes most people a few *years* to even learn how to control their own bowels!  Wow.  The rest of the animal kingdom must think we are complete idiots, which comports with an amusing *Onion* headline to that effect.

Given that we clearly don't start out with much in the way of intelligence, it seems hard to escape the conclusion that intelligence is fundamentally a product of learning (in concert with other developmental / maturational changes).  And this view is also hard to avoid when you think about all those synapses that need to get wired up in just the right way to produce whatever cognitive abilities we end up with.

So are "smart" people just better learners then?  If so, what makes some people better at learning than others?  When we explored this question in the Learning chapter, one of the major conclusions is that learning is driven fundamentally by *motivation*, and all that dopamine and related machinery that gets us up in the morning and ready to pursue our daily goals, etc.

Indeed, we will review various sources of evidence that are consistent with the overall idea that motivational differences play an outsized role in determining measured level of intelligence.  Of course, there are many, many complex factors that shape an individual's trajectory of learning and development, and motivation is itself a multi-faceted thing, so perhaps we aren't explaining too much when we say that motivation plays an important role.

But understanding the major factors shaping intelligence may affect how we think about ourselves, and others, in important ways.  If we view intelligence as a product of learning and motivation, then it is more obviously malleable.  This is the critical difference between a **fixed mindset** about intelligence, versus a **growth mindset**, as emphasized by *Carol Dweck* and colleagues [@Dweck08], in an increasingly influential body of work.  The growth mindset emphasizes that intelligence is not something that people "have", but rather, something they have to cultivate -- something that grows over time.  Increasingly, schools and teachers are recognizing that motivational factors have a huge impact on educational success, and they are developing innovative ways of motivating students to learn, and making the material more obviously self-relevant.

Fundamentally, the idea that intelligence is largely the product of time spent learning means that **anyone can learn anything**, if they only have sufficient motivation and time to invest into it.  This open-ended, ambitious view of intelligence surely has the effect of opening up your individual horizons and sense of what is possible.  Personally, I have always had this belief, and I have learned lots of complicated things, often slowly and with great difficulty.  Eventually, things that once seemed impenetrable become just another familiar part of my mental toolkit.  I have a very salient early memory of spending far longer than my peers figuring out how to simply connect a battery to some gadget in a summer school class as a kid.  I felt like an idiot.  But eventually, I figured it out, and learned this valuable lesson that, with sufficient effort, I could succeed.

Hopefully, you are now motivated to learn more about the history and current state of understanding about the nature of human intelligence, and the thinking processes that underlie it!  We'll start off by exploring the core questions of what "thinking" is, and what kinds of brain mechanisms are particularly important for it.  The conclusion from this may seem to contradict what was just said above: maybe we *do* have something like a CPU in our heads after all -- except it is a CPU made out of neurons and brain systems, and it runs on dopamine!  This is an important example of an *emergent* system, like the gears we talked about in the neuroscience chapter: the overall function of a CPU can be supported by various different "substances", just like the gears can be made of many different materials, and yet still function more-or-less the same.

Nevertheless, our neural CPU has major differences from a computer CPU, and the fact that it is made of neurons does have important implications for how it works.  Indeed, one can understand a lot about the particular strengths and limitations of human cognitive function, in terms of the overall idea that we can do both neuron-like computation, *and* something that approximates the function of a digital CPU.  We have yet to develop powerful AI (artificial intelligence) systems that capture this unique combination of both forms of computation, and perhaps once we do, we will unlock the real magic of our brains!

After gaining a better understanding of the "mechanics" of intelligence, we'll review the history of thought about the nature of intelligence, and how it has been measured.  Furthermore, we'll examine the data about the real-world implications of IQ test scores, and circle back to these big questions about the relationship between intelligence and motivation.

Another way of thinking about all of these issues, is in terms of the *control* component of our three-C's.  Our neural CPU serves as a kind of overall control system for the rest of our brain, and, as we have emphasized, this is fundamentally a *motivated* form of control, focused on getting us the things we need and want, and avoiding all the bad stuff.  Thus, the idea that motivation and intelligence are inextricably intertwined makes perfect sense from this perspective: the brain systems supporting our control systems (in the prefrontal cortex and basal ganglia) are the very same ones that directly interface with lower-level motivational and emotional pathways in the amygdala and dopamine system.

## The Neural CPU in the Prefrontal Cortex and Basal Ganglia

![Fig 7-1: The components of a Turing machine: with just three basic components, any computation can be performed!](figures/fig_turing_machine.jpg){ width=50% }

To understand what kind of neural machinery it would take to support CPU-like functionality in the brain, we start with the surprisingly simple mechanisms needed to make a computer work.  At the most abstract level, *Alan Turing* and *John Von Neumann* worked out the basic principles of a *universal* computational device (something that could in principle do *anything*) in the 1930's and 40's [@Turing36; @vonNeumann45].  Amazingly, this device only requires three essential components (Figure 7-1): 1. A way of reading and writing information from a memory system (conceptualized as a *tape* by Turing); 2. A *program* that determines how this information is transformed in between being read and written; and 3. Some *active* memory where things can be temporarily cached, for the program to refer to.  These elements were elaborated by Von Neumann, in one of the most important unpublished papers of all time [@vonNeumann45], creating the foundation for modern digital computers.  Now days, we take it for granted that computers can do almost anything, but this was just theory not so long ago.

![Fig 7-2: Computers solve problems by breaking them down into many small *sequential* steps, each one involving a specific, well-defined operation such as adding numbers, writing them down somewhere, and reading them back in for use later.  Just like you do when performing multi-digit arithmetic.  Alan Turing showed that these basic processes can be used to solve *any* problem.](figures/fig_mental_multiplication.jpg){ width=80% }

You can get a good feel for how a computer works, and why it can do anything, by considering the traditional strategies for performing multi-digit arithmetic (Figure 7-2).  Instead of just staring at those big numbers, you break the problem down into a sequence of simple, discrete steps.  That sequence of steps is the *program* or **algorithm**, and each individual *operation* involves one of a small set of different processes, such as adding or multiplying single-digit numbers, writing down some numbers for later use (i.e., storing onto the tape in a Turing machine), and reading those numbers back in at the appropriate time (as you move to the next column of digits).

This kind of sequential, discrete, step-wise processing is entirely different from how our neurons work.  Neurons also break down a problem in to simpler components, but a critical difference is that they all work together in *parallel* instead of the fundamentally sequential, *serial* processing required for a universal computer.  The major advantage of serial processing is that it is much more flexible -- any arbitrary collection of operations can be sequenced one after the other over time, but the same is *not* true for parallel computation.  Some operations are mutually incompatible with each other, or depend one on the other, and simply cannot be performed simultaneously in parallel.  Indeed, one of the great challenges of modern computer science is trying to come up with even moderately usable parallel computing frameworks, and it is very clear that the universal flexibility of traditional serial computation does not extend into the parallel realm: parallel computation must generally be setup on a case-by-case basis.  For example, in the case of multi-digit multiplication, you have to do the tens-place part of the problem first, before you know how much to carry over to the higher digits, etc -- you can't just do everything all in one step.

More generally, parallel systems are really good at doing the same kind of thing over and over again really fast (e.g., detecting patterns via networks if interacting neurons in our brain), but they are not so good at doing random, arbitrary, *different* things, which is precisely where serial computation excels.  However, serial computation is inherently much slower (one step at a time).  These fundamental tradeoffs between parallel and serial computation mean that a system that can do both will be able to achieve the best of both worlds -- that is the magic recipe that the human brain has achieved.  Our brains are parallel at the level of individual neurons and networks of neurons, but at the larger *systems* level of the brain, we can achieve a form of flexible, serial computation.

Before turning to the biology of the brain systems supporting this latter form of computation, we can see strong evidence for the presence of these two different forms of computation at the psychological level.  For example, we would predict that you need to use your "mental CPU"-like capacity whenever you take on a fundamentally novel task.  For example, when you first learn to drive a car, you rely on this same kind of sequential, deliberate process that consumes all of your attention, as you keep reminding yourself of what you are supposed to be doing at each point in time.  However, with sufficient practice over time, these slow, effortful processes gradually become **automated**, and you may now find yourself driving down the freeway with very little awareness of any of the underlying steps you're effortlessly performing.  This difference between the initial effortful __controlled processing__ and the subsequent __automatic processing__ was captured in a highly influential pair of papers by *Walter Schneider* and *Richard Shiffrin* (the same one who published the famous paper on the modal model of memory from the previous chapter) [@SchneiderShiffrin77; @ShiffrinSchneider77].

![Fig 7-3: The Stroop task (Stroop, 1935; MacLeod, 1991) demonstrates the difference between controlled and automatic processing, in the context of reading words in different ink colors.  If you try to name the color of the word "Green" when it is written in red ink (the *Conflict* condition), the automatic process of reading dominates over the relatively rare process of naming ink color, and you have to deploy controlled processing, which takes extra time as show in the plotted data.  Reading words (bottom line) isn't affected much by the ink color one way or another -- the well-trained brain networks supporting this process just proceed in parallel without any supervision required.  Interestingly, even when the response is identical in the *Congruent* case, the task of color naming is still slower than word reading, reflecting the extra control being exerted.  The *Control* condition involves either non-color word reading, or pure color naming.](figures/fig_stroop_data.png){ width=50% }

A widely-studied example of this difference between controlled vs. automatic processing is shown in Figure 7-3 -- the *Stroop* task [@Stroop35; MacLeod91].  The participant is instructed to either read the word or name the color of the ink the word is written in.  Because word reading is so overpracticed, it is an automatic process for most adults (to the point that you often can't stop yourself from re-reading the annoying text on your cereal box every morning).  Therefore, when confronted with the diabolical *Conflict* condition where a color word (e.g., "Green") is written in a different ink color (e.g., red), it takes extra cognitive control to prevent yourself from just blurting out the word ("Green"), when the task is to name the color (for which you have much less practice).  This shows up as a significant delay (and overt errors) in this condition.  Interestingly, even when the ink color and the word are *Congruent*, there is still a delay associated with naming the color -- there is extra control being exerted to support this relatively unfamiliar color naming process.

If you run the Stroop task on kids, they don't show the same effects -- reading has yet to become automatic in them.  Furthermore, some Stroop researchers spent enough time color naming so that *it* become more automatic than reading, and they showed a kind of reverse-Stroop effect!  Thus, this process of *automatization* is dependent on learning and practice -- over time, our brains naturally turn deliberate, sequential, controlled processing into more parallel, automatic fast processing.  This is the same phenomenon that occurs for driving and so many other tasks that you once found difficult and mentally all-consuming.  Automatization is analogous to the *chunking* process discussed in the memory chapter -- we have a very limited active memory capacity, but once we learn new concepts, we can greatly expand our capacity by using this limited capacity on chunks of information that used to be separate.  Automatization is the process of forming *procedural* chunks -- combining sequential steps into faster parallel processing that becomes relatively independent of our mental CPU -- we no longer need to exert detailed conscious effort keeping the process moving along.

### What it takes to be a Computer

The human brain is unique in having the ability to function like a Turing machine -- other animals have plenty of automatic parallel processing skills, but they just don't seem to be capable of solving novel, complex tasks by performing a sequence of mental processing steps.  The reason we can function like a computer is that we have some special capacities lacking in other types of brains, supplying the key ingredients of a Turing machine:

* _Program:_  we use our *natural language* as a kind of programming language.  There is abundant evidence that we routinely use verbal self-instruction to remind ourselves of what we're supposed to do next in a complex, novel task.  We literally talk ourselves through the problem, and this capacity for stringing together different such verbal programs is an essential element of flexible, universal computation.  It is unclear how far we might be able to get at flexible controlled processing without language, but likely not very far.

* _Active Memory_ (registers, cache memory): special properties of our *frontal cortex* and *basal ganglia* give us the ability to maintain a small amount of information in active, **working memory**, as mentioned in the previous chapter.  This is what you use when solving a mental arithmetic problem, by constantly juggling the digits around in your working memory.  Working memory replaces the piece of paper you would otherwise use in keeping track of all the *partial products* and *control state* needed to keep progressing through a complex problem.  It is also essential for maintaining the program itself, and in this way it much resembles the function of RAM in a computer, which maintains both the program and the *stack* and *heap* forms of active memory needed to carry out the program.

* _Controlled Memory Storage and Retrieval_: we also have the ability to take control over our hippopcampal episodic memory system, to deliberately encode and retrieve task-relevant information as needed, playing the role of the memory tape system in the Turing machine, and a hard drive in a modern computer.

![Fig 7-4: How the basal ganglia (BG) can "gate" information into different parts of prefrontal cortex (PFC) to achieve a flexible kind of variable binding like that needed for digital computers.  In this case, the BG can flexibly control whether a given word / concept ("Max") is encoded as the agent or patient in a given sentence / scenario, based on other available cues or context.](figures/fig_gating_var_binding_sci_06.png){ width=70% }

Consistent with the central role of the frontal cortex in making our mental computer work, this brain area (particularlly the part of it in front of the primary motor area -- the **prefrontal cortex**) is differentially expanded in humans relative to other primates and mammals more generally.  Thus, a simple story is that our unique mental computer skills are due to this expanded brain area.  However, we still do not know exactly how neurons in this brain area come to act like a computer program (see Figure 7-4 and [@OReilly06] for some ideas).  One thing we do know, however, is that the capacity of our prefrontal working memory system is dramatically smaller than that of even the most primitive digital computer.  Thus, we are left with this rather startling conclusion: our super huge brains packed with neurons are in many ways no match for a simple serial computational device composed of just a few basic parts.  Indeed, growing up in the era when cheap digital calculators first became widely available, it became very clear that while our brains are impressive in some ways, they pale in comparison to a dime-store calculator for doing basic arithmetic!

In summary, although we are unique in having *some* ability to perform flexible serial controlled processing to solve novel tasks, our brain is still running in automatic, parallel processing mode under the hood, and that greatly limits our computer-like abilities.  What we lack in serial computing abilities, we try to make up for with all the amazing mental skills that we have automatized through learning and practice.  This is consistent with the idea that motivation, which drives this learning, plays such an important role in human intelligence.  And it also makes sense of the many ways in which our cognition differs from that of an optimal, fully rational  computer system, as we discuss in a later section.  However, we first consider an alternative possibility for at least some individual differences in intelligence.

### Individual Differences in Prefrontal Cortex / Basal Ganglia?

The critical role for working memory, cognitive control, and the prefrontal cortex / basal ganglia system in supporting our flexible computer-like cognitive abilities does introduce another possible explanation for individual differences in intelligence, however.  It *could* be the case that different people somehow have different capacities / speeds / functionality in these particular brain systems, and that is what explains overall differences in intelligence.  Furthermore, *if* this were the case, then because this flexible controlled-processing system is used as the first step in learning new skills and cognitive abilities (especially in math and other school-based learning), then there could be a kind of snowballing effect where small initial differences in these brain areas could multiply over time, leading to larger overall differences in measured IQ scores.  This kind of scenario is most similar to the "traditional" notions of intelligence as a fixed thing that you either have or don't have, and obviously fits well with intuitions based on the computer metaphor of the mind.

But how well does it fit with the available data?  First of all, it is well-established that major disruption to the prefrontal cortex results in impairments to controlled processing, for example on the classic Stroop task [@CohenServanSchreiber92; @StussFlodenAlexanderEtAl01], and on many aspects of social, moral and other forms of reasoning [@EslingerFlaherty-CraigBenton04].  However, the latter case showed that measured IQ scores can be relatively intact even with significant early frontal brain damage [@EslingerFlaherty-CraigBenton04], suggesting that the relationship may be somewhat more complicated.

The most relevant question, however, is whether *normal* variation in prefrontal cortex / basal ganglia function accounts for much of the measured individual differences in intelligence?  One early attempt to answer this question relied on establishing correlations between measures of working memory and intelligence, and came up with a strong positive correlation on the order of .6 to .8 [@Engle02].  However, subsequent work largely undermined that conclusion, instead suggesting that there is a separate factor for general fluid intelligence, independent of working memory capacity [@Engle18].

Another angle on this question found that much of the measured differences in working memory capacity were actually due to motivational factors in the first place [@AdamVogel16].  Specifically, participants who scored lower on their working memory scale did so because they had a higher probability of "lapsing" -- just failing to engage in the task on a given trial.  However, when the did engage, their measured working memory capacity was essentially the same as those who had a high working memory capacity score overall, due to a high level of task engagement.  Thus, consistent with the overall importance of motivation, it may be the critical "third factor" that drives the relationship between measured working memory and intelligence scores.

![Fig 7-5: Correlation between brain activity and functional connectivity, and measured general intelligence (Hearne et al, 2016).  The "default-mode network" areas in red are primary emotional / motivational areas in frontal cortex (see Figure 5-14) that are active when people are left to their own thoughts in the scanner.  The fronto-parietal network in yellow are areas associated with cognitive control and working memory.  Interestingly, the emotional / motivational areas make a major contribution to the overall correlation, both in terms of within-network interconnectivity and cross-network connections to the control areas.](figures/fig_hearne_et_al_16_iq_nets.png){ width=80% }

A recent attempt to more directly find the neural correlates of individual differences in intelligence found that motivational and emotional areas of the prefrontal cortex are among the most strongly correlated with measured general intelligence [@HearneMattingleyCocchi16] (Figure 7-5).  Thus, overall, the same answer keeps coming back up across all of these different studies: individual differences in intelligence seem to be more strongly driven by motivational factors than by the raw capacity or other properties of the prefrontal cortex / basal ganglia system.  We can make sense of this result by considering that the relatively measly capacity of prefrontal cortex (around 4-7 items can be maintained at a time) seems completely unrelated to the massive numbers of neurons in this brain area (which has maybe 10 billion neurons overall).  Thus, the overall functional properties of this system are unlikely to be due to normal variation in the numbers of neurons or other basic biological properties of these areas.  Instead, they are much more likely to be due to the degree of learning and experience that has shaped these networks to perform somewhat like a serial computer.


## Strengths, Weaknesses, and Biases of our Neural Computer

* computers can do prodigious amounts of math & statistics, very quickly

* we kinda suck at that stuff

* but we have neurons!!  those neurons can use *parallel* pattern recognition kinds of strategies to solve problems in a "pragmatic" way..

* heuristics and biases..


## Measuring Intelligence and its Implications

* IQ tests

* Multiple intelligences

* real-world implications

* Dweck etc 

* Vogel WM study

* Miyake & Friedman and the genetic basis of IQ: gets stronger over time, just like a learning system.

* important considerations in interpreting genetic results..




