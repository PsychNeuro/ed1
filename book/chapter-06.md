# Chapter 6: Memory

Memory is the direct product of learning, so everything we learned in the previous chapter will help us understand how memory works.  If you can remember it, of course.  Some of the major questions that have been the focus of memory research include:

* What different kinds of memory are there?
	+ Are there specialized brain areas for different kinds of memory?
* How long does memory last (for each different type)?
* What factors determine how well memories are encoded and recalled?

Thus, the study of memory has been focused on fairly practical and descriptive questions, befitting the essential role that memory plays in everyday life (and especially for students).  Our memories are also a core aspect of our sense of self, and movies such as *Total Recall* (based on a Philip K. Dick short story, as so many good movies are) have explored this function of memories in provocative and interesting ways.  Furthermore, by now most people have heard about the profound amnesia caused by damage to the *hippocampus*, e.g., from the famous case of *Henry Molaison* (*H.M.*) who had his hippocampus surgically removed and lost the ability to form new memories for the rest of his long, exceptionally well-studied life.  The movie *Memento* artistically and accurately captures the subjective nature of this condition, and is required viewing for anyone interested in memory (I personally have *two* copies, each a gift to my wife -- memory certainly can be fallible).  What makes the hippocampus so important for memory?  What kinds of memory do *not* depend on the hippocampus?  These are some of the important questions we will address in this chapter.

## From Synapses to Memory

If memory is the direct product of learning, and learning is the direct product of synaptic plasticity as we learned in the previous chapter, then in principle *memory should be found in every synapse in the brain*.  In fact, this is *true*, but it is also true that some synapses are more important than others.  A deep understanding of memory requires reconciling these two perspectives on memory, and integrating some additional properties of neurons beyond their synapses.

First, it is useful to contrast the nature of memory in the brain with memory in a computer -- it is tempting to try to use the computer as a simple analogy (as was especially popular in the early days of cognitive psychology, which relied extensively on computer analogies), but actually it doesn't work anything like that.  In a computer, there are two major types of memory: RAM (random access memory), and a "hard disk" (which these days is typically more like RAM than the physical hard drives that were used for many years, but it still plays the same functional role).  RAM is where *active* memories reside -- the stuff the computer is currently working on.  Elements from RAM are read into the central processing unit (CPU), processed, and then written back into RAM, often many times (e.g., when you are editing a document in your word processor, those words live in RAM, and are accessed many times to redraw the screen as you edit).  When you are done working on it, you save the memories from RAM to the hard drive, where they can reside essentially permanently.  If the power goes off before you save, the RAM is lost -- it is active (fast) and *temporary*, whereas the hard disk is slower but permanent.

In contrast, there is no fundamental separation between memory and processing in the brain.  As we emphasized in the Neuroscience chapter, processing in the brain is distributed across all of the billions of neurons in the brain, with each neuron playing a small role within larger networks.  Each neuron is detecting some particular patterns as a function of its synaptic connections, helping to compress and simplify the vast stream of information flowing through the brain.  Thus, the direct effects of any given synaptic changes depend critically on where in the brain those synapses are, and what kinds of information processing those neurons are doing.  For example, synaptic changes in occipital cortex should affect visual processing much more than decision making, while synaptic changes in the frontal cortex and basal ganglia should affect decision making much more than vision.  Below, we'll see how synaptic changes in the hippocampus come to be so important for so much of what we generally think of as "memory".

The functional equivalent of RAM for a computer is not so obvious in the brain: without a CPU, there is no need for quickly reading and writing information from a RAM-like memory system.  Instead, everything the neuron needs to carry out its detection and compression function is right there in its synapses, and learning directly modifies these synaptic connections.  Furthermore, as we saw in the last chapter, this learning is long-term (i.e., long-term potentiation, LTP, and long-term depression, LTD), so it seems that memory in the brain is typically long-lasting, unlike RAM.  And yet, we have all had the experience of suddenly forgetting what we were just talking about, or entering a room with a clear purpose, which has just vanished into thin air.  These seem like distinctly RAM-like properties.  What kind of stuff in the brain are these experiences made out of? 

The answer is: *neural firing* -- the ongoing spiking activity of the vast numbers of neurons in your brain that are currently above-threshold and sending their signals to other neurons.  Indeed, this __neural activity__ is an essential additional contributor to memory, and even though it is fundamentally different from RAM in terms of the underlying function of the brain, it nevertheless has some properties in common with RAM, in terms of being relatively *transient* and, well, *active*.  Furthermore, we'll see that the frontal cortex / basal ganglia system is uniquely capable of sustaining patterns of neural activity over longer durations, and thus corresponds to a kind of RAM-like system called **working memory**.  Thus, as often happens in biology, the same functional properties (active, transient memory vs. slower, permanent memory in this case) can emerge from very different underlying mechanisms.  Furthermore, we'll see in the next chapter that even though the brain is nothing like a computer at the level of individual neurons, it does behave somewhat like a computer at the larger-scale systems level, where the RAM-like working memory system plays a critical role, but we'll postpone consideration of this level until then.

In summary, we'll start our investigation of memory with the following principles derived from neuroscience:

* Memory can be broadly defined as *any* form of persistence of information over time in the brain -- any trace of some prior event can be considered a type of memory.

* Neurons have two primary sources of such persistent information:
	+ **Activity** in the form of ongoing spiking, electrical potentials underlying that spiking, and the chemical states of other parts of the neuron, which are *transient* -- once a neuron stops firing and its other electrical and chemical states dissipate, a memory trace is no longer actively present in that neuron.
	+ **Synaptic changes** from learning, which are relatively *long-lasting*, and change what kinds of input signals will activate the neuron in the future (i.e., what it *detects*).
	+ These two aspects of neural memory directly influence each other, because learning is driven by neural activity, and changes in synapses result in different patterns of neural activity, but despite this interdependence, we can see how each plays more of an essential role in different types of memory.

* The specific *content* of the memory supported by any given neuron and its synapses is a direct function of its role within the larger neural networks of the brain -- memory happens everywhere in the brain at all times, directly within content-specific processing areas (e.g., visual memories in visual cortex, etc).

Finally, there is one more critical constraint from neuroscience, having to do with the widely-used concept of *transferring* information from one part of the brain to another.  As noted above, this is how everything works in a computer (information is constantly being transferred among the different components of RAM, CPU, and hard disk), but information in the brain is not encoded *symbolically* as it is in a computer, and therefore cannot be so easily moved around.  Instead, as we've emphasized repeatedly, each neuron has learned to detect patterns of activity in its inputs, and thus information can only be transferred by neurons in another brain area detecting their own version of the information encoded in a given brain area.  In other words, information transfer in the brain is much more like the game of *telephone*, where a given message is passed from one person to another, often resulting in amusing misunderstandings.  The same thing happens in the brain: information transfer is *always* accompanied by fundamental transformations of the content, with each area adding its own *spin* or interpretation, with important consequences for understanding the relative veracity of memory.

## The Modal Model of Memory

![Fig 6-1: The modal model of memory and its neural basis, in terms of neural activity and synaptic changes.  At each step, processing is required to transition to the next: only attended sensory items enter STM (and the rest is lost), and actively encoded STM information enters LTM.  Active rehearsal sustains information in STM.  Information in LTM can be retrieved back into STM, and is lost primarily via interference. ](figures/fig_memory_modal_model.png){ width=80% }

Figure 6-1 summarizes the **modal model** of memory, which is so-named because it summarizes the common elements of many different models of human memory that had been developed in the early part of the cognitive revolution [@AtkinsonShiffrin68].  It does a good job of capturing many different phenomenological aspects of memory, and we can use it to see how the neural principles play out in practice.  It involves three separable components, *sensory memory*, *short-term memory (STM)*, and *long-term memory (LTM)*, with information flowing from one to the next dependent on relevant active processes including *attention* (sensory memory to STM) and *encoding* (STM to LTM).

First, sensory input activates **sensory memory**, which is characterized as a transient, high-capacity memory system that represents the sensory input at various levels of abstraction.  Sensory memory corresponds largely to the *activity* of neurons that have been stimulated by the sensory input, at various levels along the kinds of hierarchically-organized processing pathways discussed in the Neuroscience chapter.

There are different names for this activity within each modality, including **iconic** memory in the visual pathways, and **echoic** memory in the auditory pathway.  Iconic memory generally persists for less than a second, whereas echoic memory lasts longer, up to about 4 seconds.  These differences reflect the extent to which the neural activity in associated visual or auditory brain areas can persist.  Because auditory information is inherently transient and evolving over time, the brain has extensive subcortical mechanisms that integrate and presereve these auditory signals over time, resulting in its longer persistence.

![Fig 6-2: Sperling's sensory memory task. In the full report condition, participants attempted to retrieve all items, and typically only recalled about 4.5 on average.  In the partial report condition, an auditory cue presented after a variable delay indicated which of the three rows to recall.  For delays less than a second, they could accurately recall the row, indicating the presence of a high-capacity sensory memory trace (iconic memory) that decays within a second if not activated into STM via attention.](figures/fig_sperling_task.png){ width=40% }

Classic experiments by *George Sperling* and others established these duration values, by flashing a display with 3 rows of 3 letters each (Figure 6-2), and probing people to report a particular row from the display after variable delays [@Sperling60].  In this *partial report* condition, people were generally able to report the information within about a second, but not longer.  Critically, the relatively large amount of information in the full display was above people's capacity to encode in its entirety (as established through other *full report* conditions where they had to try to recall all of the letters), so the partial report cue allowed them to focus attention on one row, resulting in the activation of corresponding representations in STM.  However, once the sensory memory trace fades, it is gone, and cannot be "transferred" to STM.

Experiments such as these also established the next step of the modal model, which is more strongly capacity-limited, but longer-lasting, and is referred to as **short-term memory** (**STM**).  Only information within the focus of __attention__ makes the jump from iconic or echoic sensory memory into STM, and given the capacity constraints, attention can only grab about 3-4 "items" into STM from sensory memory (corresponding to a single row from the Sperling task).  From a neural perspective, STM corresponds to neural activity in higher levels of the neocortex (in temporal and parietal lobes) that have more highly compressed encodings of the sensory input.  Thus, as noted above, the "transfer" of information from sensory memory to STM results in a significant compression and abstraction of the original signal.  The ability to uniquely activate these compressed, abstract detector neurons in higher brain areas requires attention to filter the lower-level sensory input, thus explaining both the need for attention and the lower capacity of STM relative to sensory memory.

Furthermore, the smaller capacity of STM enables it to persit for longer periods of time, because more neurons across multiple of these higher-level areas can participate in representing this information, resulting in a more redundant and robust collection of such neurons.  STM corresponds to the fully recurrent activated state that is likely to be the subject of conscious awareness, as discussed in the Consciousness chapter, and indeed one of the definining characteristics of STM is that you are consciously aware of it.  The underlying neurons are mutually activating each other via bidirectional excitatory connections, causing a bit of an "echo chamber" as these spiking signals pass back and forth among these neurons, resulting in a longer-lasting activation trace compared to sensory memory.  Rough estimates of the duration of STM extend up to about 30 seconds, but this is strongly dependent on the process of **maintenance rehearsal**, which involves the deliberate attempt to keep those neurons firing robustly by continuously focusing attention on them.

Interestingly, the modal model only includes memory mechanisms based on neural activity up to this point.  The synapses in the sensory pathways have been very well-trained by the time anyone is participating in Sperling-style experiments, and thus synaptic changes there typically don't make much of a noticeable difference.  However, there is an extensive literature on *perceptual learning* which can reveal the effects of these ongoing synaptic changes.  Thus, as noted above, memory really is happening at every synapse in the brain, whenever activity is sufficient to drive synaptic changes.  However, you sometimes have to try pretty hard to see the effects of these changes, and the modal model only covers the most obvious forms of memory.

Finally, the last component of the modal model introduces a form of memory that depends on synaptic changes, in the form of **long-term memory** (**LTM**).  In the terms of the modal model, memories are "transferred" into LTM from STM through the process of __encoding__.  They can also be recovered back from LTM into STM via __retrieval__ processes.  This model was developed during the 1960's, when the computer metaphor was at its height, and this encoding process was typically envisioned as transferring "files" between the RAM-like STM and the hard-disk of LTM.  But what does this correspond to in the brain, given that we don't think the concepts of RAM and hard-disk really apply in the brain?

## The Hippocampus

![Fig 6-3: Connectivity and structure of the hippocampus.  Sensory memory and STM are supported by activity in the posterior cortex areas, which then feed into two cortical areas in the *medial* (middle) region of the temporal lobe, the *parahippocampal* and *perirhinal* cortex.  These then feed into the *entorhinal* cortex, which thus has a maximally *compressed* encoding of everything active in the rest of the brain.  The areas of the hippocampal formation then effectively take a snapshot of this cortical activity.](figures/fig_hippo_mem_formation.png){ width=80% }

This is where the *hippocampus* makes its grand entrance on the memory scene: in most cases, the initial encoding of information from the active state of the cortex (i.e., STM) into a form that can be later retrieved depends on the hippocampus.  As noted in the Neuroscience chapter, the hippocampus sits "on top" of the neocortical hierarchy of areas, and can quickly take a "snapshot" of the current pattern of activity across the upper layers of the cortex (Figure 6-3).  Thus, the unique anatomical position of the hippocampus, plus some important special properties of the hippocampus itself, enable it to play such a critical role in the encoding and retrieval of memories.

In brief, you can think of the hippocampal neurons as *detecting* the elements of a memory (i.e., the *who, what, where* elements of an event or *episode*).  Synaptic changes in these neurons then enable even a subset of those elements (e.g., "what did you have for dinner last night?") to re-activate these same neurons, and when these neurons fire, they act to *recreate* the memory out in the neocortex (i.e., the *retrieval* arrow between LTM and STM in the modal model, Figure 6-1).  Thus, whereas neurons in the visual pathways are detecting objects and object features, neurons in the hippocampus are detecting *memories*, and that is why they play such a central role in our mnemonic life.

![Fig 6-4: Pattern separation in the hippocampus: overlapping patterns of neural activity in the cortex result in separate, non-overlapping patterns in the hippocampus, because it has *sparse* activity (i.e., very few neurons active).](figures/fig_patsep_clr.png){ width=40% }

Figure 6-4 shows one of the magic tricks used by the hippocampus to be able to rapidly encode new memories without overwriting other existing memories, known as **pattern separation**.  This is the key idea developed by *David Marr* as mentioned in the Neuroscience chapter, which applies to both the hippocampus and the cerebellum [@Marr69; @Marr71].  The idea is that if you simply reduce the number of neurons firing in the hippocampus compared to the cortex, then the patterns of activity in the hippocampus will overlap much less than those in the cortex, and therefore, there will be less overlap or interference in the synaptic changes involved in memory encoding.  Mathematically, this derives from the fact that squaring a small number, such as .01, results in a *much* smaller number (.0001) -- the small number is the probability of a neuron getting active, and the square is the resulting probability that it would be active in two different memories.  More realistic, detailed simulations of the hippocampal circuit confirm this basic principle [@OReillyMcClelland94].

There are many important implications of this pattern separation property.  First, as we noted in the Neuroscience chapter, this results in a kind of *brute force* memorization strategy in the hippocampus.  It doesn't try to make any direct connections between related memories -- instead it just effectively sticks each memory in its own separate "box".  This is great for quickly finding a place to stick a new memory, but it means that the hippocampal version of those memories is a completely disorganized, haphazard pile of these separate boxes.  Thus, a major further process in memory involves a much slower process of trying to organize and systematize all those memories, known as **memory consolidation**.  Specifically, memories that are initially encoded in the hippocampus are gradually incorporated into synaptic changes among neurons in the neocortex, resulting in the formation of more systematic, well-organized __semantic knowledge__ [@McClellandMcNaughtonOReilly95].  Some of this consolidation may take place during sleep, as memories are replayed during dreams [@WilsonMcNaughton94; @Buzsaki89; @RoumisFrank15], and much of it certainly depends on the usual retelling and ruminative replaying of memories throughout the course of daily life.

Hippocampal pattern separation and memory consolidation have major implications for educational learning and expertise.  Everything you learn in class is initially encoded through hippocampal brute-force memorization, and only over a relatively long period of repeated learning and practice does a systematic an *productive* form of semantic knowledge emerge.  This is consistent with how much experience it takes to become an expert in a given domain: roughly 10,000 hours or 10 years [@EricssonLehmann96].  Thus, if you really want to master something, be prepared to spend a long time slowly shaping your neocortical synapses to develop the necessary systematic knowledge base.

Another important implication of pattern separation is the canary-in-a-coal-mine nature of the hippocampus.  Driving down the activity level of the hippocampus requires an extensive amount of GABA inhibition, and thus the hippocampus is extra sensitive to the effects of alcohol and benzodiazapines (e.g., valium, midazolam), which are GABA agnonists as discussed in the Neuroscience chapter.  Furthermore, the rapid rate of learning in the hippocampus requires high levels of NMDA receptors, which makes this system susceptible to epileptic seizures due to the development of over-strong excitatory synaptic connections (recall that H.M. had his hippocampus removed due to epilepsy, which often has a hippocampal source).  Both of these factors may contribute to an heightened sensitivity to oxygen depravation.

Pattern separation also has important implications for the retrieval of memories from the hippocampus.  To the extent that it is always trying to keep different patterns separate, it is then hard to take a partial retrieval cue (e.g., the "what did you have for dinner?" question) and have that re-activate the original pattern of neural activity that was present when the memory was originally encoded.  This retrieval process is called **pattern completion**, as it involves filling-in or completing the partial cue pattern.  Instead of doing pattern completion, the hippocampus might just end up encoding a retrieval attempt as a brand new experience, and activate entirely new neurons as a result of pattern separation.  Thus, pattern separation and pattern completion are essentially opposing forces within the hippocampus.  Pattern completion is supported by special connections within one of the main areas of the hippocampus (the *CA3*), which effectively "glue" together the different elements of a memory.  Detailed analyses of the battle between pattern separation and pattern completion suggest that the specific anatomy of the hippocampus is particularly well-suited for balancing between these competing demands [@OReillyMcClelland94].

## Taxonomy of Long-Term Memory

![Fig 6-5: A standard Long-Term Memory taxonomy, and associated neural substrates.  The broadest distinction is between consciously accessible memories supported by the cortex and hippocampus, versus non-conscious memories largely in subcortical areas.  Priming is an interesting case of a cortical memory effect that is not directly accessible to consciousness.](figures/fig_memory_taxonomy.png){ width=70% }

Although the hippocampus plays a dominant role in the process of encoding new memories into LTM from STM (in the terms of the modal model), synaptic changes occur everywhere there is activity in the brain.  Thus, a major focus of memory research has been attempting to document and organize all these different "types" of long-term memory within overall memory **taxonomies** (akin to the taxonomies used to organize different species of animals, for example).  Because memory and processing are both occuring within each and every neuron, these taxonomies are really just descriptions of the different kinds of processing taking place in the brain, which we reviewed in detail in the Neuroscience chapter.  Nevertheless, we will briefly review the most popular taxonomy here, and the research that went into its construction.

Figure 6-5 shows perhaps the most widely adopted LTM taxonomy, proposed initially by *Endel Tulving* [@Tulving72] and refined by *Larry Squire* [@Squire92].  It features a top-level division between **explicit** or **declarative** memory, as contrasted with **implicit** or non-declarative memory.  Explicit memories are those we can have direct conscious access to, while implicit memories are not consciously accessible.  Given what we know about consciousness from Chapter 3, explicit memories are therefore those in the neocortex.  Interestingly, we likely are not directly conscious of hippocampal memories, given the requirement of recurrent / bidirectional connectivity for consciousness, which is only partially present in the hippocampus.  Instead, we become conscious of hippocampally-supported memories when they are recalled back into cortex.

The two major subtypes of explicit memory are hippocampal **episodic** memories (i.e., the memories of all the daily events and episodes in our lives, and those we read about or watch in movies or on TV), and **semantic** memory, which is a summary term for all of the facts and knowledge we have, which has been integrated into our cortical synapses over many years of memory consolidation.  During this consolidation process, the episodic character of the knowledge gets winnowed away, leaving only the bare knowledge devoid of the __source__ or **context** information about where we learned these facts.  Newly learned facts (e.g., much of what you are learning in this course) still retain their episodic trace -- you can probably recall when you heard about something interesting for the first time in lecture, or read about it in a book.  Sometimes, people feel like they have a particularly clear sense of where on the page they read something, but in my experience this has proven illusory more often than not.

Within the much more diverse umbrella of implicit memories, there are *procedural*, *conditioning*, and *priming* memory traces.  The separability of **procedural** memory from hippocampal episodic memory was vividly demonstrated in H.M., who was able to learn a challenging new procedural task such as learning to trace a picture when looking in a mirror (try it -- it is hard!) at the same rate as neurologically intact control participants.  This is because procedural learning depends on the cerebellum and basal ganglia, not on the hippocampus. Likewise, as we reviewed in the Learning chapter, __conditioning__ depends on the amygdala, basal ganglia, and dopamine systems, and is thus separable from hippocampal and cortical memories (and was also intact in H.M.).

The value of this memory taxonomy is debatable.  Really, it is just assigning new labels for the functions of brain areas, which can be much more richly and accurately described (e.g., as in the Neuroscience chapter) than in such a broad taxonomy.  Furthermore, it is missing many important parts of the brain.  Perhaps most importantly, the central division according to the criterion of conscious access is problematic at many levels.  Consciousness is inherently subjective, and putting a subjective construct at the center of a major theoretical framework jeopardizes the entire enterprise.  Furthermore, it immediately eliminates application to animal memory [@Morris01], as the notion of consciousness in animals is certainly fraught with controversy (although this exclusion was intentional on the part of Tulving).  It also unnecessarily complicates any kind of straightforward understanding of memory in terms of underlying neural mechanisms.

Given our detailed understanding of how the hippocampus works, it is highly likely that even rats (which have a large hippocampus relative to the rest of their brain), encode something like episodic memories of all the different experiences in their lives.  Rats likely don't sit around idly reminiscing as people do, but that doesn't mean they don't re-activate their episodic memories in response to relevant stimulus cues -- indeed, this has been demonstrated in many experiments recording from hippocampal neurons in rats.  Thus, it is more productive to find the many parallels in brain systems across species, so that we can integrate a much broader scope of data into our theories of memory.

The case of **priming** is particularly illustrative of the limitations of a consciousness-based framework.  Priming is the measurable facilitation in processing information that was previously processed (e.g., "priming the pump").  It results from the small synaptic changes throughout the neocortex, driven by neural activity.  Thus, although we are not directly conscious of priming itself (e.g., we don't know that our responses are faster by about 10 msec), we *are* typically conscious of much of the activity that drives priming.  And these are the very same synaptic changes that add up over time to produce new semantic memory learning.  So does it really make sense to put this in the implicit memory category?  Another example is the considerable contributions of the frontal and parietal cortex to procedural tasks: we can certainly be aware of activity in these brain areas, and yet they are put in the implicit category.

### Amnesia

Patients with hippocampal damage such as H.M. have also shown us that two different types of **amnesia** (loss of memory function) can be *dissociated* (i.e., separated, do not always co-occur): **retrograde** vs. __anterograde__ amnesia.  Retrograde refers to memories of the past (like "retro" styles etc), while anterograde refers to the ability to form new memories.  H.M. was profoundly impaired in his ability to form new memories, and thus suffered from severe antergrade amnesia.  However, many of his memories from his more distant past were largely intact, meaning that he had comparatively mild retrograde amnesia.  Furthermore, his basic semantic knowlege of facts etc was largely intact.

We can understand this dissociation in terms of the basic explanation of hippocampal function given above.  The hippocampus is critical for rapidly learning new episodic memories, because of its unique position at the top of the cortical hierarchy, and its special properties including pattern separation and a relatively fast learning rate.  Thus, damage to the hippocampus almost always produces significant impairments in encoding new episodic memories.  However, because of the gradual incorporation of episodic memories into the neocortex through the consolidation process, older memories from the past can still be recalled even without the help of the hippocampus.  Interestingly, consolidation predicts that more recent memories leading up to the point of hippocampal damage should be most impaired, as they have had less time to be consolidated into the neocortex.  This *gradient* of retrograde amnesia is often observed at least to some extent in human amnesics.  Interestingly, extensive investigations of retrograde gradients and memory consolidation in rats have produced inconsistent results, likely reflecting the variability in the extent to which rats actually recall prior episodes, across different experimental paradigms.

Another fascinating form of amnesia is **childhood amnesia**, which is the widely-documented phenomenon that people cannot generally remember anything before about 3 years of age.  Go ahead, give it a try -- can you?  Many studies have attempted to understand the reasons for this amnesia [@Hayne04].  Overall, it is likely a result of fact that the neocortex is not sufficiently well organized before that age, to support the ability of earlier hippocampal snapshots to be translated back out into the cortex.  In effect, the "language" that the earlier snapshots were recorded in is no longer something that the more mature brain can understand (and indeed language learning itself likely plays a significant role). 

Although the neocortex continues to develop and learning in significant ways beyond the age of 3, there is presumably just enough stability for those earliest memories to persist.  And those early memories that you can still recall have likely been recalled, reinforced, and elaborated many times in the ensuing years.  Nevertheless, in my own case, I feel like I do have vivid, first-person memories of my 3-year-old-self living for 6 months on the island of Grenada in the caribbean, including a scary encounter with a large crab behind the house.  Thus, as is the case with memory in general, emotional arousal and the relative novelty of experiences play a large role in your ability to later recall them.

## Memory Capacity and the Importance of *Chunks*

One of the dimensions along which memory systems vary is in terms of their capacity, with sensory memory being high capacity, STM having a strong limited capacity of around 3-4 items, and LTM being essentially unlimited in its overall storage capacity.  But any consideration of capacity raises the central question of *what counts as an* item *for the purposes of measuring capacity?*  In the Sperling experiments (Figure 6-2), items were individual letters, but what if we instead put words where the letters were in the 3x3 grid display?  Memory capacity will be about the same, now measured in words instead of letters, but that represents a considerable increase in overall *letter* memory capacity!

The answer to this puzzle is to introduce the concept of a **chunk**, which is somewhat circularly defined as an element that acts like a single item with respect to memory capacity measurements.  If the stimuli are *random* letters, then each letter is a chunk, but if the letters can be formed into words, then the word becomes the chunk.  Likewise, if words can be combined into sensible sentences, then those sentences become the chunks.  In short, a non-circular definition of a chunk is *anything that we have an existing stable neocortical semantic representation of.*  This is still not very precise, but it will do for now.

Based on an influential and provocative article by *George Miller* [@Miller56], many textbooks incorrectly cite the capacity of STM as *7 plus or minus 2*.  However, Sperling's original data, and data from many other tasks and domains, strongly suggests that the magic number is actually *4* [@Cowan01; @LuckVogel97].  Although overly simplistic, one way of thinking about this is that each cerebral hemisphere can hold 2 items when pushed to the limit, with 1 being much more comfortable (i.e., a total capacity of 2 across the 2 hemispheres) [@BuschmanSiegelRoyEtAl11].  The higher capacity of 7 applies only to verbal memory that can be sustained by a rehearsal mechanism known as the *phonological loop*, where you repeatedly verbalise (in your mind, but also using your actual vocal muscles at a subthreshold level) the to-be-remembered material.  Our extensive experience with verbal material presumably produces this larger capacity beyond what is generally available with the "default" neural mechanisms.


## Encoding and Retrieval Strategies (i.e., How to Study!)

Because memory capacity is determined by the availability of appropriate chunks, one major category of memory-enhancement tricks involves creating new chunks.  This is the main trick employed by contestants in the memory olympics competitions, and was well-documented in the case of an individual, S.F., who developed chunking strategies that allowed him to remember over 100 random digits [@EriccsonChaseFaloon80].  In this case, he turned 3-digit numbers into times to run a mile or other standard distances, as S.F. was an advid runner.  Another common example of chunking is the creation of acronyms.  For example, the "big five" personality dimensions that we'll encounter later can be organized into the acronym *OCEAN*, which then makes it much easier to remember them all.

Another effective encoding / chunking strategy (i.e., **mnemonics**) is to associate different words with different familiar spatial locations, known from the days of ancient Greece as the **method of loci**.  An even more **elaborative encoding** strategy is to create stories involving these locations and familiar people (e.g., "my mom went from the living room to the kitchen, to get a popcorn snack"), where each of the words then stands for something that you're trying to remember (e.g., mom = *memory*, living room = *hippocampus*, kitchen = *neocortex*, and popcorn = *semantic memory*).  Because the hippocampus really loves to encode episodic memories, these episodic chunks are particularly effective and memorable.

Several other related principles of effective memory encoding have been developed.  For example, the influential **levels-of-processing theory** [@CraikLockhart72] postulates that more *deeply* encoded information will be better remembered.  The notion of levels or depth here corresponds to the levels of processing in the neocortex, going from raw sensory information up to higher-level semantic information, and many studies have found that encouraging people to think about the meaning of a word, as compared to noticing the case or font of the letters, results in better memory.

One of the most fascinating encoding principle is the **encoding specificity principle** [@Tulving83], which reflects the fact that episodic memories tend to bind together all of the different elements present when a memory is encoded, and thus recall of those memories will be best when those original elements are present at the time of recall.  This is a direct result of the pattern completion vs. pattern separation battle operating in the hippocampus -- if too many elements are different from the original event, the hippocampus tends to perform pattern separation instead of the pattern completion required for recall.

All those random elements present at the time of encoding are typically summarized with the term **context**, and thus lead to the **context-dependent memory** phenomenon.  A classic example of this phenomenon is that people are better able to recall information when tested in the same physical context as it was originally learned -- for example, if you study in a library, then taking a test in that same library will generally result in better performance.  This is a tip you can use!  Don't forget it!

The most famous demonstration of this encoding specificity / context-dependent memory principle was conducted in a study where items were learned either on a beach or underwater using scuba equipment, and then tested either in the same or different context [@GoddenBaddeley75].  Participants in the same-context conditions (either on land or under water) performed significantly better than in the cross-context conditions.  Another notorious demonstration involved study and test either drunk or sober, which again found that, surprisingly, testing while drunk was better than sober *if* initial learning was drunk [@GoodwinPowellBremerEtAl69].  This has been labeled the **state-dependent memory** effect, and presumably reflects the same encoding specificity principle.  It is important to emphasize that memory was much worse when learned drunk, even when tested drunk, so that is *not* a good strategy overall.  Other demonstrations of state-dependent memory involve mood states.

Another critical way to improve encoding is to use **spaced** instead of **massed** practice -- i.e., to space out your studying over multiple separate study sessions, instead of *cramming* at the last minute.  This is beneficial for the same reason that gives rise to context-dependent effects: spacing out study causes the information to be learned across multiple different contexts, and thus helps to make the knowlege more indpendent of that context.  A critical point here is that *context* includes a significant contribution from *time* -- your internal mental state is constantly evolving over time, and will be significantly different a week or two from now.  Thus, even if you study in the same physical context, your internal mental context will be different, shaped by all those synaptic changes taking place between the two study sessions.  Indeed a critical aspect of the memory consolidation effect involves exactly this process of thinking about the same issues from the very different perspectives that emerge as your brain changes over the period of years.

In short, you should study by engaging in deep, elaborative encoding of the material, connecting it in multiple different ways with your existing knowledge.  Furthermore, you should do this in a spaced fashion, across multiple different days, ideally in different physical and mental contexts.

<--- desirable difficulties?  testing effect? --->

## Memory Retention and Interference

* proactive / reactive interference

* recall / recognition?


## The Fallibility of Memory

* reconstruction vs veridical encoding

* compressed high level inputs to hippocampus

## Working Memory and the Prefrontal Cortex

* distinction between STM and WM, important role of PFC / Bg.

## Summary of Key Terms

This is a checklist of key terms / concepts that you should know about from this chapter.


