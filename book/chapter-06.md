---
bibfile: ccnlab.bib
---

# Chapter 6: Memory

Memory is the direct product of learning, so everything we learned in the previous chapter will help us understand how memory works.  If you can remember it, of course.  Some of the major questions that have been the focus of memory research include:

* What different kinds of memory are there?
    + Are there specialized brain areas for different kinds of memory?
* How long does memory last (for each different type)?
* What factors determine how well memories are encoded and recalled?

Thus, the study of memory has been focused on fairly practical and descriptive questions, befitting the essential role that memory plays in everyday life (and especially for students).  Our memories are also a core aspect of our sense of self, and movies such as *Total Recall* (based on a Philip K. Dick short story, as so many good movies are) have explored this function of memories in provocative and interesting ways.  Furthermore, by now most people have heard about the profound amnesia caused by damage to the *hippocampus*, e.g., from the famous case of Henry Molaison (H.M.) who had his hippocampus surgically removed and lost the ability to form new memories for the rest of his long, exceptionally well-studied life.  The movie *Memento* artistically and accurately captures the subjective nature of this condition, and is required viewing for anyone interested in memory (I personally have *two* copies, each a gift to my wife -- memory certainly can be fallible).  What makes the hippocampus so important for memory?  What kinds of memory do *not* depend on the hippocampus?  These are some of the important questions we will address in this chapter.

## From Synapses to Memory

If memory is the direct product of learning, and learning is the direct product of synaptic plasticity as we learned in the previous chapter, then in principle *memory should be found in every synapse in the brain*.  In fact, this is *true*, but it is also true that some synapses are more important than others.  A deep understanding of memory requires reconciling these two perspectives on memory, and integrating some additional properties of neurons beyond their synapses.

First, it is useful to compare and contrast the nature of memory in the brain with memory in a computer.  More generally, it is tempting to try to use the computer as a model for the brain, as was especially popular in the early days of cognitive psychology, but it turns out that the brain doesn't work anything like a computer at the hardware level.  Nevertheless, some features of computers do emerge out of the brain, despite the fundamental differences in their underlying hardware -- this may make the computer analogy more confusing than not, but given the prevalence of these computer analogies, it is important to get this all straight!

In a computer, there are two major types of memory: RAM (random access memory), and a "hard disk", which these days is typically just a different kind of solid-state chip, rather than the spinning platters used in actual hard drives, but it still plays the same functional role.  RAM is where *active* memories reside -- the stuff the computer is currently working on.  Elements from RAM can be very quickly read into the central processing unit (CPU), processed, and then written back into RAM, often many times (e.g., when you are editing a document in your word processor, those words live in RAM, and are accessed many times to redraw the screen as you scroll and edit).  When you are done working, you save the memories from RAM to the hard drive, where they can reside essentially permanently.  If the power goes off before you save, the RAM is lost -- it is active (fast) and *temporary*, whereas the hard disk is slower but permanent.  Computers typically have much less RAM than hard disk storage -- the amount of information needed for active processing is typically much less than the sum total of all information that has been processed and stored.

You have likely had the experience of suddenly forgetting what you were just talking about, or entering a room with a clear purpose, which then just vanishes into thin air.  These seem like distinctly RAM-like properties: temporary, and associated with what you were currently thinking about / working on, and more capacity-limited -- you can't juggle too many things at the same time.  On the other hand, we clearly have access to a vast storehouse of long-term memories, which often require some specific cognitive effort to retrieve (and as you get older, this effort seems to grow), and clearly cannot all be active at the same time -- this seems like a hard disk type of memory.

Thus, this distinction between fast, temporary, limited memory (RAM) vs. slower, permanent, high-capacity memory (hard disk) somehow applies in the brain too, except the brain doesn't have a CPU, nor does it have discrete hardware modules like RAM and a hard disk.  As we saw in the previous chapters, processing in the brain is *distributed* across all of the billions of neurons in the brain, with each neuron playing a small role within larger networks.  Each neuron is detecting some particular patterns as a function of its synaptic connections, helping to compress and simplify the vast stream of information flowing through the brain.  Learning operates by changing these synapses (i.e., long-term potentiation, LTP, and long-term depression, LTD), such that memory is also fully distributed across the brain, instead of being concentrated in a separate device like a hard drive.

The long-term nature of synaptic learning, and the vast numbers of such synapes, provides a nice fit with the hard-disk like properties, but what could be the neural equivalent of RAM?  Without a CPU, there is no need for quickly reading and writing information from a RAM-like memory system.  Instead, everything the neuron needs to carry out its detection and compression function is right there in its synapses, and learning directly modifies these synaptic connections.  

The answer is **neural activity** -- the ongoing spiking activity of the vast numbers of neurons in your brain that are currently above-threshold and sending their signals to other neurons.  Indeed, this neural activity is an essential additional contributor to memory, because once excited, this activity tends to persist over time -- and persistence is the essence of memory.  However, unlike synaptic changes, and like RAM, neural activity is definitely transient -- once some new pattern of activity sweeps over your neurons, whatever was there before is effectively lost (unless it somehow got recorded via synaptic changes).  Furthermore, it is more limited in capacity -- you have many fewer neurons than synapses, and because all these neurons are bidirectionally communicating with each other, the actual number of distinct, coherent memory states is drastically smaller than the number of neurons (only about 4, as we'll see below).

In summary, we'll start our investigation of memory with the following principles derived from neuroscience:

* Memory can be broadly defined as *any* form of persistence of information over time in the brain -- any trace of some prior event can be considered a type of memory.

* Neurons have two primary sources of such persistent information:
    + **Activity** in the form of ongoing spiking, electrical potentials underlying that spiking, and the chemical states of other parts of the neuron, which are *transient* -- once a neuron stops firing and its other electrical and chemical states dissipate, a memory trace is no longer actively present in that neuron.
    + **Synaptic changes** from learning, which are relatively *long-lasting*, and change what kinds of input signals will activate the neuron in the future (i.e., what it *detects*).
    + These two aspects of neural memory directly influence each other, because learning is driven by neural activity, and changes in synapses result in different patterns of neural activity.  Despite this interdependence, these different types of memory have different functional properties and can be usefully distinguished.

* The specific **content** of the memory supported by any given neuron and its synapses is a direct function of its role within the larger neural networks of the brain -- memory happens everywhere in the brain at all times, directly within content-specific processing areas (e.g., visual memories in visual cortex, etc).  In addition, there are two brain areas that play an outsized role in memory, due to their specific neural properties and location within the brain's networks -- both of these areas are situated at the *top* of the overall cortical hierarchy, so they have broad *access* and broad *influence* over everything going on in the brain:

    + The **hippocampus** is specialized for performing very *fast* encoding of synaptic changes in a way that avoids the massive *interference* effects that such synaptic changes would otherwise cause in other brain areas -- this enables it to rapidly encode **episodic memory** of ongoing daily events, as H.M.'s case demonstrated.
    
    + The **frontal cortex / basal ganglia** system is uniquely capable of sustaining patterns of neural activity over longer durations and in the face of other distractions, supporting **working memory**, which is important for maintaining the current information that you happen to be "working" on.  The relative deactivation of the frontal cortex during REM sleep demonstrates what cognition would be like without this robust form of active memory -- you would become much more distractable and unable to stay focused on a given task.  This is characteristic of people with damage to the frontal cortex, and to some extent in people with ADHD (though their basic frontal function is typically indistinguishable from people without ADHD, as we'll explore in the Disorders chapter).

![**Figure 6.1:** The *telephone* game, which is an apt metaphor for how information is transferred in the brain.  Neurons, like people, take a given signal, interpret it in their own particular way (as a function of their synapses), and send out their own interpretation.  The idea of direct symbolic information transfer as in a digital computer does not apply in the brain.](figures/fig_telephone_game.jpg){ width=50% }

Finally, there is one more critical constraint from neuroscience, having to do with the widely-used concept of *transferring* information from one part of the brain to another.  As noted above, this is how everything works in a computer (information is constantly being transferred among the different components of RAM, CPU, and hard disk), but information in the brain is not encoded *symbolically* as it is in a computer, and therefore cannot be so easily moved around.  Instead, as we've emphasized repeatedly, each neuron has learned to detect patterns of activity in its inputs, and thus information can only be transferred by neurons in another brain area detecting their own version of the information encoded in a given brain area.

In other words, information transfer in the brain is much more like the game of *telephone*, where a given message is passed from one person to another, often resulting in amusing misunderstandings (Figure 6.1).  The same thing happens in the brain: information transfer is *always* accompanied by fundamental transformations of the content, with each area adding its own *spin* or interpretation, with important consequences for understanding the relative veracity of memory.

## The Modal Model of Memory

![**Figure 6.2:** The modal model of memory and its neural basis, in terms of neural activity and synaptic changes.  At each step, processing is required to transition to the next: only attended sensory items enter STM (and the rest is lost), and actively encoded STM information enters LTM.  Active rehearsal sustains information in STM.  Information in LTM can be retrieved back into STM, and is lost primarily via interference. ](figures/fig_memory_modal_model.png){ width=80% }

Figure 6.2 summarizes the **modal model** of memory, which is so-named because it summarizes the common elements of many different models of human memory that had been developed in the early part of the cognitive revolution [@AtkinsonShiffrin68].  It does a good job of capturing many different phenomenological aspects of memory, and we can use it to see how the above neural principles play out in practice.  It involves three separable components, *sensory memory*, *short-term memory (STM)*, and *long-term memory (LTM)*, with information flowing from one to the next dependent on relevant active processes including *attention* (sensory memory to STM) and *encoding* (STM to LTM).

First, sensory input activates **sensory memory**, which is characterized as a transient, high-capacity memory system that represents the sensory input at various levels of abstraction.  Sensory memory corresponds largely to the *activity* of neurons that have been stimulated by the sensory input, at various levels along the kinds of hierarchically-organized sensory processing pathways discussed in previous chapters.

There are different names for this activity within each modality, including **iconic** memory in the visual pathways, and **echoic** memory in the auditory pathway.  Iconic memory generally persists for less than a second, whereas echoic memory lasts longer, up to about 4 seconds.  These differences reflect the extent to which the neural activity in associated visual or auditory brain areas can persist.  Because auditory information is inherently transient and evolving over time, the brain has extensive subcortical mechanisms that integrate and preserve these auditory signals over time, resulting in its longer persistence.

![**Figure 6.3:** Sperling's sensory memory task. In the *full report* condition, participants attempted to retrieve all items, and typically only recalled about 4.5 on average.  In the *partial report* condition, an auditory cue presented after a variable delay indicated which of the three rows to recall.  For delays less than a second, they could accurately recall the letters within the cued row, indicating the presence of a high-capacity sensory memory trace (iconic memory) that decays within a second if not activated into STM via attention.](figures/fig_sperling_task.png){ width=20% }

Classic experiments by George Sperling and others established these duration values, by flashing a display with 3 rows of 3 letters each (Figure 6.3), and probing people to report a particular row from the display after variable delays [@Sperling60].  In this *partial report* condition, people were generally able to report the information within about a second, but not longer.  Critically, the relatively large amount of information in the full display was above people's capacity to encode in its entirety (as established through other *full report* conditions where they had to try to recall all of the letters), so the partial report cue allowed them to focus attention on one row, resulting in the activation of corresponding representations in STM.  However, once the sensory memory trace fades, it is gone, and cannot be "transferred" to STM.

Experiments such as these also established the next step of the modal model, which is more strongly capacity-limited, but longer-lasting, and is referred to as **short-term memory** (**STM**).  Only information within the focus of **attention** makes the jump from iconic or echoic sensory memory into STM, and given the capacity constraints, attention can only grab about 3-4 "items" into STM from sensory memory (corresponding to a single row from the Sperling task).  From a neural perspective, STM corresponds to neural activity in higher levels of the neocortex (in temporal and parietal lobes) that have more highly compressed encodings of the sensory input.  Thus, as noted above, the "transfer" of information from sensory memory to STM results in a significant compression and abstraction of the original signal.  The ability to uniquely activate these compressed, abstract detector neurons in higher brain areas requires attention to filter the lower-level sensory input, thus explaining both the need for attention and the lower capacity of STM relative to sensory memory.

Furthermore, the smaller capacity of STM enables it to persist for longer periods of time, because more neurons across multiple of these higher-level areas can participate in representing this information, resulting in a more redundant and robust collection of such neurons.  In the terminology from the chapter on consciousness, STM corresponds to the *fully recurrent* activated state, which is highly likely to be the subject of conscious awareness.  Indeed, one of the defining characteristics of STM is that you are consciously aware of it.

Thus, the overall picture of STM is that the underlying neurons are mutually activating each other via bidirectional excitatory connections, causing a bit of an "echo chamber" as these spiking signals pass back and forth among these neurons, resulting in a longer-lasting activation trace compared to sensory memory.  Rough estimates of the duration of STM extend up to about 30 seconds, but this is strongly dependent on the process of **maintenance rehearsal**, which involves the deliberate attempt to keep those neurons firing robustly by continuously focusing attention on them.

Interestingly, up to this point, the modal model only includes memory mechanisms based on *neural activity*.  This reflects the fact that the synapses in the sensory pathways have been very well-trained by the time anyone is participating in Sperling-style experiments, so the synaptic changes there typically don't make much of a noticeable difference.  However, there is an extensive literature on *priming* and *perceptual learning* which can reveal the effects of these ongoing synaptic changes.  Thus, as noted above, memory really is happening at every synapse in the brain, whenever activity is sufficient to drive synaptic changes.  However, you sometimes have to try pretty hard to see the effects of these changes, and the modal model only covers the most obvious forms of memory.

Finally, the last component of the modal model introduces a form of memory that does depend on synaptic changes, in the form of **long-term memory** (**LTM**).  In the terms of the modal model, memories are "transferred" into LTM from STM through the process of **encoding**.  They can also be recovered back from LTM into STM via **retrieval** processes.  This model was developed during the 1960's, when the computer metaphor was at its height, and this encoding process was typically envisioned as transferring "files" between the RAM-like STM and the hard-disk of LTM.  But what does this correspond to in the brain, given that we don't think the concepts of RAM, hard-disk, or transfer really apply in the brain?

## The Hippocampus

![**Figure 6.4:** Connectivity and structure of the hippocampus.  Sensory memory and STM are supported by activity in the posterior cortex areas, which then feed into two cortical areas in the *medial* (middle) region of the temporal lobe, the *parahippocampal* and *perirhinal* cortex.  These then feed into the *entorhinal* cortex, which thus has a maximally *compressed* encoding of everything active in the rest of the brain.  The areas of the hippocampal formation then effectively take a snapshot of this cortical activity.](figures/fig_hippo_mem_formation.png){ width=80% }

This is where the *hippocampus* makes its grand entrance on the memory scene: in most cases, the initial encoding of information from the active state of the cortex (i.e., STM) into a form that can be later retrieved (i.e., LTM) depends on the hippocampus.  Because the hippocampus sits at the top of the neocortical hierarchy of areas, it can quickly take a "snapshot" of the current pattern of activity across the upper layers of the cortex (Figure 6.4).  Thus, the unique anatomical position of the hippocampus, plus some important special properties of the hippocampus itself, enable it to play such a critical role in the encoding and retrieval of memories.

In brief, you can think of the hippocampal neurons as *detecting* the elements of a memory (e.g., the *who, what, where* elements of an event or *episode*).  Synaptic changes in these neurons then enable even a subset of those elements (e.g., the query "what did you have for dinner last night?") to re-activate these same neurons in the hippocampus.  When these neurons fire, they act in turn to re-activate the memory out in the neocortex (i.e., the *retrieval* arrow between LTM and STM in the modal model, Figure 6.2).  Thus, whereas neurons in the visual pathways are detecting objects and object features, neurons in the hippocampus are detecting *memories*, and that is why they play such a central role in our mnemonic life.

![**Figure 6.5:** Pattern separation in the hippocampus: overlapping patterns of neural activity in the cortex result in separate, non-overlapping patterns in the hippocampus, because it has *sparse* activity (i.e., very few neurons active).](figures/fig_patsep_clr.png){ width=30% }

Figure 6.5 shows one of the magic tricks used by the hippocampus to be able to rapidly encode new memories without overwriting other existing memories, known as **pattern separation**.  This is the key idea developed by David Marr as mentioned in the Neuroscience chapter, which applies to both the hippocampus and the cerebellum [@Marr69; @Marr71].  The idea is that if you simply reduce the number of neurons firing in the hippocampus compared to the cortex (i.e., make them *sparse*), then the patterns of activity in the hippocampus will overlap much less than those in the cortex, and therefore, there will be less overlap or interference in the synaptic changes involved in memory encoding.  Mathematically, this derives from the fact that squaring a small number, such as .01, results in a *much* smaller number (.0001) -- the small number (.01) is the probability of a neuron getting active, and the square is the resulting probability that it would be active in *two* different memories.  More realistic, detailed simulations of the hippocampal circuit confirm this basic principle [@OReillyMcClelland94].

There are many important implications of this pattern separation property.  First, as we noted in the Neuroscience chapter, this results in a kind of *brute force* memorization strategy in the hippocampus.  It doesn't try to make any direct connections between related memories -- instead it just effectively sticks each memory in its own separate "box" (i.e., a highly distinct neural pattern with no systematic relationship to other memories).  This is great for quickly finding a place to stick a new memory, but it means that the hippocampal version of those memories is a completely disorganized, haphazard pile of these separate boxes.

Thus, there is a much slower process of *organizing* and *systematizing* all those memories, known as **systems consolidation** (which is distinct from synaptic-level consolidation processes involved in the LTP / LTD mechanisms, as we'll clarify below).  Specifically, memories that are initially encoded in the hippocampus are gradually incorporated into synaptic changes among neurons in the neocortex, resulting in the formation of more systematic, well-organized **semantic knowledge** [@McClellandMcNaughtonOReilly95].  Some of this consolidation may take place during slow-wave sleep, as discussed in Chapter 3 [@WilsonMcNaughton94; @Buzsaki89; @RoumisFrank15], and much of it certainly depends on the usual retelling and ruminative replaying of memories throughout the course of daily life.

Hippocampal pattern separation and memory consolidation have major implications for educational learning and expertise.  Everything you learn in class is initially encoded through hippocampal brute-force memorization, and only over a relatively long period of repeated learning and practice does a systematic and *productive* form of semantic knowledge emerge.  This is consistent with how much experience it takes to become an expert in a given domain: roughly 10,000 hours or 10 years [@EricssonLehmann96].  Thus, if you really want to master something, be prepared to spend a long time slowly shaping your neocortical synapses to develop the necessary systematic knowledge base.

Another important implication of pattern separation is the canary-in-a-coal-mine nature of the hippocampus.  Driving down the activity level of the hippocampus requires an extensive amount of GABA inhibition, and thus the hippocampus is extra sensitive to the effects of alcohol and benzodiazepines (e.g., valium, midazolam), which are GABA agonists as discussed in Chapter 3.  Furthermore, the rapid rate of learning in the hippocampus requires high levels of NMDA receptors, which makes this system susceptible to epileptic seizures due to the development of over-strong excitatory synaptic connections (recall that H.M. had his hippocampus removed due to epilepsy, which often has a hippocampal source).  Both of these factors may contribute to a heightened sensitivity to oxygen depravation.

Pattern separation also has important implications for the retrieval of memories from the hippocampus.  To the extent that it is always trying to keep different patterns separate, it is then hard to take a partial retrieval cue (e.g., the "what did you have for dinner?" question) and have that re-activate the original pattern of neural activity that was present when the memory was originally encoded.  This retrieval process is called **pattern completion**, as it involves filling-in or completing the partial cue pattern.  Instead of doing pattern completion, the hippocampus might just end up encoding a retrieval attempt as a brand new experience, and activate entirely new neurons as a result of pattern separation.

Thus, pattern separation and pattern completion are essentially opposing forces within the hippocampus.  Pattern completion is supported by special connections within one of the main areas of the hippocampus (the *CA3*), which effectively "glue" together the different elements of a memory.  Detailed analyses of the battle between pattern separation and pattern completion suggest that the specific anatomy of the hippocampus is particularly well-suited for balancing between these competing demands [@OReillyMcClelland94].

## Taxonomy of Long-Term Memory

![**Figure 6.6:** A standard Long-Term Memory taxonomy, and associated neural substrates.  The broadest distinction is between consciously accessible memories supported by the cortex and hippocampus, versus non-conscious memories largely in subcortical areas.  Priming is an interesting case of a cortical memory effect that is not directly accessible to consciousness.](figures/fig_memory_taxonomy.png){ width=50% }

Although the hippocampus plays a dominant role in the process of encoding new memories into LTM from STM (in the terms of the modal model), synaptic changes occur everywhere there is activity in the brain.  Thus, a major focus of memory research has been attempting to document and organize all these different "types" of long-term memory within overall memory **taxonomies** (akin to the taxonomies used to organize different species of animals, for example).  Because memory and processing are both occurring within each and every neuron, these taxonomies are really just descriptions of the different kinds of processing taking place in the brain, which we reviewed in detail in the Neuroscience chapter.  Nevertheless, we will briefly review the most popular taxonomy, and the research that went into its construction.

Figure 6.6 shows perhaps the most widely adopted LTM taxonomy, proposed initially by Endel Tulving [@Tulving72] and refined by Larry Squire [@Squire92].  It features a top-level division between **explicit** or **declarative** memory, as contrasted with **implicit** or non-declarative memory.  Explicit memories are those we can have direct conscious access to (and declarative means you can declare it verbally), while implicit memories are not consciously accessible.  Given what we know about consciousness from Chapter 3, explicit memories are therefore those in the neocortex.  Interestingly, we likely are not directly conscious of hippocampal memories, given the requirement of recurrent / bidirectional connectivity for consciousness, which is only partially present in the hippocampus.  Instead, we become conscious of hippocampally-supported memories when they are recalled back into cortex.

The two major subtypes of explicit memory are hippocampal **episodic** memories (i.e., the memories of all the daily events and episodes in our lives, and those we read about or watch in movies or on TV), and **semantic** memory, which is a summary term for all of the facts and knowledge we have, which has been integrated into our cortical synapses over many years of memory consolidation.  During this consolidation process, the episodic character of the knowledge gets winnowed away, leaving only the bare knowledge devoid of the **source** or **context** information about where we learned these facts.  Newly learned facts (e.g., much of what you are learning in this course) still retain their episodic trace -- you can probably recall when you heard about something interesting for the first time in lecture, or read about it in a book.  Sometimes, people feel like they have a particularly clear sense of where on the page they read something, but in my experience this has proven illusory more often than not.

Within the much more diverse umbrella of implicit memories, there are *procedural*, *conditioning*, and *priming* memory traces.  The separability of **procedural** memory from hippocampal episodic memory was vividly demonstrated by H.M., who was able to learn a challenging new procedural task such as learning to trace a picture when looking in a mirror (try it -- it is hard!) at the same rate as neurologically intact control participants.  This is because procedural learning depends on circuits through the frontal cortex, cerebellum and basal ganglia, not on the hippocampus. Likewise, as we reviewed in the Learning chapter, **conditioning** depends on the amygdala, basal ganglia, and dopamine systems, and is thus separable from hippocampal and cortical memories (and was also intact in H.M.).

The value of this memory taxonomy is debatable.  Really, it is just assigning new labels for the functions of brain areas, which can be much more richly and accurately described (e.g., as in the Neuroscience chapter) than in such a broad taxonomy.  Furthermore, it is missing many important parts of the brain.  Perhaps most importantly, the central division according to the criterion of conscious access is problematic at many levels.  Consciousness is inherently subjective, and putting a subjective construct at the center of a major theoretical framework jeopardizes the entire enterprise.  Furthermore, it immediately eliminates application to animal memory [@Morris01], as the notion of consciousness in animals is certainly fraught with controversy.  It also unnecessarily complicates any kind of straightforward understanding of memory in terms of underlying neural mechanisms.

For example, given our detailed understanding of how the hippocampus works, it is highly likely that even rats (which have a large hippocampus relative to the rest of their brain) encode something like episodic memories of all the different experiences in their lives.  Rats likely don't sit around idly reminiscing as people do, but that doesn't mean they don't re-activate their episodic memories in response to relevant stimulus cues -- indeed, this has been demonstrated in many experiments recording from hippocampal neurons in rats.  Thus, it is more productive to find the many parallels in brain systems across species, so that we can integrate a much broader scope of data into our theories of memory.

The case of **priming** is particularly illustrative of the limitations of a consciousness-based framework.  Priming is the measurable facilitation in processing information that was previously processed (i.e., "priming the pump").  It results from small synaptic changes throughout the neocortex, driven by neural activity.  Thus, although we are not directly conscious of priming itself (e.g., we don't know that our responses are faster by about 10 msec), we *are* typically conscious of much of the activity that drives priming.  And these are the very same synaptic changes that add up over time to produce new semantic memory learning.  So does it really make sense to put this in the implicit memory category?  Another example is the considerable contributions of the frontal and parietal cortex to procedural tasks: we can certainly be aware of activity in these brain areas, and yet they are put in the implicit category.

Another interesting case similar to priming is the difference between **recognition** and **recall**, which has been studied extensively in the memory literature [@JacobyTothYonelinas93].  Recognition memory is characterized as using the overall feeling of *familiarity* with a given stimulus to decide whether it was on a given memory list, whereas recall involves the explicit, conscious *recollection* of episodic details from the time of study.  Recollection generally depends on the hippocampal pattern completion process to re-activate those episodic details, whereas familiarity can be supported by differences in neocortical activity patterns reflecting synaptic weight changes, which are not strong enough to drive full recollection [@NormanOReilly03].  Thus, familiarity is similar to priming, but interestingly, H.M. and some other severe amnesics were impaired at familiarity-based recognition memory, but their priming was intact.  This is because the familiarity signal is likely driven by the neocortical areas surrounding the hippocampus (e.g., perirhinal and entorhinal cortex) that were damaged along with the hippocampus proper, whereas most priming tests probe lower-level semantic or visual cortical areas.

### Amnesia

Patients with hippocampal damage such as H.M. have also shown us that two different types of **amnesia** (loss of memory function) can be *dissociated* (i.e., separated, do not always co-occur): **retrograde** vs. **anterograde** amnesia.  Retrograde refers to memories of the past (like "retro" styles etc), while anterograde refers to the ability to form new memories.  H.M. was profoundly impaired in his ability to form new memories, and thus suffered from severe anterograde amnesia.  However, many of his memories from his more distant past were largely intact, meaning that he had comparatively mild retrograde amnesia.  Furthermore, his basic semantic knowledge of facts etc was largely intact.

We can understand this dissociation in terms of the basic explanation of hippocampal function given above.  The hippocampus is critical for rapidly learning new episodic memories, because of its unique position at the top of the cortical hierarchy, and its special properties including pattern separation and a relatively fast learning rate.  Thus, damage to the hippocampus almost always produces significant impairments in encoding new episodic memories.  However, because of the gradual incorporation of episodic memories into the neocortex through the consolidation process, older memories from the past can still be recalled even without the help of the hippocampus.

Interestingly, consolidation predicts that more recent memories leading up to the point of hippocampal damage should be most impaired, as they have had less time to be consolidated into the neocortex.  This *gradient* of retrograde amnesia is often observed at least to some extent in human amnesics.  Interestingly, extensive investigations of retrograde gradients and memory consolidation in rats have produced inconsistent results, likely reflecting the variability in the extent to which rats actually recall prior episodes, across different experimental paradigms [@SutherlandOBrienLehmann08; @AnagnostarasMarenFanselow99].

Another fascinating form of amnesia is **childhood amnesia**, which is the widely-documented phenomenon that people cannot generally remember anything before about 3 years of age.  Go ahead, give it a try -- can you?  Many studies have attempted to understand the reasons for this amnesia [@Hayne04].  Overall, it is likely a result of the fact that the neocortex is not sufficiently well organized before that age, to support the ability of earlier hippocampal snapshots to be translated back out into the cortex.  In effect, the "language" that the earlier snapshots were recorded in is no longer something that the more mature brain can understand (and indeed language learning itself likely plays a significant role). 

Although the neocortex continues to develop and learn in significant ways beyond the age of 3, there is presumably just enough stability for those earliest memories to persist.  And those early memories that you can still recall have likely been recalled, reinforced, and elaborated many times in the ensuing years, so they are well-consolidated and may not actually be very accurate anymore.  Nevertheless, in my own case, I feel like I do have vivid, first-person memories of my 3-year-old-self living for 6 months on the island of Grenada in the Caribbean, including a scary encounter with a large crab behind the house.  Thus, as is the case with memory in general, emotional arousal and the relative novelty of experiences play a large role in one's ability to later recall them.

## Memory Capacity and the Importance of *Chunks*

One of the dimensions along which memory systems vary is in terms of their capacity, with sensory memory being high capacity, STM having a strongly limited capacity of around 3-4 items, and LTM being essentially unlimited in its overall storage capacity.  But any consideration of capacity raises the central question of *what counts as an* item *for the purposes of measuring capacity?*  In the Sperling experiments (Figure 6.3), items were individual letters, but what if we instead put words where the letters were in the 3x3 grid display?  Memory capacity will be about the same, now measured in words instead of letters, but that represents a considerable increase in overall *letter* memory capacity!

The answer to this puzzle is to introduce the concept of a **chunk**, which is somewhat circularly defined as an element that acts like a single item with respect to memory capacity measurements.  If the stimuli are *random* letters, then each letter is a chunk, but if the letters can be formed into words, then the word becomes the chunk.  Likewise, if words can be combined into sensible sentences, then those sentences become the chunks.  In short, a non-circular definition of a chunk is *anything that we have an existing stable neocortical semantic representation of.*  This is still not very precise, but it will do for now.

Based on an influential and provocative article by *George Miller* [@Miller56], many textbooks incorrectly cite the capacity of STM as *7 plus or minus 2*.  However, Sperling's original data, and data from many other tasks and domains, strongly suggests that it is actually **the magic number 4** [@Cowan01; @LuckVogel97].  Although overly simplistic, one way of thinking about this is that each of your two cerebral hemispheres can hold 2 items when pushed to the limit, such that 2x2=4, with 1 per hemisphere being much more comfortable [@BuschmanSiegelRoyEtAl11].  The higher capacity of 7 applies only to verbal memory that can be sustained by a rehearsal mechanism known as the *phonological loop*, where you repeatedly verbalise (in your mind, but also using your actual vocal muscles at a subthreshold level) the to-be-remembered material [@BaddeleyGathercolePapagno98].  Our extensive experience with verbal material presumably produces this larger capacity beyond what is generally available with the "default" neural mechanisms.


## Encoding and Retrieval Strategies (i.e., How to Study!)

Because memory capacity is determined by the availability of appropriate chunks, one major category of memory-enhancement tricks involves creating new chunks, and efficiently leveraging the ones you already have.  This is the main trick employed by contestants in the memory olympics competitions, and was well-documented in the case of an individual, S.F., who developed chunking strategies that allowed him to remember over 100 random digits [@EriccsonChaseFaloon80].  In this case, he turned 3-digit numbers into times to run a mile or other standard distances, as S.F. was an avid runner.  Another common example of chunking is the creation of acronyms.  For example, the "big five" personality dimensions that we'll encounter later can be organized into the acronym *OCEAN*, which then makes it much easier to remember them all.

Another effective encoding / chunking strategy (i.e., **mnemonics**) is to associate different words with different familiar spatial locations, known from the days of ancient Greece as the **method of loci**.  An even more **elaborative encoding** strategy is to create stories involving these locations and familiar people (e.g., "my mom went from the living room to the kitchen, to get a popcorn snack"), where each of the words then stands for something that you're trying to remember (e.g., mom = 3, living room = 7, kitchen = 4, and popcorn = 8).  Because the hippocampus really loves to encode episodic memories, these episodic chunks are particularly effective and memorable.

Several other related principles of effective memory encoding have been developed.  For example, the influential **levels-of-processing theory** [@CraikLockhart72] postulates that more *deeply* encoded information will be better remembered.  The notion of levels or depth here corresponds to the levels of processing in the neocortex, going from raw sensory information up to higher-level semantic information.  For example, many studies have found that encouraging people to think about the meaning of a word, as compared to noticing the case or font of the letters, results in better memory.

An interesting example of the benefits of deeper, more elaborative encoding comes from the notion of **desirable difficulties** [@Bjork94] -- memory is often better if you have to work harder to process the information, even in sometimes fairly strange ways.  For example, making information harder to read can improve subsequent memory, and a font was recently created called [Sans Forgetica](https://en.wikipedia.org/wiki/Sans_forgetica) to leverage this finding -- unfortunately it doesn't actually seem to improve memory [@TaylorSansonBurnellEtAl20], suggesting that not all forms of difficulty are created equal!

One of the most robust ways of improving learning, which is directly relevant to success school, is the **testing effect** or the **retrieval practice effect**, where taking a test improves subsequent memory [@RoedigerButler11]. Importantly, this testing effect results in significantly better learning than a *reexposure* condition, where the same material is presented in full.  In other words, you really have to *test* yourself, not just re-read something (in this way it is a kind of desirable difficulty).  This obviously has important implications for how you study: simply re-reading over the text is much less effective than testing yourself to explain a concept given a cue.  This is why we have the key words listed at the end of each chapter -- you should go through each one and try to generate a full explanation of what that term means and why it is important.  Then, you should go check your answer by finding the relevant text.  Also, hopefully your class includes weekly quizzes that give you a more structured opportunity to test yourself.  At a neural level, the testing effect creates *error signals* between your guess and the right answer, and as we discussed in the learning chapter, the brain likely uses error-driven learning.

One of the best ways to really learn something is by teaching it to others, which is a version of the **generation effect** -- having to produce a sensible explanation of something greatly improves comprehension.  This is similar to the testing effect.  Try cornering a friend and give them a mini-lecture on how memory works in the brain -- you'll soon find the gaps in your understanding, and strongly reinforce the parts you already do understand.  Seriously, if you want to learn, teach!  This is one of the most important synergies in academia: by having to explain what we've learned in our research through teaching, professors then understand it all much better.

One of the most fascinating encoding principles is the **encoding specificity principle** [@Tulving83], which reflects the fact that episodic memories tend to bind together all of the different elements present when a memory is encoded, and thus recall of those memories will be best when those original elements are present at the time of recall.  This is a direct result of the pattern completion vs. pattern separation battle operating in the hippocampus -- if too many elements are different from the original event, the hippocampus tends to perform pattern separation instead of the pattern completion required for recall.  All those random elements present at the time of encoding are typically summarized with the term **context**, and thus lead to the **context-dependent memory** phenomenon [@YonelinasRanganathEkstromEtAl19].  A classic example of this phenomenon is that people are better able to recall information when tested in the same physical context as it was originally learned -- for example, if you study in a library, then taking a test in that same library will generally result in better performance.

The most famous demonstration of this encoding specificity / context-dependent memory principle was conducted in a study where items were learned either on a beach or underwater using scuba equipment, and then tested either in the same or different context [@GoddenBaddeley75].  Participants in the same-context conditions (either on land or under water) performed significantly better than in the cross-context conditions.  Another notorious demonstration involved study and test either drunk or sober, which again found that, surprisingly, testing while drunk was better than sober *if* initial learning was drunk [@GoodwinPowellBremerEtAl69].  This has been labeled the **state-dependent memory** effect, and presumably reflects the same encoding specificity principle.

It is important to emphasize that memory was much worse when learned drunk, even when tested drunk, so that is *not* a good strategy overall.  Other demonstrations of state-dependent memory involve mood states, and we'll see later that this **mood-dependent memory** effect creates an unfortunate feedback loop in depression, where you're much more likely to remember all the bad memories in your life when you're depressed, making everything seem that much more bleak.  On the bright side, this also works for positive memories in positive mood states.

Another critical way to improve encoding is to use **spaced** instead of **massed** practice -- i.e., to space out your studying over multiple separate study sessions, instead of *cramming* at the last minute.  This is beneficial for the same reason that gives rise to context-dependent effects: spacing out study causes the information to be learned across multiple different contexts, and thus helps to make the knowledge more independent of that context.  A critical point here is that *context* includes a significant contribution from *time* -- your internal mental state is constantly evolving over time, and will be significantly different a week or two from now [@HowardKahana99].  Thus, even if you study in the same physical context, your internal mental context will be different, shaped by all those synaptic changes taking place between the two study sessions.  Indeed, a critical aspect of the memory consolidation effect involves exactly this process of thinking about the same issues from the very different perspectives that emerge as your brain changes over the period of years.

In short, you should study by engaging in deep, elaborative encoding of the material, connecting it in multiple different ways with your existing knowledge chunks, and testing yourself as much as possible, ideally by trying to teach material to others.  Furthermore, you should do this in a spaced fashion, across multiple different days, ideally in different physical and mental contexts.  That's not so hard, is it?

## Memory Retention and Interference

![**Figure 6.7:** The forgetting curve from Hermann Ebbinghaus's data.  The initial steep dropoff is likely due to synaptic-level stabilization processes, and the longer plateau reflects essentially permanent long-term memory, with loss due largely to interference.](figures/fig_ebbinghaus_forgetting_curve.png){ width=50% }

Even once you've successfully encoded some new information into LTM, it is still not safe!  Memory is often a fleeting thing, as you have almost certainly experienced.  Figure 6.7 shows the data from Hermann Ebbinghaus who pioneered the study of memory retention in the late 1800's [@Ebbinghaus1885].  This curve is striking in its steep initial dropoff, followed by a relatively stable plateau.  We can understand the nature of this curve in terms of two different processes, which have long been the subject of debate in the field:  **decay** and **interference**.  It is surprisingly difficult to distinguish these two on purely behavioral grounds, as it is generally impossible to prevent interference from happening, and decay is defined as an automatic, continuous process.  However, detailed studies of the molecular processes following the synaptic plasticity events described in the Learning chapter allow for some resolution of this debate.

![**Figure 6.8:** Forgetting curve from synapse-level stabilization effects, which shows a steep dropoff in synaptic strength for more weakly potentiated synapses (2 HFS curves in bottom graph; HFS = high frequency stimulation) compared to more strongly potentiated synapses (4 HFS, top graph).  The KT 5720 curve shows the contribution of protein synthesis, which emerges over the period of an hour or so, and if these proteins are not available, the weaker memory decays back to baseline.  From Alarcon, Barco & Kandel (2006).](figures/fig_ltp_forgetting_curve.png){ width=50% }

It is likely that the steep initial dropoff in memory is due to synapse-level processes, which can be considered a form of decay, but that after roughly a day or two, synaptic changes have stabilized to the point that subsequent forgetting is mostly due to interference effects.  These synapse-level processes are collectively known as *synaptic consolidation*, which is distinct from the *systems consolidation* processes described above, where memory initially encoded in the hippocampus is learned in the neocortex as well. As shown in Figure 6.8, there is a roughly 15-20 minute period when synaptic changes can decay rapidly if they were not sufficiently strong in the first place, or reinforced by subsequent plasticity events [@AlarconBarcoKandel06; @FreyMorris98].  Over the course of an hour, synaptic changes are reinforced by processes that depend on new proteins being synthesized, including muscle-like *actin* fibers.  Further stabilization occurs during sleep, over the next day or two (for all the details, see [@Rudy13]).

After all of this synaptic consolidation has taken place, it is likely that further loss of memory is due to interference effects, which occur when new synaptic changes move the synaptic strengths in a different direction than was needed for an existing memory.  This is known as **retroactive interference**, because it is interfering with older ("retro") memories.  The extreme pattern separation in the hippocampus can help to minimize the amount of retroactive interference, by encouraging the use of distinct sets of synapses to encode different memories, but it is impossible to completely eliminate interference.

An example of retroactive interference would be when you encode where you parked your car today, versus yesterday.  Because of the large amount of overlap in the overall context, you likely re-activate many of the same neurons involved in encoding these two memories.  Thus, the synaptic changes that are made today will help you recall that it was parked in the South-West corner of the lot, but these changes will likely overwrite many of the synapses that encoded the "South-East" location from yesterday.

There is another form of interference called **proactive interference** which is somewhat strange compared to the more intuitive nature of retroactive interference.  In proactive interference, prior learning interferes with your ability to form *new* memories.  This can happen if you are trying to learn new information about the same items over time.  For example, if you use distinctive new items on every trial of a memory task (shapes, colors, letters, words, animals, etc), then it is easier to remember those items compared to re-using the same items repeatedly [@HasselmoStern06].

## The Fallibility of Memory

In addition to failures of basic encoding and forgetting, there are other pitfalls in the domain of memory, which arise largely from the fact that the hippocampus only receives a highly *compressed* view of the outside world, filtered through many layers of cortical processing and compression as shown in Figure 6.4.  Figure 6.1 of the telephone game also captures the kind of compounding effects that emerge from information propagation through the cortex.  From this perspective it is a wonder that we can accurately remember anything at all!  

As we saw in the perception chapter, even our lower-level perceptual system is strongly influenced by top-down, internal biases, and this process is ubiquitous throughout the cortex.  Thus, we all encode our memories through spectacles of one shade or another, and are susceptible to having **false memories**.  One of the first demonstrations of this point in the memory literature was due to Frederic Bartlett, who tested people's ability to remember a story known as the "War of the Ghosts", over an extended period of time [@Bartlett32].  This story was based on Canadian Indian folklore, and contained many concepts and events that were entirely unfamiliar to the English participants in his experiment.  As a result, the participants had great difficulty remembering the story, and ended up reshaping it to fit their own conceptual structures.  These conceptual structures are called **schema**, and we'll revisit them again in the next chapter.

An important real-world implication of this strong tendency to *schematize* memory is in **eyewitness testimony**, where people are likely to encode the events of a crime according to their existing *stereotypes* and biases.   Furthermore, these biases can be activated by leading questions.  For example, in one seminal study, the experimenters manipulated the use of leading terms like "smashed" in a car crash scenario, and this had large effects on participant's memory of things like the speed and damage involved [@LoftusPalmer74].  Interestingly, as was the case in the Bartlett study, participant's confidence in their *false* memories was often higher than for their accurate ones.

The other major issue that has received considerable media attention is recovered memories of childhood sexual abuse.  Unfortunately, abuse is all too common, but it is also the case that memory in young children is even more unreliable than in adults.  Studies have shown that children can report having actually experienced events that they only imagined [@CeciHuffmanSmithEtAl94], and some forms of therapy designed to uncover repressed memories may have used leading questions that could have created false memories.

In the experimental literature, false memory has been extensively explored using the *Deese, Roediger, McDermott (DRM)* paradigm [@Deese59; @RoedigerMcDermott95].  In this paradigm, a number of words that overlap strongly with a given target word (e.g., pillow, dream, night, etc) are studied, with the result that the target word ("sleep" in this case) is often confidently endorsed as having been on the study list.  This is vivid demonstration that memory operates on high-level compressed semantic representations.

## Working Memory and the Prefrontal Cortex

Finally, we conclude with one more important distinction between different types of memory, in this case between short-term memory (STM) and **working memory** (*WM*), which was proposed by Alan Baddeley and Graham Hitch [@BaddeleyHitch74].  The notion of working memory resembles the functional properties of RAM in a standard computer: information that is currently being processed, maintained in an active, directly accessible state.  Working memory is distinguished from "regular" STM, where the latter includes just basic maintenance of information, whereas working memory is specifically about the information used for ongoing processing, which is particularly strongly maintained, even in the face of potential distracters.

As is often the case, the biology may provide a more precise definition of the difference between STM and working memory, in the form of robust sustained firing of neurons in the prefrontal cortex, which was discovered in the early 1970's [@FusterAlexander71; @KubotaNiki71].  This sustained neural activity was postulated as the neural basis of working memory [@Goldman-Rakic95], and studies showed that this form of neural activity is indeed more robust and resistant to distraction than activity in posterior cortical areas [@MillerDesimone94].  Computational models have shown that the basal ganglia can play a critical role in supporting this robust active memory in frontal areas, by dynamically switching the system between maintenance and rapid updating modes [@OReillyFrank06].  Thus, overall there is ample biological evidence that sustained neural activity in the frontal cortex is different from that in posterior cortical areas, in ways that accord with the overall distinction between working memory versus STM.  We'll focus more on this frontal / basal ganglia working memory system in the next chapter.

## Summary of Key Terms

This is a checklist of key terms / concepts that you should know about from this chapter.  As we just learned in this chapter, it is a great idea to test yourself on what was said about each of these terms, and then go back and double-check -- that provides both beneficial repetition and also the *testing effect*.

* Neural mechanisms of memory:
    + Activity (spiking etc): fast, active and transient
    + Synaptic changes: slower, long-lasting

* Modal model 
    + Sensory memory: iconic, echoic (high capacity, short-lived) -- neural activity in sensory cortex
    + Short-term memory (STM): requires attention, limited capacity (magic number 4) -- neural activity in higher cortical areas
        + Sperling task
    + Long-term memory (LTM): requires encoding -- synaptic changes in hippocampus and cortex.

* Hippocampus
    + Anatomical location on top of cortex
    + Pattern separation from sparse activity
    + Pattern completion to recall memories
    + Memory consolidation: semantic knowledge forms slowly in neocortex

* LTM Taxonomy
    + Explicit / Implicit
    + Episodic / Semantic / Priming / Conditioning / Procedural
    + Issues with consciousness

* Amnesia
    + Anterograde
    + Retrograde
    + Childhood amnesia

* Encoding / Retrieval Strategies
    + Chunk
    + Mnemonic
    + Method of loci
    + Elaborative encoding
    + Levels of processing
    + Desirable difficulties: Testing effect, generation effect
    + Encoding specificity principle
        + Context-dependent memory
        + State-dependent memory
        + Mood-dependent memory
    + Massed vs. Spaced practice (cramming is bad)

* Memory Retention and Interference
    + Decay: synaptic stabilization
    + Interference: Retroactive vs. Proactive

* Fallibility of Memory
    + False memories: War of the Ghosts
    + Schema
    + Eyewitness testimony & leading questions

* Working memory vs. STM
    + Robust firing in prefrontal cortex

